{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dagger \u00b6 Dagger is a Python library that allows you to: Define sophisticated DAGs (direct acyclic graphs) using very straightforward Pythonic code. Run those DAGs seamlessly in different runtimes or workflow orchestrators (such as Argo Workflows, Kubeflow Pipelines, and more). \ud83e\uddf0 Features \u00b6 Express DAGs succinctly. Create dynamic for loops and map-reduce operations. Invoke DAGs from other DAGs. Run your DAGs locally or using a distributed workflow orchestrator (such as Argo Workflows). Take advantage of advanced runtime features (e.g. Retry strategies, Kubernetes scheduling directives, etc.) ... All with a simple Pythonic DSL that feels just like coding regular Python functions. Other nice features of Dagger are: it adds no extra dependencies to your project, it is reliable (with 100% test coverage), and it has great documentation and plenty of examples to get you started. \ud83c\udfaf Guiding Principles \u00b6 Dagger was created to facilitate the implementation and ongoing maintenance of data and ML pipelines. This goal is reflected in Dagger 's architecture and main design decisions: To make common use cases and patterns (such as dynamic loops or map-reduce operations) as easy as possible . To minimize boilerplate, plumbing or low-level code (with Dagger you don't need to serialize your outputs, store them in a remote file system, download them and deserialize them again; all of this is done for you). To onboard users in just a couple of hours through great documentation, comprehensive examples and tutorials. To never sacrifice reliability and performance , and to keep a low memory footprint by using I/O streams and lazy loading where possible. \u26e9\ufe0f Architecture \u00b6 Dagger is built around 3 components: A set of core data structures that represent the intended behavior of a DAG. A domain-specific language (DSL) that uses metaprogramming to capture how a DAG should behave, and represents it using the core data structures. Multiple runtimes that inspect the core data structures to run the corresponding DAG, or prepare the DAG to run in a specific pipeline executor. The Core Data Structures \u00b6 Dagger defines DAGs using a series of immutable data structures. These structures are responsible for: Exposing all the relevant information so that runtimes can run the DAGs, or translate them into formats supported by other pipeline executors. Validating all the pieces of a DAG to catch errors as early as possible in the development lifecycle. For instance: Node inputs must not reference outputs that do not exist. The DAG must not contain any cycles. Nodes and outputs are partitioned in ways supported by the library. etc. They are divided into different categories: Nodes may be tasks (functions) or DAGs (a series of nodes connected together). DAGs can be nested inside of other DAGs. Inputs may come from a DAG parameter or from the output of another node. Outputs may be retrieved directly from the return value of a task's function, or from a sub-element of that value (a key or a property). Every input/output has a serializer associated with it. The serializer is responsible for turning the value of that input/output into a string of bytes, and a string of bytes back into its original value. Does it sound interesting? See it in action!","title":"Overview"},{"location":"#dagger","text":"Dagger is a Python library that allows you to: Define sophisticated DAGs (direct acyclic graphs) using very straightforward Pythonic code. Run those DAGs seamlessly in different runtimes or workflow orchestrators (such as Argo Workflows, Kubeflow Pipelines, and more).","title":"Dagger"},{"location":"#features","text":"Express DAGs succinctly. Create dynamic for loops and map-reduce operations. Invoke DAGs from other DAGs. Run your DAGs locally or using a distributed workflow orchestrator (such as Argo Workflows). Take advantage of advanced runtime features (e.g. Retry strategies, Kubernetes scheduling directives, etc.) ... All with a simple Pythonic DSL that feels just like coding regular Python functions. Other nice features of Dagger are: it adds no extra dependencies to your project, it is reliable (with 100% test coverage), and it has great documentation and plenty of examples to get you started.","title":"\ud83e\uddf0 Features"},{"location":"#guiding-principles","text":"Dagger was created to facilitate the implementation and ongoing maintenance of data and ML pipelines. This goal is reflected in Dagger 's architecture and main design decisions: To make common use cases and patterns (such as dynamic loops or map-reduce operations) as easy as possible . To minimize boilerplate, plumbing or low-level code (with Dagger you don't need to serialize your outputs, store them in a remote file system, download them and deserialize them again; all of this is done for you). To onboard users in just a couple of hours through great documentation, comprehensive examples and tutorials. To never sacrifice reliability and performance , and to keep a low memory footprint by using I/O streams and lazy loading where possible.","title":"\ud83c\udfaf Guiding Principles"},{"location":"#architecture","text":"Dagger is built around 3 components: A set of core data structures that represent the intended behavior of a DAG. A domain-specific language (DSL) that uses metaprogramming to capture how a DAG should behave, and represents it using the core data structures. Multiple runtimes that inspect the core data structures to run the corresponding DAG, or prepare the DAG to run in a specific pipeline executor.","title":"\u26e9\ufe0f Architecture"},{"location":"#the-core-data-structures","text":"Dagger defines DAGs using a series of immutable data structures. These structures are responsible for: Exposing all the relevant information so that runtimes can run the DAGs, or translate them into formats supported by other pipeline executors. Validating all the pieces of a DAG to catch errors as early as possible in the development lifecycle. For instance: Node inputs must not reference outputs that do not exist. The DAG must not contain any cycles. Nodes and outputs are partitioned in ways supported by the library. etc. They are divided into different categories: Nodes may be tasks (functions) or DAGs (a series of nodes connected together). DAGs can be nested inside of other DAGs. Inputs may come from a DAG parameter or from the output of another node. Outputs may be retrieved directly from the return value of a task's function, or from a sub-element of that value (a key or a property). Every input/output has a serializer associated with it. The serializer is responsible for turning the value of that input/output into a string of bytes, and a string of bytes back into its original value. Does it sound interesting? See it in action!","title":"The Core Data Structures"},{"location":"quick-start/","text":"Quick Start \u00b6 To get a first idea of what Dagger can do, let's install it, create our first DAG and run it locally. \ud83d\udcbe Installation \u00b6 Dagger is published to the Python Package Index (PyPI) under the name py-dagger . To install it, you can simply run: 1 pip install py-dagger \ud83e\uddb8 Dagger in Action \u00b6 Defining a DAG using the imperative DSL \u00b6 The following piece of code demonstrates how to build a DAG that performs a map-reduce operation on a series of numbers: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import random from dagger import dsl @dsl . task () def generate_numbers ( seed ): random . seed ( seed ) length = random . randint ( 3 , 20 ) numbers = list ( range ( length )) print ( f \"Generating the following list of numbers: { numbers } \" ) return numbers @dsl . task () def raise_number ( n , exponent ): print ( f \"Raising { n } to a power of { exponent } \" ) return n ** exponent @dsl . task () def sum_numbers ( numbers ): print ( f \"Calculating the sum of { numbers } \" ) return sum ( numbers ) @dsl . DAG () def map_reduce_pipeline ( seed , exponent ): numbers = generate_numbers ( seed ) raised_numbers = [] for n in numbers : raised_numbers . append ( raise_number ( n , exponent )) return sum_numbers ( raised_numbers ) Let's take it step by step. First, we use the dagger.dsl.task decorator to define different tasks. Tasks in Dagger are just Python functions. In this case, we have 3 tasks: generate_numbers() returns a list of numbers. The length of the list varies dynamically. raise_number(n, exponent) receives a number and an exponent, and returns n^exponent . sum_numbers(numbers) receives a list of numbers and returns the sum of all of them. Next, we use the dagger.dsl.DAG decorator on another function that invokes all the tasks we've defined and connects their inputs/outputs . In the example, we are iterating over all the numbers generated by the first task and running the second one. Then, we collect all the results of running the second task into a list and pass that list to a function that sums all the results together. We've used a traditional for loop and appended elements to the list, but you can try replacing it with something more Pythonic : 27 28 29 @dsl . DAG () def map_reduce_pipeline ( exponent ): return sum_numbers ([ raise_number ( n , exponent ) for n in generate_numbers ()]) Transforming the DAG into a set of data structures \u00b6 After defining how our DAG should behave using a function decorated by dagger.dsl.DAG , we will need to use dagger.dsl.build to transform it into a dagger.DAG data structure , like this: 1 dag = dsl . build ( map_reduce_pipeline ) The variable dag now contains our pipeline expressed as a collection of data structures. These data structures validate that our DAG has been built correctly, and allow us to run it using one of the available runtimes. Running our DAG locally \u00b6 The final step will be to test our DAG locally using the dagger.runtime.local . Just do: 1 2 3 4 from dagger.runtime.local import invoke result = invoke ( dag , params = { \"seed\" : 1 , \"exponent\" : 2 }) print ( f \"The final result was { result } \" ) After this, you should see the results of the DAG printed to your screen. Other Runtimes \u00b6 The previous example showed how we can model a fairly complex use case (a dynamic map-reduce) and run it locally in just a few lines of code. The great thing about Dagger is that running your pipeline in a distributed pipeline engine (such as Argo Workflows or Kubeflow Pipelines ) is just as easy! You can check the Runtimes Documentation to learn about the available runtimes. \ud83e\udde0 Tutorials, Examples and API Reference \u00b6 Does it look useful so far? We're just scratching the surface of what's possible with Dagger . To learn more, you can: Start by defining some tasks . Check all the examples . Browse the API Reference .","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"To get a first idea of what Dagger can do, let's install it, create our first DAG and run it locally.","title":"Quick Start"},{"location":"quick-start/#installation","text":"Dagger is published to the Python Package Index (PyPI) under the name py-dagger . To install it, you can simply run: 1 pip install py-dagger","title":"\ud83d\udcbe Installation"},{"location":"quick-start/#dagger-in-action","text":"","title":"\ud83e\uddb8 Dagger in Action"},{"location":"quick-start/#defining-a-dag-using-the-imperative-dsl","text":"The following piece of code demonstrates how to build a DAG that performs a map-reduce operation on a series of numbers: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import random from dagger import dsl @dsl . task () def generate_numbers ( seed ): random . seed ( seed ) length = random . randint ( 3 , 20 ) numbers = list ( range ( length )) print ( f \"Generating the following list of numbers: { numbers } \" ) return numbers @dsl . task () def raise_number ( n , exponent ): print ( f \"Raising { n } to a power of { exponent } \" ) return n ** exponent @dsl . task () def sum_numbers ( numbers ): print ( f \"Calculating the sum of { numbers } \" ) return sum ( numbers ) @dsl . DAG () def map_reduce_pipeline ( seed , exponent ): numbers = generate_numbers ( seed ) raised_numbers = [] for n in numbers : raised_numbers . append ( raise_number ( n , exponent )) return sum_numbers ( raised_numbers ) Let's take it step by step. First, we use the dagger.dsl.task decorator to define different tasks. Tasks in Dagger are just Python functions. In this case, we have 3 tasks: generate_numbers() returns a list of numbers. The length of the list varies dynamically. raise_number(n, exponent) receives a number and an exponent, and returns n^exponent . sum_numbers(numbers) receives a list of numbers and returns the sum of all of them. Next, we use the dagger.dsl.DAG decorator on another function that invokes all the tasks we've defined and connects their inputs/outputs . In the example, we are iterating over all the numbers generated by the first task and running the second one. Then, we collect all the results of running the second task into a list and pass that list to a function that sums all the results together. We've used a traditional for loop and appended elements to the list, but you can try replacing it with something more Pythonic : 27 28 29 @dsl . DAG () def map_reduce_pipeline ( exponent ): return sum_numbers ([ raise_number ( n , exponent ) for n in generate_numbers ()])","title":"Defining a DAG using the imperative DSL"},{"location":"quick-start/#transforming-the-dag-into-a-set-of-data-structures","text":"After defining how our DAG should behave using a function decorated by dagger.dsl.DAG , we will need to use dagger.dsl.build to transform it into a dagger.DAG data structure , like this: 1 dag = dsl . build ( map_reduce_pipeline ) The variable dag now contains our pipeline expressed as a collection of data structures. These data structures validate that our DAG has been built correctly, and allow us to run it using one of the available runtimes.","title":"Transforming the DAG into a set of data structures"},{"location":"quick-start/#running-our-dag-locally","text":"The final step will be to test our DAG locally using the dagger.runtime.local . Just do: 1 2 3 4 from dagger.runtime.local import invoke result = invoke ( dag , params = { \"seed\" : 1 , \"exponent\" : 2 }) print ( f \"The final result was { result } \" ) After this, you should see the results of the DAG printed to your screen.","title":"Running our DAG locally"},{"location":"quick-start/#other-runtimes","text":"The previous example showed how we can model a fairly complex use case (a dynamic map-reduce) and run it locally in just a few lines of code. The great thing about Dagger is that running your pipeline in a distributed pipeline engine (such as Argo Workflows or Kubeflow Pipelines ) is just as easy! You can check the Runtimes Documentation to learn about the available runtimes.","title":"Other Runtimes"},{"location":"quick-start/#tutorials-examples-and-api-reference","text":"Does it look useful so far? We're just scratching the surface of what's possible with Dagger . To learn more, you can: Start by defining some tasks . Check all the examples . Browse the API Reference .","title":"\ud83e\udde0 Tutorials, Examples and API Reference"},{"location":"api/dag/","text":"package dagger. dag Define workflows/pipelines as Directed Acyclic Graphs (DAGs) of Tasks. Top-level objects exported \u00b6 1 2 3 4 5 6 7 8 9 10 \"\"\"Define workflows/pipelines as Directed Acyclic Graphs (DAGs) of Tasks.\"\"\" from dagger.dag.dag import ( # noqa DAG , Node , SupportedInputs , SupportedOutputs , validate_parameters , ) from dagger.dag.topological_sort import CyclicDependencyError # noqa DAG \u00b6 class dagger.dag.dag. DAG ( nodes , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Data Structure that represents a DAG for later execution. DAGs are made up of: - A set of nodes, connected to each other through their inputs/outputs - A set of inputs to the DAG - A set of outputs from the DAG Attributes inputs \u2014 Get the inputs the DAG expects. node_execution_order \u2014 Get a list of nodes to execute in order, respecting dependencies between nodes. nodes \u2014 Get the nodes that compose the DAG. outputs \u2014 Get the outputs the DAG produces. partition_by_input (str, optional) \u2014 Return the input this task should be partitioned by, if any. runtime_options \u2014 Get the specified runtime options. Methods __eq__ ( obj ) (bool) \u2014 Return true if the two DAGs are equivalent to each other. __repr__ ( ) (str) \u2014 Return a human-readable representation of the DAG. method __repr__ ( ) \u2192 str Return a human-readable representation of the DAG. method __eq__ ( obj ) \u2192 bool Return true if the two DAGs are equivalent to each other. DAG Initialization \u00b6 method __init__ ( nodes , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Validate and initialize a DAG. Parameters nodes (Mapping[str, Node]) \u2014 A mapping from node names to nodes. Only certain types are allowed as nodes. inputs (Mapping[str, SupportedInputs], default={}) \u2014 A mapping from input names to DAG inputs. Only certain types are allowed as inputs. outputs (Mapping[str, SupportedOutputs], default={}) \u2014 A mapping from output names to DAG outputs. Outputs must come from the output of a node within the DAG. runtime_options (Mapping[str, Any], default={}) \u2014 A list of options to supply to all runtimes. This allows you to take full advantage of the features of each runtime. For instance, you can use it to manipulate node affinities and tolerations in Kubernetes. Check the documentation of each runtime to see potential options. partition_by_input (str, optional) \u2014 If specified, it signals the task should be run as many times as partitions in the specified input. Each of the executions will only receive one of the partitions of that input. Raises CyclicDependencyError \u2014 If the nodes contain cyclic dependencies to one another. TypeError \u2014 If any of the supplied parameters has an invalid type. Types should match the ones defined in the method signature. ValueError \u2014 If any of the names are invalid, or any of the inputs/outputs point to non-existing nodes. validate_parameters \u00b6 function dagger.dag.dag. validate_parameters ( inputs , params ) Validate a series of parameters against the inputs of a DAG. Parameters inputs \u2014 A mapping of input names to inputs. params \u2014 A mapping of input names to parameters or input values. Input values must be passed in their serialized representation. Raises ValueError \u2014 If the set of parameters does not contain all the required inputs. CyclicDependencyError \u00b6 class dagger.dag.topological_sort. CyclicDependencyError ( ) Bases Exception BaseException Error raised when a DAG contains a cyclic dependency, and thus cannot be executed.","title":"dagger.dag"},{"location":"api/dag/#daggerdag","text":"Define workflows/pipelines as Directed Acyclic Graphs (DAGs) of Tasks.","title":"dagger.dag"},{"location":"api/dag/#top-level-objects-exported","text":"1 2 3 4 5 6 7 8 9 10 \"\"\"Define workflows/pipelines as Directed Acyclic Graphs (DAGs) of Tasks.\"\"\" from dagger.dag.dag import ( # noqa DAG , Node , SupportedInputs , SupportedOutputs , validate_parameters , ) from dagger.dag.topological_sort import CyclicDependencyError # noqa","title":"Top-level objects exported"},{"location":"api/dag/#dag","text":"class dagger.dag.dag. DAG ( nodes , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Data Structure that represents a DAG for later execution. DAGs are made up of: - A set of nodes, connected to each other through their inputs/outputs - A set of inputs to the DAG - A set of outputs from the DAG Attributes inputs \u2014 Get the inputs the DAG expects. node_execution_order \u2014 Get a list of nodes to execute in order, respecting dependencies between nodes. nodes \u2014 Get the nodes that compose the DAG. outputs \u2014 Get the outputs the DAG produces. partition_by_input (str, optional) \u2014 Return the input this task should be partitioned by, if any. runtime_options \u2014 Get the specified runtime options. Methods __eq__ ( obj ) (bool) \u2014 Return true if the two DAGs are equivalent to each other. __repr__ ( ) (str) \u2014 Return a human-readable representation of the DAG. method __repr__ ( ) \u2192 str Return a human-readable representation of the DAG. method __eq__ ( obj ) \u2192 bool Return true if the two DAGs are equivalent to each other.","title":"DAG"},{"location":"api/dag/#dag-initialization","text":"method __init__ ( nodes , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Validate and initialize a DAG. Parameters nodes (Mapping[str, Node]) \u2014 A mapping from node names to nodes. Only certain types are allowed as nodes. inputs (Mapping[str, SupportedInputs], default={}) \u2014 A mapping from input names to DAG inputs. Only certain types are allowed as inputs. outputs (Mapping[str, SupportedOutputs], default={}) \u2014 A mapping from output names to DAG outputs. Outputs must come from the output of a node within the DAG. runtime_options (Mapping[str, Any], default={}) \u2014 A list of options to supply to all runtimes. This allows you to take full advantage of the features of each runtime. For instance, you can use it to manipulate node affinities and tolerations in Kubernetes. Check the documentation of each runtime to see potential options. partition_by_input (str, optional) \u2014 If specified, it signals the task should be run as many times as partitions in the specified input. Each of the executions will only receive one of the partitions of that input. Raises CyclicDependencyError \u2014 If the nodes contain cyclic dependencies to one another. TypeError \u2014 If any of the supplied parameters has an invalid type. Types should match the ones defined in the method signature. ValueError \u2014 If any of the names are invalid, or any of the inputs/outputs point to non-existing nodes.","title":"DAG Initialization"},{"location":"api/dag/#validate_parameters","text":"function dagger.dag.dag. validate_parameters ( inputs , params ) Validate a series of parameters against the inputs of a DAG. Parameters inputs \u2014 A mapping of input names to inputs. params \u2014 A mapping of input names to parameters or input values. Input values must be passed in their serialized representation. Raises ValueError \u2014 If the set of parameters does not contain all the required inputs.","title":"validate_parameters"},{"location":"api/dag/#cyclicdependencyerror","text":"class dagger.dag.topological_sort. CyclicDependencyError ( ) Bases Exception BaseException Error raised when a DAG contains a cyclic dependency, and thus cannot be executed.","title":"CyclicDependencyError"},{"location":"api/dsl/","text":"package dagger. dsl Define DAGs through an imperative domain-specific language. Top-level objects exported \u00b6 1 2 3 4 5 \"\"\"Define DAGs through an imperative domain-specific language.\"\"\" from dagger.dsl.build import build # noqa from dagger.dsl.dsl import DAG , task # noqa from dagger.dsl.node_output_serializer import NodeOutputSerializer as Serialize # noqa @task \u00b6 function dagger.dsl.dsl. task ( serializer=Serialize(root=AsJSON(indent=None, allow_nan=False)) , runtime_options=None ) \u2192 callable(: NodeInvocationRecorder) Decorate a function as a Task. You can check examples of how to use the DSL in the examples/dsl directory. @DAG \u00b6 function dagger.dsl.dsl. DAG ( runtime_options=None ) \u2192 callable(: NodeInvocationRecorder) Decorate a function as a DAG. You can check examples of how to use the DSL in the examples/dsl directory. build \u00b6 module dagger.dsl. build Build DAGs through an imperative domain-specific language. Functions build ( dag ) (DAG) \u2014 Build a DAG data structure it defines. Serialize \u00b6 class dagger.dsl.node_output_serializer. NodeOutputSerializer ( root=AsJSON(indent=None, allow_nan=False) , **kwargs ) Indicate the serializer that should be used for the outputs of a specific node. Attributes root (Serializer) \u2014 Return the serializer for the root output of the function. Methods __eq__ ( obj ) (bool) \u2014 Return true if the object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of this class. sub_output ( output_name ) (Serializer, optional) \u2014 Return the serializer assigned to the output with the name provided, if any. method sub_output ( output_name ) \u2192 Serializer, optional Return the serializer assigned to the output with the name provided, if any. method __eq__ ( obj ) \u2192 bool Return true if the object is equivalent to the current instance. method __repr__ ( ) \u2192 str Return a human-readable representation of this class.","title":"dagger.dsl"},{"location":"api/dsl/#daggerdsl","text":"Define DAGs through an imperative domain-specific language.","title":"dagger.dsl"},{"location":"api/dsl/#top-level-objects-exported","text":"1 2 3 4 5 \"\"\"Define DAGs through an imperative domain-specific language.\"\"\" from dagger.dsl.build import build # noqa from dagger.dsl.dsl import DAG , task # noqa from dagger.dsl.node_output_serializer import NodeOutputSerializer as Serialize # noqa","title":"Top-level objects exported"},{"location":"api/dsl/#task","text":"function dagger.dsl.dsl. task ( serializer=Serialize(root=AsJSON(indent=None, allow_nan=False)) , runtime_options=None ) \u2192 callable(: NodeInvocationRecorder) Decorate a function as a Task. You can check examples of how to use the DSL in the examples/dsl directory.","title":"@task"},{"location":"api/dsl/#dag","text":"function dagger.dsl.dsl. DAG ( runtime_options=None ) \u2192 callable(: NodeInvocationRecorder) Decorate a function as a DAG. You can check examples of how to use the DSL in the examples/dsl directory.","title":"@DAG"},{"location":"api/dsl/#build","text":"module dagger.dsl. build Build DAGs through an imperative domain-specific language. Functions build ( dag ) (DAG) \u2014 Build a DAG data structure it defines.","title":"build"},{"location":"api/dsl/#serialize","text":"class dagger.dsl.node_output_serializer. NodeOutputSerializer ( root=AsJSON(indent=None, allow_nan=False) , **kwargs ) Indicate the serializer that should be used for the outputs of a specific node. Attributes root (Serializer) \u2014 Return the serializer for the root output of the function. Methods __eq__ ( obj ) (bool) \u2014 Return true if the object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of this class. sub_output ( output_name ) (Serializer, optional) \u2014 Return the serializer assigned to the output with the name provided, if any. method sub_output ( output_name ) \u2192 Serializer, optional Return the serializer assigned to the output with the name provided, if any. method __eq__ ( obj ) \u2192 bool Return true if the object is equivalent to the current instance. method __repr__ ( ) \u2192 str Return a human-readable representation of this class.","title":"Serialize"},{"location":"api/init/","text":"package dagger Define sophisticated data pipelines as Directed Acyclic Graphs (DAGs) and execute them with different runtimes, either locally or remotely. Top-level objects exported \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \"\"\"Define sophisticated data pipelines as Directed Acyclic Graphs (DAGs) and execute them with different runtimes, either locally or remotely.\"\"\" import dagger.dsl as dsl # noqa from dagger.dag import DAG # noqa from dagger.input import FromNodeOutput , FromParam # noqa from dagger.output import FromKey , FromProperty , FromReturnValue # noqa from dagger.serializer import ( # noqa AsJSON , AsPickle , DeserializationError , SerializationError , Serializer , ) from dagger.task import Task # noqa # This will be replaced at package publication time by the latest git tag __version__ = \"0.0.0\"","title":"dagger"},{"location":"api/init/#dagger","text":"Define sophisticated data pipelines as Directed Acyclic Graphs (DAGs) and execute them with different runtimes, either locally or remotely.","title":"dagger"},{"location":"api/init/#top-level-objects-exported","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \"\"\"Define sophisticated data pipelines as Directed Acyclic Graphs (DAGs) and execute them with different runtimes, either locally or remotely.\"\"\" import dagger.dsl as dsl # noqa from dagger.dag import DAG # noqa from dagger.input import FromNodeOutput , FromParam # noqa from dagger.output import FromKey , FromProperty , FromReturnValue # noqa from dagger.serializer import ( # noqa AsJSON , AsPickle , DeserializationError , SerializationError , Serializer , ) from dagger.task import Task # noqa # This will be replaced at package publication time by the latest git tag __version__ = \"0.0.0\"","title":"Top-level objects exported"},{"location":"api/input/","text":"package dagger. input Inputs for DAGs/Tasks. Top-level objects exported \u00b6 1 2 3 4 5 \"\"\"Inputs for DAGs/Tasks.\"\"\" from dagger.input.from_node_output import FromNodeOutput # noqa from dagger.input.from_param import FromParam # noqa from dagger.input.validators import validate_name # noqa FromParam \u00b6 class dagger.input.from_param. FromParam ( name=None , serializer=AsJSON(indent=None, allow_nan=False) ) Input retrieved from the parameters passed to the parent node. Attributes name (str, optional) \u2014 Get the name the input references, if any. serializer (Serializer) \u2014 Get the strategy to use in order to deserialize the supplied inputs. Methods __eq__ ( obj ) \u2014 Return true if both inputs are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the input. method __repr__ ( ) \u2192 str Get a human-readable string representation of the input. method __eq__ ( obj ) Return true if both inputs are equivalent. FromNodeOutput \u00b6 class dagger.input.from_node_output. FromNodeOutput ( node , output , serializer=AsJSON(indent=None, allow_nan=False) ) Input retrieved from the output of another node. Attributes node (str) \u2014 Get the name of the node the input should be retrieved from. output (str) \u2014 Get the name of the output the input should be retrieved from. serializer (Serializer) \u2014 Get the strategy to use in order to deserialize the supplied inputs. Methods __eq__ ( obj ) \u2014 Return true if both inputs are equivalent. __hash__ ( ) (int) \u2014 Return a hash that will be the same for two equivalent instances of this type. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the input. method __repr__ ( ) \u2192 str Get a human-readable string representation of the input. method __eq__ ( obj ) Return true if both inputs are equivalent. method __hash__ ( ) \u2192 int Return a hash that will be the same for two equivalent instances of this type. validate_name \u00b6 function dagger.input.validators. validate_name ( name ) Verify whether a string is a valid name for an input. Raises ValueError \u2014 If the name doesn't match the constraints set for an input name.","title":"dagger.input"},{"location":"api/input/#daggerinput","text":"Inputs for DAGs/Tasks.","title":"dagger.input"},{"location":"api/input/#top-level-objects-exported","text":"1 2 3 4 5 \"\"\"Inputs for DAGs/Tasks.\"\"\" from dagger.input.from_node_output import FromNodeOutput # noqa from dagger.input.from_param import FromParam # noqa from dagger.input.validators import validate_name # noqa","title":"Top-level objects exported"},{"location":"api/input/#fromparam","text":"class dagger.input.from_param. FromParam ( name=None , serializer=AsJSON(indent=None, allow_nan=False) ) Input retrieved from the parameters passed to the parent node. Attributes name (str, optional) \u2014 Get the name the input references, if any. serializer (Serializer) \u2014 Get the strategy to use in order to deserialize the supplied inputs. Methods __eq__ ( obj ) \u2014 Return true if both inputs are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the input. method __repr__ ( ) \u2192 str Get a human-readable string representation of the input. method __eq__ ( obj ) Return true if both inputs are equivalent.","title":"FromParam"},{"location":"api/input/#fromnodeoutput","text":"class dagger.input.from_node_output. FromNodeOutput ( node , output , serializer=AsJSON(indent=None, allow_nan=False) ) Input retrieved from the output of another node. Attributes node (str) \u2014 Get the name of the node the input should be retrieved from. output (str) \u2014 Get the name of the output the input should be retrieved from. serializer (Serializer) \u2014 Get the strategy to use in order to deserialize the supplied inputs. Methods __eq__ ( obj ) \u2014 Return true if both inputs are equivalent. __hash__ ( ) (int) \u2014 Return a hash that will be the same for two equivalent instances of this type. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the input. method __repr__ ( ) \u2192 str Get a human-readable string representation of the input. method __eq__ ( obj ) Return true if both inputs are equivalent. method __hash__ ( ) \u2192 int Return a hash that will be the same for two equivalent instances of this type.","title":"FromNodeOutput"},{"location":"api/input/#validate_name","text":"function dagger.input.validators. validate_name ( name ) Verify whether a string is a valid name for an input. Raises ValueError \u2014 If the name doesn't match the constraints set for an input name.","title":"validate_name"},{"location":"api/output/","text":"package dagger. output Outputs for DAGs/Tasks. Top-level objects exported \u00b6 1 2 3 4 5 6 \"\"\"Outputs for DAGs/Tasks.\"\"\" from dagger.output.from_key import FromKey # noqa from dagger.output.from_property import FromProperty # noqa from dagger.output.from_return_value import FromReturnValue # noqa from dagger.output.validators import validate_name # noqa FromReturnValue \u00b6 class dagger.output.from_return_value. FromReturnValue ( serializer=AsJSON(indent=None, allow_nan=False) , is_partitioned=False ) Bases typing.Generic Output retrieved directly from the return value of the task's function. Attributes is_partitioned (bool) \u2014 Return true if the output should be partitioned. serializer (Serializer) \u2014 Get the strategy to use in order to serialize the output. Methods __eq__ ( obj ) (bool) \u2014 Return true if both outputs are equivalent. __init_subclass__ ( *args , **kwargs ) \u2014 This method is called when a class is subclassed. __repr__ ( ) (str) \u2014 Return a human-readable representation of the output. from_function_return_value ( return_value ) (T) \u2014 Retrieve the output from the return value of the task's function. classmethod __init_subclass__ ( *args , **kwargs ) This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. method from_function_return_value ( return_value ) \u2192 T Retrieve the output from the return value of the task's function. method __eq__ ( obj ) \u2192 bool Return true if both outputs are equivalent. method __repr__ ( ) \u2192 str Return a human-readable representation of the output. FromKey \u00b6 class dagger.output.from_key. FromKey ( name , serializer=AsJSON(indent=None, allow_nan=False) , is_partitioned=False ) Bases typing.Generic Output retrieved from a key, when the function returns a Mapping. Attributes is_partitioned (bool) \u2014 Return true if the output should be partitioned. serializer (Serializer) \u2014 Get the strategy to use in order to serialize the output. Methods __eq__ ( obj ) (bool) \u2014 Return true if both outputs are equivalent. __init_subclass__ ( *args , **kwargs ) \u2014 This method is called when a class is subclassed. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the output. from_function_return_value ( return_value ) (The value that corresponds to that key) \u2014 Retrieve the output from the return value of the task's function. classmethod __init_subclass__ ( *args , **kwargs ) This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. method from_function_return_value ( return_value ) Retrieve the output from the return value of the task's function. Parameters return_value \u2014 A mapping returned by the Task's function Raises TypeError \u2014 If the function's return value is not a mapping. ValueError \u2014 If the function's return value does not contain the specified key. method __repr__ ( ) \u2192 str Get a human-readable string representation of the output. method __eq__ ( obj ) \u2192 bool Return true if both outputs are equivalent. FromProperty \u00b6 class dagger.output.from_property. FromProperty ( name , serializer=AsJSON(indent=None, allow_nan=False) , is_partitioned=False ) Output retrieved from a property, when the function returns an object. Attributes is_partitioned (bool) \u2014 Return true if the output should be partitioned. serializer (Serializer) \u2014 Get the strategy to use in order to serialize the output. Methods __eq__ ( obj ) (bool) \u2014 Return true if both outputs are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the output. from_function_return_value ( return_value ) (The value that corresponds to that property.) \u2014 Retrieve the output from a property of the return value. method from_function_return_value ( return_value ) Retrieve the output from a property of the return value. Parameters return_value \u2014 An object with a property with the expected name. Raises TypeError \u2014 If the function's return value does not contain the expected property. method __repr__ ( ) \u2192 str Get a human-readable string representation of the output. method __eq__ ( obj ) \u2192 bool Return true if both outputs are equivalent. validate_name \u00b6 function dagger.output.validators. validate_name ( name ) Verify whether a string is a valid name for an output. Raises ValueError \u2014 If the name doesn't match the constraints set for an output name.","title":"dagger.output"},{"location":"api/output/#daggeroutput","text":"Outputs for DAGs/Tasks.","title":"dagger.output"},{"location":"api/output/#top-level-objects-exported","text":"1 2 3 4 5 6 \"\"\"Outputs for DAGs/Tasks.\"\"\" from dagger.output.from_key import FromKey # noqa from dagger.output.from_property import FromProperty # noqa from dagger.output.from_return_value import FromReturnValue # noqa from dagger.output.validators import validate_name # noqa","title":"Top-level objects exported"},{"location":"api/output/#fromreturnvalue","text":"class dagger.output.from_return_value. FromReturnValue ( serializer=AsJSON(indent=None, allow_nan=False) , is_partitioned=False ) Bases typing.Generic Output retrieved directly from the return value of the task's function. Attributes is_partitioned (bool) \u2014 Return true if the output should be partitioned. serializer (Serializer) \u2014 Get the strategy to use in order to serialize the output. Methods __eq__ ( obj ) (bool) \u2014 Return true if both outputs are equivalent. __init_subclass__ ( *args , **kwargs ) \u2014 This method is called when a class is subclassed. __repr__ ( ) (str) \u2014 Return a human-readable representation of the output. from_function_return_value ( return_value ) (T) \u2014 Retrieve the output from the return value of the task's function. classmethod __init_subclass__ ( *args , **kwargs ) This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. method from_function_return_value ( return_value ) \u2192 T Retrieve the output from the return value of the task's function. method __eq__ ( obj ) \u2192 bool Return true if both outputs are equivalent. method __repr__ ( ) \u2192 str Return a human-readable representation of the output.","title":"FromReturnValue"},{"location":"api/output/#fromkey","text":"class dagger.output.from_key. FromKey ( name , serializer=AsJSON(indent=None, allow_nan=False) , is_partitioned=False ) Bases typing.Generic Output retrieved from a key, when the function returns a Mapping. Attributes is_partitioned (bool) \u2014 Return true if the output should be partitioned. serializer (Serializer) \u2014 Get the strategy to use in order to serialize the output. Methods __eq__ ( obj ) (bool) \u2014 Return true if both outputs are equivalent. __init_subclass__ ( *args , **kwargs ) \u2014 This method is called when a class is subclassed. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the output. from_function_return_value ( return_value ) (The value that corresponds to that key) \u2014 Retrieve the output from the return value of the task's function. classmethod __init_subclass__ ( *args , **kwargs ) This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. method from_function_return_value ( return_value ) Retrieve the output from the return value of the task's function. Parameters return_value \u2014 A mapping returned by the Task's function Raises TypeError \u2014 If the function's return value is not a mapping. ValueError \u2014 If the function's return value does not contain the specified key. method __repr__ ( ) \u2192 str Get a human-readable string representation of the output. method __eq__ ( obj ) \u2192 bool Return true if both outputs are equivalent.","title":"FromKey"},{"location":"api/output/#fromproperty","text":"class dagger.output.from_property. FromProperty ( name , serializer=AsJSON(indent=None, allow_nan=False) , is_partitioned=False ) Output retrieved from a property, when the function returns an object. Attributes is_partitioned (bool) \u2014 Return true if the output should be partitioned. serializer (Serializer) \u2014 Get the strategy to use in order to serialize the output. Methods __eq__ ( obj ) (bool) \u2014 Return true if both outputs are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the output. from_function_return_value ( return_value ) (The value that corresponds to that property.) \u2014 Retrieve the output from a property of the return value. method from_function_return_value ( return_value ) Retrieve the output from a property of the return value. Parameters return_value \u2014 An object with a property with the expected name. Raises TypeError \u2014 If the function's return value does not contain the expected property. method __repr__ ( ) \u2192 str Get a human-readable string representation of the output. method __eq__ ( obj ) \u2192 bool Return true if both outputs are equivalent.","title":"FromProperty"},{"location":"api/output/#validate_name","text":"function dagger.output.validators. validate_name ( name ) Verify whether a string is a valid name for an output. Raises ValueError \u2014 If the name doesn't match the constraints set for an output name.","title":"validate_name"},{"location":"api/runtime-argo/","text":"package dagger.runtime. argo Compile DAGs into manifests that will run on top of Argo Workflows . This runtime produces Python mappings (dictionaries) that contain all the necessary Kubernetes resources for Argo Workflows to execute the supplied DAG. Please note that this runtime doesn't produce JSON/YAML files. Users will still need to take the manifests generated by this runtime and apply them to a Kubernetes cluster. This runtime requires a container image and entrypoint to be specified, and assumes the supplied entrypoint exposes the same DAG that was used to generate the manifests through the CLI runtime. Please check the Argo runtime user guide for more details. Top-level objects exported \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \"\"\" Compile DAGs into manifests that will run on top of [Argo Workflows](https://argoproj.github.io/projects/argo). This runtime produces Python mappings (dictionaries) that contain all the necessary Kubernetes resources for Argo Workflows to execute the supplied DAG. Please note that this runtime doesn't produce JSON/YAML files. Users will still need to take the manifests generated by this runtime and apply them to a Kubernetes cluster. This runtime requires a container image and entrypoint to be specified, and assumes the supplied entrypoint exposes the same DAG that was used to generate the manifests through the CLI runtime. Please check the Argo runtime user guide for more details. \"\"\" from dagger.runtime.argo.cron import Cron , CronConcurrencyPolicy # noqa from dagger.runtime.argo.metadata import Metadata # noqa from dagger.runtime.argo.v1alpha1 import ( # noqa cluster_workflow_template_manifest , cron_workflow_manifest , workflow_manifest , workflow_template_manifest , ) from dagger.runtime.argo.workflow import Workflow # noqa Metadata \u00b6 class dagger.runtime.argo.metadata. Metadata ( name , generate_name_from_prefix=False , namespace=None , annotations=None , labels=None ) Metadata that may be provided for an Argo CRD. Attributes annotations \u2014 Return the annotation for the object's metadata. generate_name_from_prefix (bool) \u2014 Return a flag indicating whether the supplied name should be generated. labels \u2014 Return the labels for the object's metadata. name (str) \u2014 Return the name specified. namespace (str, optional) \u2014 Return the object's namespace, if any. Methods __eq__ ( obj ) (bool) \u2014 Return true if the supplied object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of the object. method __repr__ ( ) \u2192 str Return a human-readable representation of the object. method __eq__ ( obj ) \u2192 bool Return true if the supplied object is equivalent to the current instance. Metadata Initialization \u00b6 method __init__ ( name , generate_name_from_prefix=False , namespace=None , annotations=None , labels=None ) Initialize an object's Kubernetes metadata. Parameters name (str) \u2014 The name of the object generate_name_from_prefix (bool, optional) \u2014 Whether or not the supplied name should be used as a prefix of the actual name (which would be autogenerated). namespace (str, optional) \u2014 The namespace where the object should exist. annotations (Mapping[str, str], default={}) \u2014 A mapping of annotation keys and values to attach to the object. labels (Mapping[str, str], default={}) \u2014 A mapping of label keys and values to attach to the object. Workflow \u00b6 class dagger.runtime.argo.workflow. Workflow ( container_image , container_entrypoint_to_dag_cli=None , params=None , extra_spec_options=None ) Configuration for a Workflow. This class will be supplied to the runtime to help it create a workflow that will work on your environment. Attributes container_entrypoint_to_dag_cli (list of str) \u2014 Return the container entrypoint that will be used in order to run every step in this workflow. The entrypoint must expose the CLI runtime for the same DAG that was used to generate the Argo manifests. container_image (str) \u2014 Return the container image that will be used to run every step in this workflow. extra_spec_options \u2014 Return any extra options that should be passed to the WorkflowSpec. params \u2014 Return the parameters to supply to the workflow or workflow template. Methods __eq__ ( obj ) (bool) \u2014 Return true if the object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of this instance. method __repr__ ( ) \u2192 str Return a human-readable representation of this instance. method __eq__ ( obj ) \u2192 bool Return true if the object is equivalent to the current instance. Workflow Initialization \u00b6 method __init__ ( container_image , container_entrypoint_to_dag_cli=None , params=None , extra_spec_options=None ) Create a workflow configuration. Parameters container_image (str) \u2014 The URI to the container image Argo will use for every node in the DAG. container_entrypoint_to_dag_cli (List[str], default=[]) \u2014 The container entrypoint Argo will use for every node in the DAG. The entrypoint must expose the CLI runtime for the same DAG that was used to generate the Argo manifests. Check the section about the Argo runtime to understand better how to containerize your projects and expose your DAGs. params (Mapping[str, Any], default={}) \u2014 Parameters to inject to the DAG. They must match the inputs the DAG expects. extra_spec_options, Mapping[str, Any], default={} \u2014 WorkflowSpec properties to set (if they are not used by the runtime). Cron \u00b6 class dagger.runtime.argo.cron. Cron ( schedule , starting_deadline_seconds=0 , concurrency_policy=<CronConcurrencyPolicy.ALLOW: 'Allow'> , timezone=None , successful_jobs_history_limit=None , failed_jobs_history_limit=None , extra_spec_options=None ) Scheduling options for the cron job. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#cronworkflowspec Attributes concurrency_policy (CronConcurrencyPolicy) \u2014 Return the concurrency policy for this cron workflow. extra_spec_options \u2014 Return any extra options that should be passed to the Cron spec. failed_jobs_history_limit (int, optional) \u2014 Return the limit of failed jobs to keep a historical record for, if defined. schedule (str) \u2014 Return the schedule for this cron workflow. starting_deadline_seconds (int) \u2014 Return the starting deadline for this cron workflow, in seconds. successful_jobs_history_limit (int, optional) \u2014 Return the limit of successful jobs to keep a historical record for, if defined. timezone (str, optional) \u2014 Return the timezone this cron workflow should be scheduled according to, if any. Methods __eq__ ( obj ) (bool) \u2014 Return true if the object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of this instance. method __repr__ ( ) \u2192 str Return a human-readable representation of this instance. method __eq__ ( obj ) \u2192 bool Return true if the object is equivalent to the current instance. Cron Initialization \u00b6 method __init__ ( schedule , starting_deadline_seconds=0 , concurrency_policy=<CronConcurrencyPolicy.ALLOW: 'Allow'> , timezone=None , successful_jobs_history_limit=None , failed_jobs_history_limit=None , extra_spec_options=None ) Initialize the configuration for a cron workflow. Parameters schedule (str) \u2014 The schedule the workflow should run according to. starting_deadline_seconds (int, default=0) \u2014 The time to wait for a scheduled workflow to start. If the original schedule + deadline are missed, the workflow will be cancelled. concurrency_policy (CronConcurrencyPolicy, default=REPLACE) \u2014 The kubernetes concurrency policy to use. Check https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/cron-job-v1/ timezone (str, optional) \u2014 The timezone to use when scheduling the workflow. successful_jobs_history_limit (int, optional) \u2014 The limit of jobs (which have ended successfully) to keep in the history. failed_jobs_history_limit (int, optional) \u2014 The limit of jobs (which have ended with a failure) to keep in the history. extra_spec_options (Mapping[str, Any], default={}) \u2014 CronWorkflowSpec properties to set (if they are not used by the runtime). CronConcurrencyPolicy \u00b6 class dagger.runtime.argo.cron. CronConcurrencyPolicy ( value , names=None , module=None , qualname=None , type=None , start=1 ) Bases enum.Enum Concurrency policies allowed by Argo/Kubernetes. Docs: https://v1-20.docs.kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#concurrency-policy Classes EnumMeta \u2014 Metaclass for Enum class enum. EnumMeta ( cls , bases , classdict , **kwds ) Metaclass for Enum Attributes __members__ \u2014 Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. Methods __bool__ ( ) \u2014 classes/types should always be True. __call__ ( cls , value , names , module , qualname , type , start ) \u2014 Either returns an existing member, or creates a new enum class. __dir__ ( ) \u2014 Specialized dir implementation for types. __getattr__ ( cls , name ) \u2014 Return the enum member matching name __iter__ ( cls ) \u2014 Returns members in definition order. __reversed__ ( cls ) \u2014 Returns members in reverse definition order. __setattr__ ( cls , name , value ) \u2014 Block attempts to reassign Enum members. method __bool__ ( ) classes/types should always be True. staticmethod __call__ ( cls , value , names=None , module=None , qualname=None , type=None , start=1 ) Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. method __dir__ ( ) Specialized dir implementation for types. staticmethod __getattr__ ( cls , name ) Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. staticmethod __iter__ ( cls ) Returns members in definition order. staticmethod __reversed__ ( cls ) Returns members in reverse definition order. staticmethod __setattr__ ( cls , name , value ) Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. workflow_manifest \u00b6 function dagger.runtime.argo.v1alpha1. workflow_manifest ( dag , metadata , workflow ) Return a minimal representation of a Workflow to execute the supplied DAG with the specified metadata. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#workflow Parameters dag (DAG) \u2014 The DAG to convert into an Argo Workflow metadata (Metadata) \u2014 Kubernetes metadata (name, namespace, labels, ...) to inject to the workflow workflow (Workflow) \u2014 Workflow configuration (parameters, container image and entrypoint, ...) Raises ValueError \u2014 If any of the extra_spec_options collides with a property used by the runtime. workflow_template_manifest \u00b6 function dagger.runtime.argo.v1alpha1. workflow_template_manifest ( dag , metadata , workflow ) Return a minimal representation of a WorkflowTemplate to execute the supplied DAG with the specified metadata. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#workflowtemplate To see the parameters required and exceptions raised by this function, please refer to the workflow_manifest function. cluster_workflow_template_manifest \u00b6 function dagger.runtime.argo.v1alpha1. cluster_workflow_template_manifest ( dag , metadata , workflow ) Return a minimal representation of a ClusterWorkflowTemplate to execute the supplied DAG with the specified metadata. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#workflowtemplate To see the parameters required and exceptions raised by this function, please refer to the workflow_manifest function. cron_workflow_manifest \u00b6 function dagger.runtime.argo.v1alpha1. cron_workflow_manifest ( dag , metadata , workflow , cron ) Return a minimal representation of a CronWorkflow to execute the supplied DAG with the specified metadata and scheduling parameters. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#cronworkflow Parameters dag (DAG) \u2014 The DAG to convert into an Argo Workflow metadata (Metadata) \u2014 Kubernetes metadata (name, namespace, labels, ...) to inject to the workflow workflow (Workflow) \u2014 Workflow configuration (parameters, container image and entrypoint, ...) cron (Cron) \u2014 Cron configuration (schedule, concurrency, ...) Raises ValueError \u2014 If any of the extra_spec_options collides with a property used by the runtime.","title":"dagger.runtime.argo"},{"location":"api/runtime-argo/#daggerruntimeargo","text":"Compile DAGs into manifests that will run on top of Argo Workflows . This runtime produces Python mappings (dictionaries) that contain all the necessary Kubernetes resources for Argo Workflows to execute the supplied DAG. Please note that this runtime doesn't produce JSON/YAML files. Users will still need to take the manifests generated by this runtime and apply them to a Kubernetes cluster. This runtime requires a container image and entrypoint to be specified, and assumes the supplied entrypoint exposes the same DAG that was used to generate the manifests through the CLI runtime. Please check the Argo runtime user guide for more details.","title":"dagger.runtime.argo"},{"location":"api/runtime-argo/#top-level-objects-exported","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \"\"\" Compile DAGs into manifests that will run on top of [Argo Workflows](https://argoproj.github.io/projects/argo). This runtime produces Python mappings (dictionaries) that contain all the necessary Kubernetes resources for Argo Workflows to execute the supplied DAG. Please note that this runtime doesn't produce JSON/YAML files. Users will still need to take the manifests generated by this runtime and apply them to a Kubernetes cluster. This runtime requires a container image and entrypoint to be specified, and assumes the supplied entrypoint exposes the same DAG that was used to generate the manifests through the CLI runtime. Please check the Argo runtime user guide for more details. \"\"\" from dagger.runtime.argo.cron import Cron , CronConcurrencyPolicy # noqa from dagger.runtime.argo.metadata import Metadata # noqa from dagger.runtime.argo.v1alpha1 import ( # noqa cluster_workflow_template_manifest , cron_workflow_manifest , workflow_manifest , workflow_template_manifest , ) from dagger.runtime.argo.workflow import Workflow # noqa","title":"Top-level objects exported"},{"location":"api/runtime-argo/#metadata","text":"class dagger.runtime.argo.metadata. Metadata ( name , generate_name_from_prefix=False , namespace=None , annotations=None , labels=None ) Metadata that may be provided for an Argo CRD. Attributes annotations \u2014 Return the annotation for the object's metadata. generate_name_from_prefix (bool) \u2014 Return a flag indicating whether the supplied name should be generated. labels \u2014 Return the labels for the object's metadata. name (str) \u2014 Return the name specified. namespace (str, optional) \u2014 Return the object's namespace, if any. Methods __eq__ ( obj ) (bool) \u2014 Return true if the supplied object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of the object. method __repr__ ( ) \u2192 str Return a human-readable representation of the object. method __eq__ ( obj ) \u2192 bool Return true if the supplied object is equivalent to the current instance.","title":"Metadata"},{"location":"api/runtime-argo/#metadata-initialization","text":"method __init__ ( name , generate_name_from_prefix=False , namespace=None , annotations=None , labels=None ) Initialize an object's Kubernetes metadata. Parameters name (str) \u2014 The name of the object generate_name_from_prefix (bool, optional) \u2014 Whether or not the supplied name should be used as a prefix of the actual name (which would be autogenerated). namespace (str, optional) \u2014 The namespace where the object should exist. annotations (Mapping[str, str], default={}) \u2014 A mapping of annotation keys and values to attach to the object. labels (Mapping[str, str], default={}) \u2014 A mapping of label keys and values to attach to the object.","title":"Metadata Initialization"},{"location":"api/runtime-argo/#workflow","text":"class dagger.runtime.argo.workflow. Workflow ( container_image , container_entrypoint_to_dag_cli=None , params=None , extra_spec_options=None ) Configuration for a Workflow. This class will be supplied to the runtime to help it create a workflow that will work on your environment. Attributes container_entrypoint_to_dag_cli (list of str) \u2014 Return the container entrypoint that will be used in order to run every step in this workflow. The entrypoint must expose the CLI runtime for the same DAG that was used to generate the Argo manifests. container_image (str) \u2014 Return the container image that will be used to run every step in this workflow. extra_spec_options \u2014 Return any extra options that should be passed to the WorkflowSpec. params \u2014 Return the parameters to supply to the workflow or workflow template. Methods __eq__ ( obj ) (bool) \u2014 Return true if the object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of this instance. method __repr__ ( ) \u2192 str Return a human-readable representation of this instance. method __eq__ ( obj ) \u2192 bool Return true if the object is equivalent to the current instance.","title":"Workflow"},{"location":"api/runtime-argo/#workflow-initialization","text":"method __init__ ( container_image , container_entrypoint_to_dag_cli=None , params=None , extra_spec_options=None ) Create a workflow configuration. Parameters container_image (str) \u2014 The URI to the container image Argo will use for every node in the DAG. container_entrypoint_to_dag_cli (List[str], default=[]) \u2014 The container entrypoint Argo will use for every node in the DAG. The entrypoint must expose the CLI runtime for the same DAG that was used to generate the Argo manifests. Check the section about the Argo runtime to understand better how to containerize your projects and expose your DAGs. params (Mapping[str, Any], default={}) \u2014 Parameters to inject to the DAG. They must match the inputs the DAG expects. extra_spec_options, Mapping[str, Any], default={} \u2014 WorkflowSpec properties to set (if they are not used by the runtime).","title":"Workflow Initialization"},{"location":"api/runtime-argo/#cron","text":"class dagger.runtime.argo.cron. Cron ( schedule , starting_deadline_seconds=0 , concurrency_policy=<CronConcurrencyPolicy.ALLOW: 'Allow'> , timezone=None , successful_jobs_history_limit=None , failed_jobs_history_limit=None , extra_spec_options=None ) Scheduling options for the cron job. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#cronworkflowspec Attributes concurrency_policy (CronConcurrencyPolicy) \u2014 Return the concurrency policy for this cron workflow. extra_spec_options \u2014 Return any extra options that should be passed to the Cron spec. failed_jobs_history_limit (int, optional) \u2014 Return the limit of failed jobs to keep a historical record for, if defined. schedule (str) \u2014 Return the schedule for this cron workflow. starting_deadline_seconds (int) \u2014 Return the starting deadline for this cron workflow, in seconds. successful_jobs_history_limit (int, optional) \u2014 Return the limit of successful jobs to keep a historical record for, if defined. timezone (str, optional) \u2014 Return the timezone this cron workflow should be scheduled according to, if any. Methods __eq__ ( obj ) (bool) \u2014 Return true if the object is equivalent to the current instance. __repr__ ( ) (str) \u2014 Return a human-readable representation of this instance. method __repr__ ( ) \u2192 str Return a human-readable representation of this instance. method __eq__ ( obj ) \u2192 bool Return true if the object is equivalent to the current instance.","title":"Cron"},{"location":"api/runtime-argo/#cron-initialization","text":"method __init__ ( schedule , starting_deadline_seconds=0 , concurrency_policy=<CronConcurrencyPolicy.ALLOW: 'Allow'> , timezone=None , successful_jobs_history_limit=None , failed_jobs_history_limit=None , extra_spec_options=None ) Initialize the configuration for a cron workflow. Parameters schedule (str) \u2014 The schedule the workflow should run according to. starting_deadline_seconds (int, default=0) \u2014 The time to wait for a scheduled workflow to start. If the original schedule + deadline are missed, the workflow will be cancelled. concurrency_policy (CronConcurrencyPolicy, default=REPLACE) \u2014 The kubernetes concurrency policy to use. Check https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/cron-job-v1/ timezone (str, optional) \u2014 The timezone to use when scheduling the workflow. successful_jobs_history_limit (int, optional) \u2014 The limit of jobs (which have ended successfully) to keep in the history. failed_jobs_history_limit (int, optional) \u2014 The limit of jobs (which have ended with a failure) to keep in the history. extra_spec_options (Mapping[str, Any], default={}) \u2014 CronWorkflowSpec properties to set (if they are not used by the runtime).","title":"Cron Initialization"},{"location":"api/runtime-argo/#cronconcurrencypolicy","text":"class dagger.runtime.argo.cron. CronConcurrencyPolicy ( value , names=None , module=None , qualname=None , type=None , start=1 ) Bases enum.Enum Concurrency policies allowed by Argo/Kubernetes. Docs: https://v1-20.docs.kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#concurrency-policy Classes EnumMeta \u2014 Metaclass for Enum class enum. EnumMeta ( cls , bases , classdict , **kwds ) Metaclass for Enum Attributes __members__ \u2014 Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. Methods __bool__ ( ) \u2014 classes/types should always be True. __call__ ( cls , value , names , module , qualname , type , start ) \u2014 Either returns an existing member, or creates a new enum class. __dir__ ( ) \u2014 Specialized dir implementation for types. __getattr__ ( cls , name ) \u2014 Return the enum member matching name __iter__ ( cls ) \u2014 Returns members in definition order. __reversed__ ( cls ) \u2014 Returns members in reverse definition order. __setattr__ ( cls , name , value ) \u2014 Block attempts to reassign Enum members. method __bool__ ( ) classes/types should always be True. staticmethod __call__ ( cls , value , names=None , module=None , qualname=None , type=None , start=1 ) Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. method __dir__ ( ) Specialized dir implementation for types. staticmethod __getattr__ ( cls , name ) Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. staticmethod __iter__ ( cls ) Returns members in definition order. staticmethod __reversed__ ( cls ) Returns members in reverse definition order. staticmethod __setattr__ ( cls , name , value ) Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration.","title":"CronConcurrencyPolicy"},{"location":"api/runtime-argo/#workflow_manifest","text":"function dagger.runtime.argo.v1alpha1. workflow_manifest ( dag , metadata , workflow ) Return a minimal representation of a Workflow to execute the supplied DAG with the specified metadata. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#workflow Parameters dag (DAG) \u2014 The DAG to convert into an Argo Workflow metadata (Metadata) \u2014 Kubernetes metadata (name, namespace, labels, ...) to inject to the workflow workflow (Workflow) \u2014 Workflow configuration (parameters, container image and entrypoint, ...) Raises ValueError \u2014 If any of the extra_spec_options collides with a property used by the runtime.","title":"workflow_manifest"},{"location":"api/runtime-argo/#workflow_template_manifest","text":"function dagger.runtime.argo.v1alpha1. workflow_template_manifest ( dag , metadata , workflow ) Return a minimal representation of a WorkflowTemplate to execute the supplied DAG with the specified metadata. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#workflowtemplate To see the parameters required and exceptions raised by this function, please refer to the workflow_manifest function.","title":"workflow_template_manifest"},{"location":"api/runtime-argo/#cluster_workflow_template_manifest","text":"function dagger.runtime.argo.v1alpha1. cluster_workflow_template_manifest ( dag , metadata , workflow ) Return a minimal representation of a ClusterWorkflowTemplate to execute the supplied DAG with the specified metadata. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#workflowtemplate To see the parameters required and exceptions raised by this function, please refer to the workflow_manifest function.","title":"cluster_workflow_template_manifest"},{"location":"api/runtime-argo/#cron_workflow_manifest","text":"function dagger.runtime.argo.v1alpha1. cron_workflow_manifest ( dag , metadata , workflow , cron ) Return a minimal representation of a CronWorkflow to execute the supplied DAG with the specified metadata and scheduling parameters. Spec: https://github.com/argoproj/argo-workflows/blob/v3.0.4/docs/fields.md#cronworkflow Parameters dag (DAG) \u2014 The DAG to convert into an Argo Workflow metadata (Metadata) \u2014 Kubernetes metadata (name, namespace, labels, ...) to inject to the workflow workflow (Workflow) \u2014 Workflow configuration (parameters, container image and entrypoint, ...) cron (Cron) \u2014 Cron configuration (schedule, concurrency, ...) Raises ValueError \u2014 If any of the extra_spec_options collides with a property used by the runtime.","title":"cron_workflow_manifest"},{"location":"api/runtime-cli/","text":"package dagger.runtime. cli Run DAGs or Tasks taking their inputs from files and storing their outputs into files. It defines a Command-Line Interface through which users can specify all input/output locations, and optionally select a specific task to invoke. It runs all tasks/DAGs using the \"local\" runtime. Top-level objects exported \u00b6 1 2 3 4 5 6 7 8 9 10 11 \"\"\" Run DAGs or Tasks taking their inputs from files and storing their outputs into files. It defines a Command-Line Interface through which users can specify all input/output locations, and optionally select a specific task to invoke. It runs all tasks/DAGs using the \"local\" runtime. \"\"\" from dagger.runtime.cli.cli import invoke # noqa from dagger.runtime.cli.locations import PARTITION_MANIFEST_FILENAME # noqa invoke \u00b6 function dagger.runtime.cli.cli. invoke ( dag , argv=['build'] ) Invoke the supplied DAG (or a node therein) retrieving the inputs from, and storing the outputs into, the specified locations. You can run this command with the --help flag to get more details about all supported arguments. Here is a non-exhaustive list: --input <name> <location> -- Retrieve input of the DAG from --output <name> <location> -- Store output of the DAG into --node-name <name> (optional) -- Select a specific node of the DAG to run. If your DAG contains other nested DAGs you can access nodes using dot-notation (e.g. nested-dag-name.node-name) Parameters dag (DAG) \u2014 DAG to execute argv (List of str (by default, the system's CLI arguments)) \u2014 List of arguments expected by the Command-Line Interface of this runtime. Check the documentation or tun --help for more details. Raises SerializationError \u2014 When some of the outputs cannot be serialized with the specified Serializer TypeError \u2014 When any of the outputs cannot be obtained from the return value of their node ValueError \u2014 When the location of any required input/output is missing","title":"dagger.runtime.cli"},{"location":"api/runtime-cli/#daggerruntimecli","text":"Run DAGs or Tasks taking their inputs from files and storing their outputs into files. It defines a Command-Line Interface through which users can specify all input/output locations, and optionally select a specific task to invoke. It runs all tasks/DAGs using the \"local\" runtime.","title":"dagger.runtime.cli"},{"location":"api/runtime-cli/#top-level-objects-exported","text":"1 2 3 4 5 6 7 8 9 10 11 \"\"\" Run DAGs or Tasks taking their inputs from files and storing their outputs into files. It defines a Command-Line Interface through which users can specify all input/output locations, and optionally select a specific task to invoke. It runs all tasks/DAGs using the \"local\" runtime. \"\"\" from dagger.runtime.cli.cli import invoke # noqa from dagger.runtime.cli.locations import PARTITION_MANIFEST_FILENAME # noqa","title":"Top-level objects exported"},{"location":"api/runtime-cli/#invoke","text":"function dagger.runtime.cli.cli. invoke ( dag , argv=['build'] ) Invoke the supplied DAG (or a node therein) retrieving the inputs from, and storing the outputs into, the specified locations. You can run this command with the --help flag to get more details about all supported arguments. Here is a non-exhaustive list: --input <name> <location> -- Retrieve input of the DAG from --output <name> <location> -- Store output of the DAG into --node-name <name> (optional) -- Select a specific node of the DAG to run. If your DAG contains other nested DAGs you can access nodes using dot-notation (e.g. nested-dag-name.node-name) Parameters dag (DAG) \u2014 DAG to execute argv (List of str (by default, the system's CLI arguments)) \u2014 List of arguments expected by the Command-Line Interface of this runtime. Check the documentation or tun --help for more details. Raises SerializationError \u2014 When some of the outputs cannot be serialized with the specified Serializer TypeError \u2014 When any of the outputs cannot be obtained from the return value of their node ValueError \u2014 When the location of any required input/output is missing","title":"invoke"},{"location":"api/runtime-local/","text":"package dagger.runtime. local Run DAGs or nodes in memory. Top-level objects exported \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 \"\"\"Run DAGs or nodes in memory.\"\"\" from dagger.runtime.local.invoke import ( # noqa ReturnDeserializedOutputs , StoreSerializedOutputsInPath , invoke , ) from dagger.runtime.local.types import ( # noqa NodeOutput , NodeOutputs , OutputFile , PartitionedOutput , ) invoke \u00b6 module dagger.runtime.local. invoke Invoke a node and store/returns its outputs. Classes ReturnDeserializedOutputs \u2014 Indicates that the outputs of a node invoked with the local runtime should be returned in their deserialized format. StoreSerializedOutputsInPath \u2014 Indicates that the outputs of a node invoked with the local runtime should be stored in files in the local filesystem and the invocation should return pointers to those files. Functions invoke ( node , params , outputs ) (Serialized outputs of the task, indexed by output name.) \u2014 Invoke a node with a series of parameters. Types \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \"\"\"Data types used for local invocations.\"\"\" from typing import Any , Generic , Iterable , Iterator , Mapping , NamedTuple , TypeVar , Union from dagger.serializer import Serializer T = TypeVar ( \"T\" ) class OutputFile ( NamedTuple ): \"\"\"Represents a file in the local file system that holds the serialized value for a node output.\"\"\" filename : str serializer : Serializer class PartitionedOutput ( Generic [ T ]): \"\"\"Represents a partitioned output explicitly.\"\"\" def __init__ ( self , iterable : Iterable [ T ]): \"\"\"Build a partitioned output from an Iterable.\"\"\" self . _iterable = iterable self . _iterator = iter ( iterable ) def __iter__ ( self ) -> Iterator [ T ]: \"\"\"Return an iterator over the partitions of the output.\"\"\" return self def __next__ ( self ) -> T : \"\"\"Return the next element in the partitioned output.\"\"\" return next ( self . _iterator ) def __repr__ ( self ) -> str : \"\"\"Return a human-readable representation of the partitioned output.\"\"\" return repr ( self . _iterable ) #: One of the outputs of a node, which may be partitioned NodeOutput = Union [ OutputFile , PartitionedOutput [ OutputFile ]] #: All outputs of a node indexed by their name. Node executions may be partitioned, in which case this is a list. NodeOutputs = Mapping [ str , NodeOutput ] #: All executions of a node. If the node is partitioned there will only be one. Otherwise, there may be many. NodeExecutions = Union [ NodeOutputs , PartitionedOutput [ NodeOutputs ]] #: The parameters supplied to a node (plain, not serialized) NodeParams = Mapping [ str , Any ]","title":"dagger.runtime.local"},{"location":"api/runtime-local/#daggerruntimelocal","text":"Run DAGs or nodes in memory.","title":"dagger.runtime.local"},{"location":"api/runtime-local/#top-level-objects-exported","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 \"\"\"Run DAGs or nodes in memory.\"\"\" from dagger.runtime.local.invoke import ( # noqa ReturnDeserializedOutputs , StoreSerializedOutputsInPath , invoke , ) from dagger.runtime.local.types import ( # noqa NodeOutput , NodeOutputs , OutputFile , PartitionedOutput , )","title":"Top-level objects exported"},{"location":"api/runtime-local/#invoke","text":"module dagger.runtime.local. invoke Invoke a node and store/returns its outputs. Classes ReturnDeserializedOutputs \u2014 Indicates that the outputs of a node invoked with the local runtime should be returned in their deserialized format. StoreSerializedOutputsInPath \u2014 Indicates that the outputs of a node invoked with the local runtime should be stored in files in the local filesystem and the invocation should return pointers to those files. Functions invoke ( node , params , outputs ) (Serialized outputs of the task, indexed by output name.) \u2014 Invoke a node with a series of parameters.","title":"invoke"},{"location":"api/runtime-local/#types","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 \"\"\"Data types used for local invocations.\"\"\" from typing import Any , Generic , Iterable , Iterator , Mapping , NamedTuple , TypeVar , Union from dagger.serializer import Serializer T = TypeVar ( \"T\" ) class OutputFile ( NamedTuple ): \"\"\"Represents a file in the local file system that holds the serialized value for a node output.\"\"\" filename : str serializer : Serializer class PartitionedOutput ( Generic [ T ]): \"\"\"Represents a partitioned output explicitly.\"\"\" def __init__ ( self , iterable : Iterable [ T ]): \"\"\"Build a partitioned output from an Iterable.\"\"\" self . _iterable = iterable self . _iterator = iter ( iterable ) def __iter__ ( self ) -> Iterator [ T ]: \"\"\"Return an iterator over the partitions of the output.\"\"\" return self def __next__ ( self ) -> T : \"\"\"Return the next element in the partitioned output.\"\"\" return next ( self . _iterator ) def __repr__ ( self ) -> str : \"\"\"Return a human-readable representation of the partitioned output.\"\"\" return repr ( self . _iterable ) #: One of the outputs of a node, which may be partitioned NodeOutput = Union [ OutputFile , PartitionedOutput [ OutputFile ]] #: All outputs of a node indexed by their name. Node executions may be partitioned, in which case this is a list. NodeOutputs = Mapping [ str , NodeOutput ] #: All executions of a node. If the node is partitioned there will only be one. Otherwise, there may be many. NodeExecutions = Union [ NodeOutputs , PartitionedOutput [ NodeOutputs ]] #: The parameters supplied to a node (plain, not serialized) NodeParams = Mapping [ str , Any ]","title":"Types"},{"location":"api/serializer/","text":"package dagger. serializer Serialization strategies to pass inputs/outputs safely between tasks in a distributed environment. Top-level objects exported \u00b6 1 2 3 4 5 6 7 8 \"\"\"Serialization strategies to pass inputs/outputs safely between tasks in a distributed environment.\"\"\" from dagger.serializer.as_json import AsJSON # noqa from dagger.serializer.as_pickle import AsPickle # noqa from dagger.serializer.errors import DeserializationError , SerializationError # noqa from dagger.serializer.protocol import Serializer # noqa DefaultSerializer = AsJSON () Serializer Protocol \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \"\"\"Protocol all serializers should conform to.\"\"\" from typing import Any , BinaryIO , Protocol , runtime_checkable @runtime_checkable class Serializer ( Protocol ): # pragma: no cover \"\"\"Protocol all serializers should conform to.\"\"\" @property def extension ( self ) -> str : \"\"\"Extension to use for the files generated by this serializer.\"\"\" ... def serialize ( self , value : Any , writer : BinaryIO ): \"\"\"Serialize a value and write it to the provided writer stream.\"\"\" ... def deserialize ( self , reader : BinaryIO ) -> Any : \"\"\"Deserialize a stream of bytes into a value.\"\"\" ... AsJSON \u00b6 class dagger.serializer.as_json. AsJSON ( indent=None , allow_nan=False ) Serializer implementation that uses JSON to marshal/unmarshal Python data structures. Methods __eq__ ( obj ) (bool) \u2014 Return true if both serializers are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the serializer. deserialize ( reader ) (any) \u2014 Deserialize a utf-8-encoded json object into the value it represents. serialize ( value , writer ) \u2014 Serialize a value into a JSON object, encoded into binary format using utf-8. method serialize ( value , writer ) Serialize a value into a JSON object, encoded into binary format using utf-8. The value needs to be serializable into JSON by the standard 'json' library in Python. method deserialize ( reader ) \u2192 any Deserialize a utf-8-encoded json object into the value it represents. method __repr__ ( ) \u2192 str Get a human-readable string representation of the serializer. method __eq__ ( obj ) \u2192 bool Return true if both serializers are equivalent. Initialization \u00b6 method __init__ ( indent=None , allow_nan=False ) Initialize a JSON serializer. Parameters indent (int, optional) \u2014 Set the indentation to format the json with. This may come in handy in some situations if you need to debug/troubleshoot an issue with parameters that are passed from one task to another. However, note that extra indentation makes the serialized payload heavier. Thus, we don't recommend setting this in a production environment. allow_nan (bool) \u2014 Whether or not to allow NaN values. See the official json library in Python for more details about the expected behavior. AsPickle \u00b6 class dagger.serializer.as_pickle. AsPickle ( ) Serializer implementation that uses Pickle to marshal/unmarshal Python data structures. Reference: https://docs.python.org/3/library/pickle.html Methods __eq__ ( obj ) (bool) \u2014 Return true if both serializers are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the serializer. deserialize ( reader ) (any) \u2014 Deserialize a pickled object into the value it represents. serialize ( value , writer ) \u2014 Serialize a value using the Pickle protocol. method serialize ( value , writer ) Serialize a value using the Pickle protocol. method deserialize ( reader ) \u2192 any Deserialize a pickled object into the value it represents. method __repr__ ( ) \u2192 str Get a human-readable string representation of the serializer. method __eq__ ( obj ) \u2192 bool Return true if both serializers are equivalent.","title":"dagger.serializer"},{"location":"api/serializer/#daggerserializer","text":"Serialization strategies to pass inputs/outputs safely between tasks in a distributed environment.","title":"dagger.serializer"},{"location":"api/serializer/#top-level-objects-exported","text":"1 2 3 4 5 6 7 8 \"\"\"Serialization strategies to pass inputs/outputs safely between tasks in a distributed environment.\"\"\" from dagger.serializer.as_json import AsJSON # noqa from dagger.serializer.as_pickle import AsPickle # noqa from dagger.serializer.errors import DeserializationError , SerializationError # noqa from dagger.serializer.protocol import Serializer # noqa DefaultSerializer = AsJSON ()","title":"Top-level objects exported"},{"location":"api/serializer/#serializer-protocol","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \"\"\"Protocol all serializers should conform to.\"\"\" from typing import Any , BinaryIO , Protocol , runtime_checkable @runtime_checkable class Serializer ( Protocol ): # pragma: no cover \"\"\"Protocol all serializers should conform to.\"\"\" @property def extension ( self ) -> str : \"\"\"Extension to use for the files generated by this serializer.\"\"\" ... def serialize ( self , value : Any , writer : BinaryIO ): \"\"\"Serialize a value and write it to the provided writer stream.\"\"\" ... def deserialize ( self , reader : BinaryIO ) -> Any : \"\"\"Deserialize a stream of bytes into a value.\"\"\" ...","title":"Serializer Protocol"},{"location":"api/serializer/#asjson","text":"class dagger.serializer.as_json. AsJSON ( indent=None , allow_nan=False ) Serializer implementation that uses JSON to marshal/unmarshal Python data structures. Methods __eq__ ( obj ) (bool) \u2014 Return true if both serializers are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the serializer. deserialize ( reader ) (any) \u2014 Deserialize a utf-8-encoded json object into the value it represents. serialize ( value , writer ) \u2014 Serialize a value into a JSON object, encoded into binary format using utf-8. method serialize ( value , writer ) Serialize a value into a JSON object, encoded into binary format using utf-8. The value needs to be serializable into JSON by the standard 'json' library in Python. method deserialize ( reader ) \u2192 any Deserialize a utf-8-encoded json object into the value it represents. method __repr__ ( ) \u2192 str Get a human-readable string representation of the serializer. method __eq__ ( obj ) \u2192 bool Return true if both serializers are equivalent.","title":"AsJSON"},{"location":"api/serializer/#initialization","text":"method __init__ ( indent=None , allow_nan=False ) Initialize a JSON serializer. Parameters indent (int, optional) \u2014 Set the indentation to format the json with. This may come in handy in some situations if you need to debug/troubleshoot an issue with parameters that are passed from one task to another. However, note that extra indentation makes the serialized payload heavier. Thus, we don't recommend setting this in a production environment. allow_nan (bool) \u2014 Whether or not to allow NaN values. See the official json library in Python for more details about the expected behavior.","title":"Initialization"},{"location":"api/serializer/#aspickle","text":"class dagger.serializer.as_pickle. AsPickle ( ) Serializer implementation that uses Pickle to marshal/unmarshal Python data structures. Reference: https://docs.python.org/3/library/pickle.html Methods __eq__ ( obj ) (bool) \u2014 Return true if both serializers are equivalent. __repr__ ( ) (str) \u2014 Get a human-readable string representation of the serializer. deserialize ( reader ) (any) \u2014 Deserialize a pickled object into the value it represents. serialize ( value , writer ) \u2014 Serialize a value using the Pickle protocol. method serialize ( value , writer ) Serialize a value using the Pickle protocol. method deserialize ( reader ) \u2192 any Deserialize a pickled object into the value it represents. method __repr__ ( ) \u2192 str Get a human-readable string representation of the serializer. method __eq__ ( obj ) \u2192 bool Return true if both serializers are equivalent.","title":"AsPickle"},{"location":"api/task/","text":"package dagger. task Define a Task that runs a specific function inside of a DAG. Top-level objects exported \u00b6 1 2 3 \"\"\"Define a Task that runs a specific function inside of a DAG.\"\"\" from dagger.task.task import SupportedInputs , SupportedOutputs , Task # noqa Task \u00b6 class dagger.task.task. Task ( func , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) A task that executes a given function taking inputs from the specified sources and producing the specified outputs. Attributes func \u2014 Get the function associated with the Task. inputs \u2014 Get the inputs the Task expects. outputs \u2014 Get the outputs the task produces. partition_by_input (str, optional) \u2014 Return the input this task should be partitioned by, if any. runtime_options \u2014 Get the specified runtime options. Methods __eq__ ( obj ) (bool) \u2014 Return true if the two tasks are equivalent to each other. __repr__ ( ) (str) \u2014 Return a human-readable representation of the task. method __eq__ ( obj ) \u2192 bool Return true if the two tasks are equivalent to each other. method __repr__ ( ) \u2192 str Return a human-readable representation of the task. Task Initialization \u00b6 method __init__ ( func , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Validate and initialize a Task. Parameters func (Callable) \u2014 The Python function for the Task to execute inputs (Mapping[str, SupportedInputs], default={}) \u2014 A mapping from input names to Task inputs. Only certain types are allowed as inputs. outputs (Mapping[str, SupportedOutputs], default={}) \u2014 A mapping from output names to Task outputs. Outputs must be retrievable from the function's outputs. runtime_options (Mapping[str, Any], default={}) \u2014 A mapping of options to supply to all runtimes. This allows you to take full advantage of the features of each runtime. For instance, you can use it to manipulate node affinities and tolerations in Kubernetes. Check the documentation of each runtime to see potential options. partition_by_input (str, optional) \u2014 If specified, it signals the task should be run as many times as partitions in the specified input. Each of the executions will only receive one of the partitions of that input. Raises TypeError \u2014 If any of the inputs/outputs is not supported. If inputs do not match the arguments of the function. ValueError \u2014 If the names of the inputs/outputs have unsupported characters. If the partition_by field doesn't link to a valid input.","title":"dagger.task"},{"location":"api/task/#daggertask","text":"Define a Task that runs a specific function inside of a DAG.","title":"dagger.task"},{"location":"api/task/#top-level-objects-exported","text":"1 2 3 \"\"\"Define a Task that runs a specific function inside of a DAG.\"\"\" from dagger.task.task import SupportedInputs , SupportedOutputs , Task # noqa","title":"Top-level objects exported"},{"location":"api/task/#task","text":"class dagger.task.task. Task ( func , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) A task that executes a given function taking inputs from the specified sources and producing the specified outputs. Attributes func \u2014 Get the function associated with the Task. inputs \u2014 Get the inputs the Task expects. outputs \u2014 Get the outputs the task produces. partition_by_input (str, optional) \u2014 Return the input this task should be partitioned by, if any. runtime_options \u2014 Get the specified runtime options. Methods __eq__ ( obj ) (bool) \u2014 Return true if the two tasks are equivalent to each other. __repr__ ( ) (str) \u2014 Return a human-readable representation of the task. method __eq__ ( obj ) \u2192 bool Return true if the two tasks are equivalent to each other. method __repr__ ( ) \u2192 str Return a human-readable representation of the task.","title":"Task"},{"location":"api/task/#task-initialization","text":"method __init__ ( func , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Validate and initialize a Task. Parameters func (Callable) \u2014 The Python function for the Task to execute inputs (Mapping[str, SupportedInputs], default={}) \u2014 A mapping from input names to Task inputs. Only certain types are allowed as inputs. outputs (Mapping[str, SupportedOutputs], default={}) \u2014 A mapping from output names to Task outputs. Outputs must be retrievable from the function's outputs. runtime_options (Mapping[str, Any], default={}) \u2014 A mapping of options to supply to all runtimes. This allows you to take full advantage of the features of each runtime. For instance, you can use it to manipulate node affinities and tolerations in Kubernetes. Check the documentation of each runtime to see potential options. partition_by_input (str, optional) \u2014 If specified, it signals the task should be run as many times as partitions in the specified input. Each of the executions will only receive one of the partitions of that input. Raises TypeError \u2014 If any of the inputs/outputs is not supported. If inputs do not match the arguments of the function. ValueError \u2014 If the names of the inputs/outputs have unsupported characters. If the partition_by field doesn't link to a valid input.","title":"Task Initialization"},{"location":"user-guide/dag-composition/","text":"DAG Composition \u00b6 In Dagger , you can invoke DAGs inside of other DAGs. This feature allows you to design complex data pipelines and keep each group of steps clean and testable . \ud83d\udca1 Example \u00b6 Let's say we are modelling an ETL pipeline where we want to fetch a raw dataset and apply some transformations to it. The list of transformations we apply may be arbitrarily complex and we want to keep track of each transformation separately, so we wrap each of them in a separate task. In order to make our DAGs simpler and easier to test, we have decided to decouple the steps that transform the dataset from the steps that deal with database access, so we have created 2 different DAGs and invoked one from the other. As your DAGs evolve and become more sophisticated, DAG composition will become one of your go-to features of the library. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from typing import List from dagger import dsl DataSet = str # # DAG that processes and transforms a dataset # =========================================== # @dsl . task () def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" @dsl . task () def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" @dsl . task () def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" @dsl . DAG () def transform_dataset ( dataset : DataSet ): ds_1 = encode_field_a ( dataset ) ds_2 = aggregate_fields_b_and_c ( ds_1 ) return calculate_moving_average_for_d ( ds_2 ) # # DAG that retrieves the dataset and invokes # the previous DAG for each chunk. # ========================================== # @dsl . task () def retrieve_dataset () -> DataSet : return \"original dataset\" @dsl . DAG () def dag (): dataset = retrieve_dataset () return transform_dataset ( dataset ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 from dagger import DAG , FromNodeOutput , FromParam , FromReturnValue , Task DataSet = str # # DAG that processes and transforms a dataset # =========================================== # def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" def dataset_transformation_dag ( dataset_input : FromNodeOutput ) -> DAG : return DAG ( inputs = { \"dataset\" : dataset_input }, nodes = { \"encode-field-a\" : Task ( encode_field_a , inputs = { \"dataset\" : FromParam ( \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"aggregate-fields-b-and-c\" : Task ( aggregate_fields_b_and_c , inputs = { \"dataset\" : FromNodeOutput ( \"encode-field-a\" , \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"calculate-moving-average-for-d\" : Task ( calculate_moving_average_for_d , inputs = { \"dataset\" : FromNodeOutput ( \"aggregate-fields-b-and-c\" , \"dataset\" ) }, outputs = { \"dataset\" : FromReturnValue ()}, ), }, outputs = { \"dataset\" : FromNodeOutput ( \"calculate-moving-average-for-d\" , \"dataset\" ) }, ) # # DAG that retrieves the dataset and invokes the previous DAG for each chunk # ========================================================================== # def retrieve_dataset () -> DataSet : return \"original dataset\" dag = DAG ( nodes = { \"retrieve-dataset\" : Task ( retrieve_dataset , outputs = { \"dataset\" : FromReturnValue ()}, ), \"transform-dataset\" : dataset_transformation_dag ( FromNodeOutput ( \"retrieve-dataset\" , \"dataset\" ), ), }, outputs = { \"dataset\" : FromNodeOutput ( \"transform-dataset\" , \"dataset\" ), }, ) \ud83e\udde0 Learn more about... \u00b6 How to work with partitioned outputs and nodes .","title":"DAG Composition"},{"location":"user-guide/dag-composition/#dag-composition","text":"In Dagger , you can invoke DAGs inside of other DAGs. This feature allows you to design complex data pipelines and keep each group of steps clean and testable .","title":"DAG Composition"},{"location":"user-guide/dag-composition/#example","text":"Let's say we are modelling an ETL pipeline where we want to fetch a raw dataset and apply some transformations to it. The list of transformations we apply may be arbitrarily complex and we want to keep track of each transformation separately, so we wrap each of them in a separate task. In order to make our DAGs simpler and easier to test, we have decided to decouple the steps that transform the dataset from the steps that deal with database access, so we have created 2 different DAGs and invoked one from the other. As your DAGs evolve and become more sophisticated, DAG composition will become one of your go-to features of the library. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from typing import List from dagger import dsl DataSet = str # # DAG that processes and transforms a dataset # =========================================== # @dsl . task () def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" @dsl . task () def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" @dsl . task () def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" @dsl . DAG () def transform_dataset ( dataset : DataSet ): ds_1 = encode_field_a ( dataset ) ds_2 = aggregate_fields_b_and_c ( ds_1 ) return calculate_moving_average_for_d ( ds_2 ) # # DAG that retrieves the dataset and invokes # the previous DAG for each chunk. # ========================================== # @dsl . task () def retrieve_dataset () -> DataSet : return \"original dataset\" @dsl . DAG () def dag (): dataset = retrieve_dataset () return transform_dataset ( dataset ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 from dagger import DAG , FromNodeOutput , FromParam , FromReturnValue , Task DataSet = str # # DAG that processes and transforms a dataset # =========================================== # def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" def dataset_transformation_dag ( dataset_input : FromNodeOutput ) -> DAG : return DAG ( inputs = { \"dataset\" : dataset_input }, nodes = { \"encode-field-a\" : Task ( encode_field_a , inputs = { \"dataset\" : FromParam ( \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"aggregate-fields-b-and-c\" : Task ( aggregate_fields_b_and_c , inputs = { \"dataset\" : FromNodeOutput ( \"encode-field-a\" , \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"calculate-moving-average-for-d\" : Task ( calculate_moving_average_for_d , inputs = { \"dataset\" : FromNodeOutput ( \"aggregate-fields-b-and-c\" , \"dataset\" ) }, outputs = { \"dataset\" : FromReturnValue ()}, ), }, outputs = { \"dataset\" : FromNodeOutput ( \"calculate-moving-average-for-d\" , \"dataset\" ) }, ) # # DAG that retrieves the dataset and invokes the previous DAG for each chunk # ========================================================================== # def retrieve_dataset () -> DataSet : return \"original dataset\" dag = DAG ( nodes = { \"retrieve-dataset\" : Task ( retrieve_dataset , outputs = { \"dataset\" : FromReturnValue ()}, ), \"transform-dataset\" : dataset_transformation_dag ( FromNodeOutput ( \"retrieve-dataset\" , \"dataset\" ), ), }, outputs = { \"dataset\" : FromNodeOutput ( \"transform-dataset\" , \"dataset\" ), }, )","title":"\ud83d\udca1 Example"},{"location":"user-guide/dag-composition/#learn-more-about","text":"How to work with partitioned outputs and nodes .","title":"\ud83e\udde0 Learn more about..."},{"location":"user-guide/dags/","text":"DAGs \u00b6 Directed Acyclic Graphs (DAGs) describe how multiple nodes are connected to each other . Nodes in a DAG may be either Tasks or other DAGs (that is, you are allowed to nest DAGs inside of other DAGs). Nodes are connected to each other through their inputs and outputs. If a node x has an input that depends on the output of another node y , Dagger will register a dependency of y -> x , and runtimes will execute x before y according to their topological ordering . All dependencies in Dagger are defined implicitly by the relationship between the nodes' inputs and outputs . It is not possible to declare a dependency explicitly. If two nodes do not depend on each other's outputs, Dagger will assume they can be executed independently, potentially in parallel. Like tasks, DAGs have inputs and outputs: A DAG input may come from a parameter, or from the output of a sibling node. A DAG output must come from the output of one of its nodes . \ud83d\udca1 Example \u00b6 The following code snippet describes a simple DAG that mocks a simplified training pipeline for a Machine Learning model. The DAG has 3 tasks: A first task prepares the training and test datasets for the model (note how the task has 2 outputs). A second task trains the model based on the training dataset. A third task measures the performance of the trained model against the test dataset. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dagger import dsl @dsl . task () def prepare_datasets ( sample_size ): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } @dsl . task () def train_model ( training_dataset ): return \"trained model\" @dsl . task () def measure_model_performance ( model , test_dataset ): return \"performance report\" @dsl . DAG () def dag ( sample_size ): datasets = prepare_datasets ( sample_size ) model = train_model ( datasets [ \"training\" ]) return measure_model_performance ( model , datasets [ \"test\" ]) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from dagger import DAG , FromKey , FromNodeOutput , FromParam , FromReturnValue , Task def prepare_datasets ( sample_size ): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } def train_model ( training_dataset ): return \"trained model\" def measure_model_performance ( model , test_dataset ): return \"performance report\" dag = DAG ( inputs = { \"sample_size\" : FromParam (), }, outputs = { \"performance_report\" : FromNodeOutput ( \"measure-model-performance\" , \"performance_report\" ), }, nodes = { \"prepare-datasets\" : Task ( prepare_datasets , inputs = { \"sample_size\" : FromParam ( \"sample_size\" ), }, outputs = { \"training_dataset\" : FromKey ( \"training\" ), \"test_dataset\" : FromKey ( \"test\" ), }, ), \"train-model\" : Task ( train_model , inputs = { \"training_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"training_dataset\" ), }, outputs = { \"model\" : FromReturnValue (), }, ), \"measure-model-performance\" : Task ( measure_model_performance , inputs = { \"model\" : FromNodeOutput ( \"train-model\" , \"model\" ), \"test_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"test_dataset\" ), }, outputs = { \"performance_report\" : FromReturnValue (), }, ), }, ) \u27a1\ufe0f Inputs \u00b6 A DAG can have multiple inputs. Inputs can come: FromParam ( name : str ) . This indicates the input comes from a parameter named name , supplied when the DAG is executed. FromNodeOutput ( node : str , output : str ) . This indicates the input comes from an output named output , which comes from another node named node . The current DAG and the node must be siblings in the same parent DAG. \u2b05\ufe0f Outputs \u00b6 A DAG does not produce outputs of its own. Instead, it just exposes the output(s) of some of its nodes. Thus, outputs are always of type: FromNodeOutput ( node : str , output : str ) , where node points to the name of one of the DAG's nodes, and output to the name of the output to expose from that node. \u26d4 Limitations \u00b6 DAGs are validated against the following rules: DAGs should have at least one node. There would be no point in having an empty DAG. Node names must be 1-64 characters long, begin by a letter or number, and only contain letters, numbers and hyphens. Input and output names have the same constraints as node names, but they can also contain underscores. If a node input comes FromParam ( name : str ) , then the DAG must have an input named name . If a node input comes FromNodeOutput ( node : str , output : str ) , then the DAG must contain a node named node which exposes an output named output . DAGs may not contain cyclic dependencies. A cyclic dependency occurs whenever there is a chain of nodes {n(1), n(2), ..., n(last)} so that every n(i) depends on an output of n(i-1) , but n(1) also contains an input that depends on the output of n(last) . \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about the specific options supported by the DAG class. \ud83e\udde0 Learn more about... \u00b6 How to compose DAGs (that is, nest DAGs inside of other DAGs). How to work with partitioned outputs and nodes . How to run your DAGs with the different runtimes .","title":"DAGs"},{"location":"user-guide/dags/#dags","text":"Directed Acyclic Graphs (DAGs) describe how multiple nodes are connected to each other . Nodes in a DAG may be either Tasks or other DAGs (that is, you are allowed to nest DAGs inside of other DAGs). Nodes are connected to each other through their inputs and outputs. If a node x has an input that depends on the output of another node y , Dagger will register a dependency of y -> x , and runtimes will execute x before y according to their topological ordering . All dependencies in Dagger are defined implicitly by the relationship between the nodes' inputs and outputs . It is not possible to declare a dependency explicitly. If two nodes do not depend on each other's outputs, Dagger will assume they can be executed independently, potentially in parallel. Like tasks, DAGs have inputs and outputs: A DAG input may come from a parameter, or from the output of a sibling node. A DAG output must come from the output of one of its nodes .","title":"DAGs"},{"location":"user-guide/dags/#example","text":"The following code snippet describes a simple DAG that mocks a simplified training pipeline for a Machine Learning model. The DAG has 3 tasks: A first task prepares the training and test datasets for the model (note how the task has 2 outputs). A second task trains the model based on the training dataset. A third task measures the performance of the trained model against the test dataset. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dagger import dsl @dsl . task () def prepare_datasets ( sample_size ): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } @dsl . task () def train_model ( training_dataset ): return \"trained model\" @dsl . task () def measure_model_performance ( model , test_dataset ): return \"performance report\" @dsl . DAG () def dag ( sample_size ): datasets = prepare_datasets ( sample_size ) model = train_model ( datasets [ \"training\" ]) return measure_model_performance ( model , datasets [ \"test\" ]) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from dagger import DAG , FromKey , FromNodeOutput , FromParam , FromReturnValue , Task def prepare_datasets ( sample_size ): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } def train_model ( training_dataset ): return \"trained model\" def measure_model_performance ( model , test_dataset ): return \"performance report\" dag = DAG ( inputs = { \"sample_size\" : FromParam (), }, outputs = { \"performance_report\" : FromNodeOutput ( \"measure-model-performance\" , \"performance_report\" ), }, nodes = { \"prepare-datasets\" : Task ( prepare_datasets , inputs = { \"sample_size\" : FromParam ( \"sample_size\" ), }, outputs = { \"training_dataset\" : FromKey ( \"training\" ), \"test_dataset\" : FromKey ( \"test\" ), }, ), \"train-model\" : Task ( train_model , inputs = { \"training_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"training_dataset\" ), }, outputs = { \"model\" : FromReturnValue (), }, ), \"measure-model-performance\" : Task ( measure_model_performance , inputs = { \"model\" : FromNodeOutput ( \"train-model\" , \"model\" ), \"test_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"test_dataset\" ), }, outputs = { \"performance_report\" : FromReturnValue (), }, ), }, )","title":"\ud83d\udca1 Example"},{"location":"user-guide/dags/#inputs","text":"A DAG can have multiple inputs. Inputs can come: FromParam ( name : str ) . This indicates the input comes from a parameter named name , supplied when the DAG is executed. FromNodeOutput ( node : str , output : str ) . This indicates the input comes from an output named output , which comes from another node named node . The current DAG and the node must be siblings in the same parent DAG.","title":"\u27a1\ufe0f Inputs"},{"location":"user-guide/dags/#outputs","text":"A DAG does not produce outputs of its own. Instead, it just exposes the output(s) of some of its nodes. Thus, outputs are always of type: FromNodeOutput ( node : str , output : str ) , where node points to the name of one of the DAG's nodes, and output to the name of the output to expose from that node.","title":"\u2b05\ufe0f Outputs"},{"location":"user-guide/dags/#limitations","text":"DAGs are validated against the following rules: DAGs should have at least one node. There would be no point in having an empty DAG. Node names must be 1-64 characters long, begin by a letter or number, and only contain letters, numbers and hyphens. Input and output names have the same constraints as node names, but they can also contain underscores. If a node input comes FromParam ( name : str ) , then the DAG must have an input named name . If a node input comes FromNodeOutput ( node : str , output : str ) , then the DAG must contain a node named node which exposes an output named output . DAGs may not contain cyclic dependencies. A cyclic dependency occurs whenever there is a chain of nodes {n(1), n(2), ..., n(last)} so that every n(i) depends on an output of n(i-1) , but n(1) also contains an input that depends on the output of n(last) .","title":"\u26d4 Limitations"},{"location":"user-guide/dags/#api-reference","text":"Check the API Reference for more details about the specific options supported by the DAG class.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/dags/#learn-more-about","text":"How to compose DAGs (that is, nest DAGs inside of other DAGs). How to work with partitioned outputs and nodes . How to run your DAGs with the different runtimes .","title":"\ud83e\udde0 Learn more about..."},{"location":"user-guide/dsl/","text":"Imperative DSL \u00b6 If you've been following along the examples in the previous sections, you will have noticed there are two ways of defining your DAGs with Dagger : An imperative DSL and the (declarative) data structures. The core data structures are the foundation of Dagger (they are the language runtimes understand). However, as DAGs become more complex, the data structures can become quite verbose and difficult to understand. To make the task of defining data pipelines as intuitive as possible, Dagger also exposes an imperative domain-specific language (DSL) whose domain consists of defining DAGs. You can use the DSL to define DAGs as if you were writing local Python code . What could be more natural than that? The Illusion of Control \u00b6 Notice how in the last paragraph we said \"...as if you were writing local Python code\" ? One of the most important concepts to grasp when using Dagger 's DSL is that, as soon as functions are decorated with dagger.dsl.task or dagger.dsl.DAG , they stop behaving as regular Python functions , so to speak. Instead, they become NodeInvocationRecorder objects whose purpose is to record how you expect a DAG to behave without actually executing any of its tasks. When you do: 1 2 3 4 5 @dsl . DAG () def dag ( sample_size ): datasets = prepare_datasets ( sample_size ) model = train_model ( datasets [ \"training\" ]) return measure_model_performance ( model , datasets [ \"test\" ]) This is what Dagger is able to understand: From the signature of the function dag , the DSL knows the DAG has a single parameter named sample_size . Then, the DSL records your intention to invoke: The task prepare_datasets passing the parameter as a single argument. The task train_model passing the sub-key \"training\" from the output of prepare_datasets . The task measure_model_performance passing both the output of train_model and the sub-key \"test\" from the output of prepare_datasets . Finally, the DSL interprets that you want to return the output of measure_model_performance as the output of the DAG. Build time vs. runtime \u00b6 When you use a compiled language like C, your code has two phases: compilation (where C code gets transformed into machine code for a particular computer architecture) and execution (where the machine code is run by that computer architecture). You can think of the DSL as a compilation process that takes Python code and transforms it into a dagger.DAG data structure . Thus, when you do this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from dagger import dsl from dagger.runtime.local import invoke @dsl . task () def my_task (): print ( \"Hello at runtime!\" ) @dsl . DAG () def my_pipeline (): print ( \"Hello at build time!\" ) my_task () dag = dsl . build ( my_pipeline ) invoke ( dag ) The string \"Hello at build time!\" will only be printed when you call dsl.build() , whereas the string \"Hello at runtime!\" will be printed when you call invoke() . Notice how the print statement is NOT executed at runtime. Similarly, if you mix functions decorated with @dsl.task() and other Python functions, you may obtain unintended results. Take the following piece of code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import random from dagger import dsl from dagger.runtime.local import invoke def coin_toss (): return random . choice ([ \"heads\" , \"tails\" ]) @dsl . task () def order ( food_type ): print ( f \"Ordering { food_type } !\" ) @dsl . DAG () def my_pipeline (): if coin_toss () == \"heads\" : order ( \"chinese\" ) else : order ( \"italian\" ) In this case, coin_toss will be invoked at build time and return heads or tails. The if condition will be evaluated at build time, which means when we call dsl.build() , it will generate one of two possible DAGs: a DAG that always orders Chinese food, or a DAG that always orders Italian. If you had intended to execute this DAG periodically and add some variety to your diet, we've got bad news: It seems you'll be stuck with the same type of food for some time! This may or may not be what you intended. Note Mixing decorated and undecorated functions may certainly be useful. What's important is that you are able to distinguish between the things that will happen at build time and the things that will happen at runtime . Capabilities \u00b6 Like with all domain-specific languages, the number of things you can do inside of a function decorated with dagger.dsl.DAG is limited . Here goes a short summary: Parameterize a DAG by adding arguments to its function signature. Invoke nodes passing literal or hardcoded values as parameters. Invoke nodes passing the outputs of other nodes as parameters. Access keys from a node's output, when the output of that node is a mapping (e.g. a dictionary). Access properties of a node's output, when the output of that node is an object (e.g. a data class). Iterate over the results of an invocation to parallelize the execution of a node. Add all the results of a partitioned node in a list and pass that list to a fan-in node. Return multiple outputs from the DAG by returning a dictionary of str -> node_output . \u26d4 Limitations \u00b6 Technically speaking, anything that hasn't been listed as a capability in the previous section can be considered a limitation. That said, there are some extra limitations to keep in mind when using the DSL: As far as we know, Python is not able to inspect the name of the local variable that holds a value. Therefore, when you assign the output of a task to a variable my_var = my_task () and use it from another_task ( my_var , my_var [ \"my-key\" ], my_var . my_attr ) , the DAG you build from the DSL will name the outputs of my_task : \"return_value\" , \"key_my-key\" and \"property_my_attr\" . Parameter defaults are ignored by the DSL . A task with the signature def my_task ( a , b = 2 ) will still expect b to be passed as a parameter every time it's used. The same is true for DAG parameters. This may change in the future, if this feature request is implemented. Only the outputs that are used by other tasks will be exported . The rest will be omitted. If you assign the output of a task to a variable ( output = my_task () ), but don't use the variable output again, Dagger will assume you don't care about the output of that task. The task will still be executed, of course, but you may find the artifact not being saved in a runtime such as Argo Workflows . \ud83d\udee0\ufe0f DSL Implementation \u00b6 To be able to capture this definition from a simple Python function, we make extensive use of Python's magic methods , context variables and other metaprogramming capabilities. The specific implementation of Dagger 's DSL is quite interesting and will probably be the subject of an article in the future. Here are some of the highlights: Functions decorated by dagger.dsl.task or dagger.dsl.DAG are converted into an instance of NodeInvocationRecorder . This object implements the magic method __call__ ( * args , ** kwargs ) so that invoking the decorated function looks just like invoking the original function. Whenever the function dagger . dsl . build ( func_decorated_by_dag ) is called, it: Inspects the signature of the DAG to look for parameters. Creates a new context to isolate the definition of different DAGs (as we use global variables to record node invocations). Runs the body of the decorated function within that context. When we run the body of a function decorated with dagger.dsl.DAG , we may be invoking NodeInvocationRecorder . __call__ () multiple times. Each invocation: Generates a unique ID for the execution of that task. Binds the *args, **kwargs to the arguments of the function we decorated. Marks all the inputs it receives as \"consumed\". We do this to understand which values from another task need to be exported as outputs. Registers itself on the global list of node_invocations . Returns an object of type NodeOutputUsage . The purpose of a NodeOutputUsage is to record how the output of a node is used by the rest of the nodes in a DAG: If the output is never consumed, the DSL assumes the node that generated it doesn't need to export any outputs (since no other node seems to care about them). If the output is consumed directly, the DSL assumes we are interested in the return value as a whole. If a sub-key ( __getitem__ ( str ) ) or sub-property ( __getattr__ ( str ) ) of the output is consumed, the DSL assumes we're interested in that sub-value. If the output it iterated upon ( __iter__ () ), the DSL assumes the output is partitioned, and so is the node that depends on the elements returned by that iterator. After all node invocations and output usages have been properly recorded, the dagger.dsl.build() method inspects them and generates a dagger.DAG data structure . \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details. \ud83e\udde0 Learn more about... \u00b6 How to use different serializers for your inputs and outputs. How to run your DAGs with the different runtimes .","title":"Imperative DSL"},{"location":"user-guide/dsl/#imperative-dsl","text":"If you've been following along the examples in the previous sections, you will have noticed there are two ways of defining your DAGs with Dagger : An imperative DSL and the (declarative) data structures. The core data structures are the foundation of Dagger (they are the language runtimes understand). However, as DAGs become more complex, the data structures can become quite verbose and difficult to understand. To make the task of defining data pipelines as intuitive as possible, Dagger also exposes an imperative domain-specific language (DSL) whose domain consists of defining DAGs. You can use the DSL to define DAGs as if you were writing local Python code . What could be more natural than that?","title":"Imperative DSL"},{"location":"user-guide/dsl/#the-illusion-of-control","text":"Notice how in the last paragraph we said \"...as if you were writing local Python code\" ? One of the most important concepts to grasp when using Dagger 's DSL is that, as soon as functions are decorated with dagger.dsl.task or dagger.dsl.DAG , they stop behaving as regular Python functions , so to speak. Instead, they become NodeInvocationRecorder objects whose purpose is to record how you expect a DAG to behave without actually executing any of its tasks. When you do: 1 2 3 4 5 @dsl . DAG () def dag ( sample_size ): datasets = prepare_datasets ( sample_size ) model = train_model ( datasets [ \"training\" ]) return measure_model_performance ( model , datasets [ \"test\" ]) This is what Dagger is able to understand: From the signature of the function dag , the DSL knows the DAG has a single parameter named sample_size . Then, the DSL records your intention to invoke: The task prepare_datasets passing the parameter as a single argument. The task train_model passing the sub-key \"training\" from the output of prepare_datasets . The task measure_model_performance passing both the output of train_model and the sub-key \"test\" from the output of prepare_datasets . Finally, the DSL interprets that you want to return the output of measure_model_performance as the output of the DAG.","title":"The Illusion of Control"},{"location":"user-guide/dsl/#build-time-vs-runtime","text":"When you use a compiled language like C, your code has two phases: compilation (where C code gets transformed into machine code for a particular computer architecture) and execution (where the machine code is run by that computer architecture). You can think of the DSL as a compilation process that takes Python code and transforms it into a dagger.DAG data structure . Thus, when you do this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from dagger import dsl from dagger.runtime.local import invoke @dsl . task () def my_task (): print ( \"Hello at runtime!\" ) @dsl . DAG () def my_pipeline (): print ( \"Hello at build time!\" ) my_task () dag = dsl . build ( my_pipeline ) invoke ( dag ) The string \"Hello at build time!\" will only be printed when you call dsl.build() , whereas the string \"Hello at runtime!\" will be printed when you call invoke() . Notice how the print statement is NOT executed at runtime. Similarly, if you mix functions decorated with @dsl.task() and other Python functions, you may obtain unintended results. Take the following piece of code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import random from dagger import dsl from dagger.runtime.local import invoke def coin_toss (): return random . choice ([ \"heads\" , \"tails\" ]) @dsl . task () def order ( food_type ): print ( f \"Ordering { food_type } !\" ) @dsl . DAG () def my_pipeline (): if coin_toss () == \"heads\" : order ( \"chinese\" ) else : order ( \"italian\" ) In this case, coin_toss will be invoked at build time and return heads or tails. The if condition will be evaluated at build time, which means when we call dsl.build() , it will generate one of two possible DAGs: a DAG that always orders Chinese food, or a DAG that always orders Italian. If you had intended to execute this DAG periodically and add some variety to your diet, we've got bad news: It seems you'll be stuck with the same type of food for some time! This may or may not be what you intended. Note Mixing decorated and undecorated functions may certainly be useful. What's important is that you are able to distinguish between the things that will happen at build time and the things that will happen at runtime .","title":"Build time vs. runtime"},{"location":"user-guide/dsl/#capabilities","text":"Like with all domain-specific languages, the number of things you can do inside of a function decorated with dagger.dsl.DAG is limited . Here goes a short summary: Parameterize a DAG by adding arguments to its function signature. Invoke nodes passing literal or hardcoded values as parameters. Invoke nodes passing the outputs of other nodes as parameters. Access keys from a node's output, when the output of that node is a mapping (e.g. a dictionary). Access properties of a node's output, when the output of that node is an object (e.g. a data class). Iterate over the results of an invocation to parallelize the execution of a node. Add all the results of a partitioned node in a list and pass that list to a fan-in node. Return multiple outputs from the DAG by returning a dictionary of str -> node_output .","title":"Capabilities"},{"location":"user-guide/dsl/#limitations","text":"Technically speaking, anything that hasn't been listed as a capability in the previous section can be considered a limitation. That said, there are some extra limitations to keep in mind when using the DSL: As far as we know, Python is not able to inspect the name of the local variable that holds a value. Therefore, when you assign the output of a task to a variable my_var = my_task () and use it from another_task ( my_var , my_var [ \"my-key\" ], my_var . my_attr ) , the DAG you build from the DSL will name the outputs of my_task : \"return_value\" , \"key_my-key\" and \"property_my_attr\" . Parameter defaults are ignored by the DSL . A task with the signature def my_task ( a , b = 2 ) will still expect b to be passed as a parameter every time it's used. The same is true for DAG parameters. This may change in the future, if this feature request is implemented. Only the outputs that are used by other tasks will be exported . The rest will be omitted. If you assign the output of a task to a variable ( output = my_task () ), but don't use the variable output again, Dagger will assume you don't care about the output of that task. The task will still be executed, of course, but you may find the artifact not being saved in a runtime such as Argo Workflows .","title":"\u26d4 Limitations"},{"location":"user-guide/dsl/#dsl-implementation","text":"To be able to capture this definition from a simple Python function, we make extensive use of Python's magic methods , context variables and other metaprogramming capabilities. The specific implementation of Dagger 's DSL is quite interesting and will probably be the subject of an article in the future. Here are some of the highlights: Functions decorated by dagger.dsl.task or dagger.dsl.DAG are converted into an instance of NodeInvocationRecorder . This object implements the magic method __call__ ( * args , ** kwargs ) so that invoking the decorated function looks just like invoking the original function. Whenever the function dagger . dsl . build ( func_decorated_by_dag ) is called, it: Inspects the signature of the DAG to look for parameters. Creates a new context to isolate the definition of different DAGs (as we use global variables to record node invocations). Runs the body of the decorated function within that context. When we run the body of a function decorated with dagger.dsl.DAG , we may be invoking NodeInvocationRecorder . __call__ () multiple times. Each invocation: Generates a unique ID for the execution of that task. Binds the *args, **kwargs to the arguments of the function we decorated. Marks all the inputs it receives as \"consumed\". We do this to understand which values from another task need to be exported as outputs. Registers itself on the global list of node_invocations . Returns an object of type NodeOutputUsage . The purpose of a NodeOutputUsage is to record how the output of a node is used by the rest of the nodes in a DAG: If the output is never consumed, the DSL assumes the node that generated it doesn't need to export any outputs (since no other node seems to care about them). If the output is consumed directly, the DSL assumes we are interested in the return value as a whole. If a sub-key ( __getitem__ ( str ) ) or sub-property ( __getattr__ ( str ) ) of the output is consumed, the DSL assumes we're interested in that sub-value. If the output it iterated upon ( __iter__ () ), the DSL assumes the output is partitioned, and so is the node that depends on the elements returned by that iterator. After all node invocations and output usages have been properly recorded, the dagger.dsl.build() method inspects them and generates a dagger.DAG data structure .","title":"\ud83d\udee0\ufe0f DSL Implementation"},{"location":"user-guide/dsl/#api-reference","text":"Check the API Reference for more details.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/dsl/#learn-more-about","text":"How to use different serializers for your inputs and outputs. How to run your DAGs with the different runtimes .","title":"\ud83e\udde0 Learn more about..."},{"location":"user-guide/map-reduce/","text":"Map-Reduce \u00b6 One of the most common patterns you may find yourself implementing with Dagger is a map-reduce. Also known as a fan-out-fan-in or scatter-and-gather , a map-reduce pattern has the following shape: There will be a node producing a partitioned output (fan-out). This could be a list of countries, a dataset split into multiple chunks, or any other segmentation strategy you may be using in your project. After the partitions are generated, a number of \"mapping\" nodes will run in parallel , processing each of the partitions and generating a partial result. Finally, there will be a node that waits until all the \"mapping\" nodes have finished and will aggregate the partial results into a final result . The following picture shows how Argo Workflows represents the execution of a very basic map-reduce DAG: Common Use Cases \u00b6 Here are some common map-reduce implementations we've seen in Dagger : A pipeline that trains multiple version of a Machine Learning model and then picks the one that performs best. A pipeline that trains ML models for different countries in parallel, uploads each separately, and then integrates the metadata of every training session into a single human-readable report. An ETL pipeline that splits a large dataset into chunks, aggregates each chunk into a partial result and then aggregates all the partial results together (for instance, to calculate the average delivery time of an online purchase). \ud83d\udca1 Example \u00b6 The following example demonstrates how we can use Dagger to implement a (mocked) Machine Learning training pipeline that trains multiple versions of a model in parallel, and then picks the one that performs the best against a test suite. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from dagger import dsl @dsl . task () def prepare_datasets (): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } @dsl . task () def generate_training_combinations (): return [ { \"model_type\" : \"neural_network\" , \"params\" : [ \"...\" ]}, { \"model_type\" : \"boosted_tree\" , \"params\" : [ \"...\" ]}, ] @dsl . task () def train_model ( training_dataset , parameters ): return \"trained model\" @dsl . task () def choose_best_model ( alternative_models , test_dataset ): return \"best_model\" @dsl . DAG () def dag (): datasets = prepare_datasets () alternative_models = [] for training_parameters in generate_training_combinations (): model = train_model ( datasets [ \"training\" ], training_parameters ) alternative_models . append ( model ) best_model = choose_best_model ( alternative_models , datasets [ \"test\" ]) return best_model Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 from dagger import DAG , FromKey , FromNodeOutput , FromReturnValue , Task def prepare_datasets (): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } def generate_training_combinations (): return [ { \"model_type\" : \"neural_network\" , \"params\" : [ \"...\" ]}, { \"model_type\" : \"boosted_tree\" , \"params\" : [ \"...\" ]}, ] def train_model ( training_dataset , parameters ): return \"trained model\" def choose_best_model ( alternative_models , test_dataset ): return \"best_model\" dag = DAG ( nodes = { \"prepare-datasets\" : Task ( prepare_datasets , outputs = { \"training\" : FromKey ( \"training\" ), \"test\" : FromKey ( \"test\" ), }, ), \"generate-training-combinations\" : Task ( generate_training_combinations , outputs = { \"combinations\" : FromReturnValue ( is_partitioned = True ), }, ), \"train-model\" : Task ( train_model , inputs = { \"training_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"training\" ), \"parameters\" : FromNodeOutput ( \"generate-training-combinations\" , \"combinations\" ), }, outputs = { \"model\" : FromReturnValue (), }, partition_by_input = \"parameters\" , ), \"choose-best-model\" : Task ( choose_best_model , inputs = { \"alternative_models\" : FromNodeOutput ( \"train-model\" , \"model\" ), \"test_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"test\" ), }, outputs = { \"best_model\" : FromReturnValue (), }, ), }, outputs = { \"best_model\" : FromNodeOutput ( \"choose-best-model\" , \"best_model\" ), }, ) \u26d4 Limitations \u00b6 As explained in the partitioning limitations section, map-reduce patterns in Dagger have very specific constraints. At first sight, these constraints may look too strict, but in the long run they will make your code more understandable and predictable, and the Dagger codebase more reliable and extensible. In this section we will go through the different constraints and show you how you can overcome them. You may only have a single mapping node per DAG \u00b6 Say you have a DAG where you want to perform multiple mapping operations after a fan-out. In Dagger , you cannot do this in the same for block. Instead, you need to wrap all the mapping operations inside of another DAG. Note On the plus side, this allows you to keep most map-reduce DAGs to a single line: return reduce([map(partition) for partition in fan_out()]) . Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from typing import List from dagger import dsl Partition = str @dsl . task () def get_partitions () -> List [ Partition ]: return [ \"first\" , \"second\" , \"...\" , \"last\" ] @dsl . task () def do_something_with ( partition : Partition ) -> Partition : return f \" { partition } *\" @dsl . task () def do_something_else_with ( partition : Partition ) -> Partition : return f \" { partition } $\" @dsl . task () def aggregate ( partial_results : List [ Partition ]): return \", \" . join ( partial_results ) @dsl . DAG () def dag (): partitions = get_partitions () partial_results = [] for partition in partitions : p1 = do_something_with ( partition ) p2 = do_something_else_with ( p1 ) partial_results . append ( p2 ) return aggregate ( partial_results ) Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from typing import List from dagger import dsl Partition = str @dsl . task () def get_partitions () -> List [ Partition ]: return [ \"first\" , \"second\" , \"...\" , \"last\" ] @dsl . task () def do_something_with ( partition : Partition ) -> Partition : return f \" { partition } *\" @dsl . task () def do_something_else_with ( partition : Partition ) -> Partition : return f \" { partition } $\" @dsl . DAG () def map_partition ( partition : Partition ): transformed_partition = do_something_with ( partition ) return do_something_else_with ( transformed_partition ) @dsl . task () def aggregate ( partial_results : List [ Partition ]): return \", \" . join ( partial_results ) @dsl . DAG () def dag (): partitions = get_partitions () partial_results = [ map_partition ( partition ) for partition in partitions ] return aggregate ( partial_results ) # or simply: # return aggregate([map_partition(partition) for partition in get_partitions()]) You cannot parallelize directly from a DAG parameter \u00b6 Say you have a DAG where you receive all potential partitions at runtime as a parameter. In Dagger , you cannot do a for loop directly over the parameter. Only the outputs of other nodes are iterable . The solution here would be to transform that parameter into a node output by just passing it through an \"identity\" function (that is, a function that returns the same parameters it received). Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from typing import List from dagger import dsl @dsl . task () def do_something_for ( country : str ): pass @dsl . DAG () def dag ( countries : List [ str ]): for country in countries : do_something_for ( country ) Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from typing import List from dagger import dsl @dsl . task () def identity ( x ): return x @dsl . task () def do_something_for ( country : str ): pass @dsl . DAG () def dag ( countries : List [ str ]): countries_ = identity ( countries ) for country in countries_ : do_something_for ( country ) You cannot nest two for loops together \u00b6 Say you have two sets of partitions (e.g. user cohorts and product types) and you want to create a DAG that produces recommendations for each combination of (user cohort, product type) in parallel. In Dagger , you cannot nest multiple for loops within the same DAG. Instead, you need to create 2 separate DAGs (each parallelizing by a different dimension) and invoke one from the other. Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from dagger import dsl @dsl . task () def find_user_cohorts (): return [ \"baby-boomers\" , \"millenials\" , \"gen-z\" ] @dsl . task () def find_product_categories (): return [ \"veggies\" , \"meat\" ] @dsl . task () def generate_recommendations ( user_cohort , product_category ): pass @dsl . DAG () def dag (): for user_cohort in find_user_cohorts (): for product_category in find_product_categories (): generate_recommendations ( user_cohort , product_category ) Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from dagger import dsl @dsl . task () def find_user_cohorts (): return [ \"baby-boomers\" , \"millenials\" , \"gen-z\" ] @dsl . task () def find_product_categories (): return [ \"veggies\" , \"meat\" ] @dsl . task () def generate_recommendations ( user_cohort , product_category ): pass @dsl . DAG () def generate_recommendations_for_cohort ( user_cohort ): for product_category in find_product_categories (): generate_recommendations ( user_cohort , product_category ) @dsl . DAG () def dag (): for user_cohort in find_user_cohorts (): generate_recommendations_for_cohort ( user_cohort ) You cannot return the output of a partitioned node from a DAG \u00b6 Say you want to split a large dataset into smaller chunks, transform each chunk in parallel and then return the whole dataset as the result of the DAG. In Dagger , DAG outputs can only come from the outputs of nodes that are not partitioned . If you want to return the full dataset as the DAG's output you will need to concatenate the chunks back together in a fan-in/reduce task. You can use an \"identity\" function for this (a function that returns the same parameters it received). Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from dagger import dsl @dsl . task () def retrieve_dataset (): return \"dataset\" @dsl . task () def split_dataset_into_chunks ( dataset ): return [ \"chunk1\" , \"chunk2\" ] @dsl . task () def process_chunk ( chunk ): return f \"processed { chunk } \" @dsl . DAG () def dag (): dataset = retrieve_dataset () return [ process_chunk ( chunk ) for chunk in split_dataset_into_chunks ( dataset )] Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from dagger import dsl @dsl . task () def identity ( x ): return x @dsl . task () def retrieve_dataset (): return \"dataset\" @dsl . task () def split_dataset_into_chunks ( dataset ): return [ \"chunk1\" , \"chunk2\" ] @dsl . task () def process_chunk ( chunk ): return f \"processed { chunk } \" @dsl . DAG () def dag (): dataset = retrieve_dataset () processed_chunks = [ process_chunk ( chunk ) for chunk in split_dataset_into_chunks ( dataset ) ] return identity ( processed_chunks ) \ud83e\udde0 Learn more about... \u00b6 How the imperative DSL works. How to run your DAGs with the different runtimes .","title":"Map-Reduce"},{"location":"user-guide/map-reduce/#map-reduce","text":"One of the most common patterns you may find yourself implementing with Dagger is a map-reduce. Also known as a fan-out-fan-in or scatter-and-gather , a map-reduce pattern has the following shape: There will be a node producing a partitioned output (fan-out). This could be a list of countries, a dataset split into multiple chunks, or any other segmentation strategy you may be using in your project. After the partitions are generated, a number of \"mapping\" nodes will run in parallel , processing each of the partitions and generating a partial result. Finally, there will be a node that waits until all the \"mapping\" nodes have finished and will aggregate the partial results into a final result . The following picture shows how Argo Workflows represents the execution of a very basic map-reduce DAG:","title":"Map-Reduce"},{"location":"user-guide/map-reduce/#common-use-cases","text":"Here are some common map-reduce implementations we've seen in Dagger : A pipeline that trains multiple version of a Machine Learning model and then picks the one that performs best. A pipeline that trains ML models for different countries in parallel, uploads each separately, and then integrates the metadata of every training session into a single human-readable report. An ETL pipeline that splits a large dataset into chunks, aggregates each chunk into a partial result and then aggregates all the partial results together (for instance, to calculate the average delivery time of an online purchase).","title":"Common Use Cases"},{"location":"user-guide/map-reduce/#example","text":"The following example demonstrates how we can use Dagger to implement a (mocked) Machine Learning training pipeline that trains multiple versions of a model in parallel, and then picks the one that performs the best against a test suite. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from dagger import dsl @dsl . task () def prepare_datasets (): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } @dsl . task () def generate_training_combinations (): return [ { \"model_type\" : \"neural_network\" , \"params\" : [ \"...\" ]}, { \"model_type\" : \"boosted_tree\" , \"params\" : [ \"...\" ]}, ] @dsl . task () def train_model ( training_dataset , parameters ): return \"trained model\" @dsl . task () def choose_best_model ( alternative_models , test_dataset ): return \"best_model\" @dsl . DAG () def dag (): datasets = prepare_datasets () alternative_models = [] for training_parameters in generate_training_combinations (): model = train_model ( datasets [ \"training\" ], training_parameters ) alternative_models . append ( model ) best_model = choose_best_model ( alternative_models , datasets [ \"test\" ]) return best_model Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 from dagger import DAG , FromKey , FromNodeOutput , FromReturnValue , Task def prepare_datasets (): return { \"training\" : [ \"...\" ], \"test\" : [ \"...\" ], } def generate_training_combinations (): return [ { \"model_type\" : \"neural_network\" , \"params\" : [ \"...\" ]}, { \"model_type\" : \"boosted_tree\" , \"params\" : [ \"...\" ]}, ] def train_model ( training_dataset , parameters ): return \"trained model\" def choose_best_model ( alternative_models , test_dataset ): return \"best_model\" dag = DAG ( nodes = { \"prepare-datasets\" : Task ( prepare_datasets , outputs = { \"training\" : FromKey ( \"training\" ), \"test\" : FromKey ( \"test\" ), }, ), \"generate-training-combinations\" : Task ( generate_training_combinations , outputs = { \"combinations\" : FromReturnValue ( is_partitioned = True ), }, ), \"train-model\" : Task ( train_model , inputs = { \"training_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"training\" ), \"parameters\" : FromNodeOutput ( \"generate-training-combinations\" , \"combinations\" ), }, outputs = { \"model\" : FromReturnValue (), }, partition_by_input = \"parameters\" , ), \"choose-best-model\" : Task ( choose_best_model , inputs = { \"alternative_models\" : FromNodeOutput ( \"train-model\" , \"model\" ), \"test_dataset\" : FromNodeOutput ( \"prepare-datasets\" , \"test\" ), }, outputs = { \"best_model\" : FromReturnValue (), }, ), }, outputs = { \"best_model\" : FromNodeOutput ( \"choose-best-model\" , \"best_model\" ), }, )","title":"\ud83d\udca1 Example"},{"location":"user-guide/map-reduce/#limitations","text":"As explained in the partitioning limitations section, map-reduce patterns in Dagger have very specific constraints. At first sight, these constraints may look too strict, but in the long run they will make your code more understandable and predictable, and the Dagger codebase more reliable and extensible. In this section we will go through the different constraints and show you how you can overcome them.","title":"\u26d4 Limitations"},{"location":"user-guide/map-reduce/#you-may-only-have-a-single-mapping-node-per-dag","text":"Say you have a DAG where you want to perform multiple mapping operations after a fan-out. In Dagger , you cannot do this in the same for block. Instead, you need to wrap all the mapping operations inside of another DAG. Note On the plus side, this allows you to keep most map-reduce DAGs to a single line: return reduce([map(partition) for partition in fan_out()]) . Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from typing import List from dagger import dsl Partition = str @dsl . task () def get_partitions () -> List [ Partition ]: return [ \"first\" , \"second\" , \"...\" , \"last\" ] @dsl . task () def do_something_with ( partition : Partition ) -> Partition : return f \" { partition } *\" @dsl . task () def do_something_else_with ( partition : Partition ) -> Partition : return f \" { partition } $\" @dsl . task () def aggregate ( partial_results : List [ Partition ]): return \", \" . join ( partial_results ) @dsl . DAG () def dag (): partitions = get_partitions () partial_results = [] for partition in partitions : p1 = do_something_with ( partition ) p2 = do_something_else_with ( p1 ) partial_results . append ( p2 ) return aggregate ( partial_results ) Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from typing import List from dagger import dsl Partition = str @dsl . task () def get_partitions () -> List [ Partition ]: return [ \"first\" , \"second\" , \"...\" , \"last\" ] @dsl . task () def do_something_with ( partition : Partition ) -> Partition : return f \" { partition } *\" @dsl . task () def do_something_else_with ( partition : Partition ) -> Partition : return f \" { partition } $\" @dsl . DAG () def map_partition ( partition : Partition ): transformed_partition = do_something_with ( partition ) return do_something_else_with ( transformed_partition ) @dsl . task () def aggregate ( partial_results : List [ Partition ]): return \", \" . join ( partial_results ) @dsl . DAG () def dag (): partitions = get_partitions () partial_results = [ map_partition ( partition ) for partition in partitions ] return aggregate ( partial_results ) # or simply: # return aggregate([map_partition(partition) for partition in get_partitions()])","title":"You may only have a single mapping node per DAG"},{"location":"user-guide/map-reduce/#you-cannot-parallelize-directly-from-a-dag-parameter","text":"Say you have a DAG where you receive all potential partitions at runtime as a parameter. In Dagger , you cannot do a for loop directly over the parameter. Only the outputs of other nodes are iterable . The solution here would be to transform that parameter into a node output by just passing it through an \"identity\" function (that is, a function that returns the same parameters it received). Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from typing import List from dagger import dsl @dsl . task () def do_something_for ( country : str ): pass @dsl . DAG () def dag ( countries : List [ str ]): for country in countries : do_something_for ( country ) Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from typing import List from dagger import dsl @dsl . task () def identity ( x ): return x @dsl . task () def do_something_for ( country : str ): pass @dsl . DAG () def dag ( countries : List [ str ]): countries_ = identity ( countries ) for country in countries_ : do_something_for ( country )","title":"You cannot parallelize directly from a DAG parameter"},{"location":"user-guide/map-reduce/#you-cannot-nest-two-for-loops-together","text":"Say you have two sets of partitions (e.g. user cohorts and product types) and you want to create a DAG that produces recommendations for each combination of (user cohort, product type) in parallel. In Dagger , you cannot nest multiple for loops within the same DAG. Instead, you need to create 2 separate DAGs (each parallelizing by a different dimension) and invoke one from the other. Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from dagger import dsl @dsl . task () def find_user_cohorts (): return [ \"baby-boomers\" , \"millenials\" , \"gen-z\" ] @dsl . task () def find_product_categories (): return [ \"veggies\" , \"meat\" ] @dsl . task () def generate_recommendations ( user_cohort , product_category ): pass @dsl . DAG () def dag (): for user_cohort in find_user_cohorts (): for product_category in find_product_categories (): generate_recommendations ( user_cohort , product_category ) Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from dagger import dsl @dsl . task () def find_user_cohorts (): return [ \"baby-boomers\" , \"millenials\" , \"gen-z\" ] @dsl . task () def find_product_categories (): return [ \"veggies\" , \"meat\" ] @dsl . task () def generate_recommendations ( user_cohort , product_category ): pass @dsl . DAG () def generate_recommendations_for_cohort ( user_cohort ): for product_category in find_product_categories (): generate_recommendations ( user_cohort , product_category ) @dsl . DAG () def dag (): for user_cohort in find_user_cohorts (): generate_recommendations_for_cohort ( user_cohort )","title":"You cannot nest two for loops together"},{"location":"user-guide/map-reduce/#you-cannot-return-the-output-of-a-partitioned-node-from-a-dag","text":"Say you want to split a large dataset into smaller chunks, transform each chunk in parallel and then return the whole dataset as the result of the DAG. In Dagger , DAG outputs can only come from the outputs of nodes that are not partitioned . If you want to return the full dataset as the DAG's output you will need to concatenate the chunks back together in a fan-in/reduce task. You can use an \"identity\" function for this (a function that returns the same parameters it received). Invalid pattern 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from dagger import dsl @dsl . task () def retrieve_dataset (): return \"dataset\" @dsl . task () def split_dataset_into_chunks ( dataset ): return [ \"chunk1\" , \"chunk2\" ] @dsl . task () def process_chunk ( chunk ): return f \"processed { chunk } \" @dsl . DAG () def dag (): dataset = retrieve_dataset () return [ process_chunk ( chunk ) for chunk in split_dataset_into_chunks ( dataset )] Solution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from dagger import dsl @dsl . task () def identity ( x ): return x @dsl . task () def retrieve_dataset (): return \"dataset\" @dsl . task () def split_dataset_into_chunks ( dataset ): return [ \"chunk1\" , \"chunk2\" ] @dsl . task () def process_chunk ( chunk ): return f \"processed { chunk } \" @dsl . DAG () def dag (): dataset = retrieve_dataset () processed_chunks = [ process_chunk ( chunk ) for chunk in split_dataset_into_chunks ( dataset ) ] return identity ( processed_chunks )","title":"You cannot return the output of a partitioned node from a DAG"},{"location":"user-guide/map-reduce/#learn-more-about","text":"How the imperative DSL works. How to run your DAGs with the different runtimes .","title":"\ud83e\udde0 Learn more about..."},{"location":"user-guide/partitioning/","text":"Parallelization: Output and node partitioning \u00b6 Tasks in Dagger can return partitioned outputs. Partitioned outputs are just outputs that can be split into several chunks , and joined back together later on. The value of a partitioned output is expected to be an Iterable . Nodes may also be partitioned based on one of their inputs , provided that such an input comes from a partitioned output. When a node is partitioned, each of the partitions may be executed in parallel . The outputs of a partitioned node are also partitioned. \ud83d\udca1 Example \u00b6 Let's take the example from the section about dag composition and extend it a bit. In that scenario, we were first retrieving a dataset from a source (e.g. a data lake), and then invoking a different DAG to apply multiple transformations to that dataset. Now, we will assume the dataset we fetch can be arbitrarily large, so we will partition it into chunks according to some logic and invoke the set of transformations separately for each chunk. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from typing import List from dagger import dsl DataSet = str # # DAG that processes and transforms a dataset # =========================================== # @dsl . task () def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" @dsl . task () def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" @dsl . task () def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" @dsl . DAG () def transform_dataset ( dataset : DataSet ): ds_1 = encode_field_a ( dataset ) ds_2 = aggregate_fields_b_and_c ( ds_1 ) return calculate_moving_average_for_d ( ds_2 ) # # DAG that splits a large dataset into chunks # and invokes the previous DAG for each chunk # =========================================== # @dsl . task () def retrieve_dataset () -> DataSet : return \"original dataset\" @dsl . task () def split_dataset_into_chunks ( dataset : DataSet ) -> List [ DataSet ]: return [ f \" { dataset } (chunk { i } )\" for i in range ( 3 )] @dsl . DAG () def dag (): dataset = retrieve_dataset () chunks = split_dataset_into_chunks ( dataset ) for chunk in chunks : transform_dataset ( chunk ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 from typing import List from dagger import DAG , FromNodeOutput , FromParam , FromReturnValue , Task DataSet = str # # DAG that processes and transforms a dataset # =========================================== # def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" def dataset_transformation_dag ( dataset_input : FromNodeOutput ) -> DAG : return DAG ( inputs = { \"dataset\" : dataset_input }, nodes = { \"encode-field-a\" : Task ( encode_field_a , inputs = { \"dataset\" : FromParam ( \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"aggregate-fields-b-and-c\" : Task ( aggregate_fields_b_and_c , inputs = { \"dataset\" : FromNodeOutput ( \"encode-field-a\" , \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"calculate-moving-average-for-d\" : Task ( calculate_moving_average_for_d , inputs = { \"dataset\" : FromNodeOutput ( \"aggregate-fields-b-and-c\" , \"dataset\" ) }, outputs = { \"dataset\" : FromReturnValue ()}, ), }, outputs = { \"dataset\" : FromNodeOutput ( \"calculate-moving-average-for-d\" , \"dataset\" ) }, partition_by_input = \"dataset\" , ) # # DAG that splits a large dataset into chunks # and invokes the previous DAG for each chunk # =========================================== # def retrieve_dataset () -> DataSet : return \"original dataset\" def split_dataset_into_chunks ( dataset : DataSet ) -> List [ DataSet ]: return [ f \" { dataset } (chunk { i } )\" for i in range ( 3 )] dag = DAG ( nodes = { \"retrieve-dataset\" : Task ( retrieve_dataset , outputs = { \"dataset\" : FromReturnValue ()}, ), \"split-dataset-into-chunks\" : Task ( split_dataset_into_chunks , inputs = { \"dataset\" : FromNodeOutput ( \"retrieve-dataset\" , \"dataset\" )}, outputs = { \"chunks\" : FromReturnValue ( is_partitioned = True )}, ), \"process-chunk\" : dataset_transformation_dag ( FromNodeOutput ( \"split-dataset-into-chunks\" , \"chunks\" ), ), } ) \u26d4 Limitations \u00b6 If not managed properly, combining partitioned outputs and nodes may lead to a lot of complexity (resulting in non-intuitive behavior for you, the library's users, and making the implementation of runtimes quite hard for anyone who wants to maintain or extend its behavior by creating new runtimes ). When designing Dagger , we made a conscious decision to limit the way in which partitioning can be used . Our goal is to provide parallelization and map-reduce patterns that are easy to use and reason about, and rely on DAG composition and application code to combine the existing patterns into more sophisticated scenarios as elegantly as possible. Here are some of the limitations of node and output partitioning: Nodes may only be partitioned by one of their inputs. Nodes may only be partitioned by an input that comes from a partitioned output. Nodes may NOT be partitioned by an input that comes from a parameter. DAGs may NOT return outputs that come directly from a partitioned node. All these limitations can be overcome by the use of DAG composition and extra application code. The section on map-reduce patterns provides useful solutions to all these limitations . Note Whenever you try to use a partitioning pattern that is not allowed, Dagger will try to provide a useful error message explaining the root of the problem and pointing you to the documentation. \ud83e\udde0 Learn more about... \u00b6 How to implement map-reduce patterns (also known as fan-out-fan-in, or scatter-gather patterns). How to run your DAGs with the different runtimes .","title":"Parallelization: Output and node partitioning"},{"location":"user-guide/partitioning/#parallelization-output-and-node-partitioning","text":"Tasks in Dagger can return partitioned outputs. Partitioned outputs are just outputs that can be split into several chunks , and joined back together later on. The value of a partitioned output is expected to be an Iterable . Nodes may also be partitioned based on one of their inputs , provided that such an input comes from a partitioned output. When a node is partitioned, each of the partitions may be executed in parallel . The outputs of a partitioned node are also partitioned.","title":"Parallelization: Output and node partitioning"},{"location":"user-guide/partitioning/#example","text":"Let's take the example from the section about dag composition and extend it a bit. In that scenario, we were first retrieving a dataset from a source (e.g. a data lake), and then invoking a different DAG to apply multiple transformations to that dataset. Now, we will assume the dataset we fetch can be arbitrarily large, so we will partition it into chunks according to some logic and invoke the set of transformations separately for each chunk. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 from typing import List from dagger import dsl DataSet = str # # DAG that processes and transforms a dataset # =========================================== # @dsl . task () def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" @dsl . task () def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" @dsl . task () def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" @dsl . DAG () def transform_dataset ( dataset : DataSet ): ds_1 = encode_field_a ( dataset ) ds_2 = aggregate_fields_b_and_c ( ds_1 ) return calculate_moving_average_for_d ( ds_2 ) # # DAG that splits a large dataset into chunks # and invokes the previous DAG for each chunk # =========================================== # @dsl . task () def retrieve_dataset () -> DataSet : return \"original dataset\" @dsl . task () def split_dataset_into_chunks ( dataset : DataSet ) -> List [ DataSet ]: return [ f \" { dataset } (chunk { i } )\" for i in range ( 3 )] @dsl . DAG () def dag (): dataset = retrieve_dataset () chunks = split_dataset_into_chunks ( dataset ) for chunk in chunks : transform_dataset ( chunk ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 from typing import List from dagger import DAG , FromNodeOutput , FromParam , FromReturnValue , Task DataSet = str # # DAG that processes and transforms a dataset # =========================================== # def encode_field_a ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with field a encoded\" def aggregate_fields_b_and_c ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with fields b and c aggregated\" def calculate_moving_average_for_d ( dataset : DataSet ) -> DataSet : return f \" { dataset } , with moving average for d calculated\" def dataset_transformation_dag ( dataset_input : FromNodeOutput ) -> DAG : return DAG ( inputs = { \"dataset\" : dataset_input }, nodes = { \"encode-field-a\" : Task ( encode_field_a , inputs = { \"dataset\" : FromParam ( \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"aggregate-fields-b-and-c\" : Task ( aggregate_fields_b_and_c , inputs = { \"dataset\" : FromNodeOutput ( \"encode-field-a\" , \"dataset\" )}, outputs = { \"dataset\" : FromReturnValue ()}, ), \"calculate-moving-average-for-d\" : Task ( calculate_moving_average_for_d , inputs = { \"dataset\" : FromNodeOutput ( \"aggregate-fields-b-and-c\" , \"dataset\" ) }, outputs = { \"dataset\" : FromReturnValue ()}, ), }, outputs = { \"dataset\" : FromNodeOutput ( \"calculate-moving-average-for-d\" , \"dataset\" ) }, partition_by_input = \"dataset\" , ) # # DAG that splits a large dataset into chunks # and invokes the previous DAG for each chunk # =========================================== # def retrieve_dataset () -> DataSet : return \"original dataset\" def split_dataset_into_chunks ( dataset : DataSet ) -> List [ DataSet ]: return [ f \" { dataset } (chunk { i } )\" for i in range ( 3 )] dag = DAG ( nodes = { \"retrieve-dataset\" : Task ( retrieve_dataset , outputs = { \"dataset\" : FromReturnValue ()}, ), \"split-dataset-into-chunks\" : Task ( split_dataset_into_chunks , inputs = { \"dataset\" : FromNodeOutput ( \"retrieve-dataset\" , \"dataset\" )}, outputs = { \"chunks\" : FromReturnValue ( is_partitioned = True )}, ), \"process-chunk\" : dataset_transformation_dag ( FromNodeOutput ( \"split-dataset-into-chunks\" , \"chunks\" ), ), } )","title":"\ud83d\udca1 Example"},{"location":"user-guide/partitioning/#limitations","text":"If not managed properly, combining partitioned outputs and nodes may lead to a lot of complexity (resulting in non-intuitive behavior for you, the library's users, and making the implementation of runtimes quite hard for anyone who wants to maintain or extend its behavior by creating new runtimes ). When designing Dagger , we made a conscious decision to limit the way in which partitioning can be used . Our goal is to provide parallelization and map-reduce patterns that are easy to use and reason about, and rely on DAG composition and application code to combine the existing patterns into more sophisticated scenarios as elegantly as possible. Here are some of the limitations of node and output partitioning: Nodes may only be partitioned by one of their inputs. Nodes may only be partitioned by an input that comes from a partitioned output. Nodes may NOT be partitioned by an input that comes from a parameter. DAGs may NOT return outputs that come directly from a partitioned node. All these limitations can be overcome by the use of DAG composition and extra application code. The section on map-reduce patterns provides useful solutions to all these limitations . Note Whenever you try to use a partitioning pattern that is not allowed, Dagger will try to provide a useful error message explaining the root of the problem and pointing you to the documentation.","title":"\u26d4 Limitations"},{"location":"user-guide/partitioning/#learn-more-about","text":"How to implement map-reduce patterns (also known as fan-out-fan-in, or scatter-gather patterns). How to run your DAGs with the different runtimes .","title":"\ud83e\udde0 Learn more about..."},{"location":"user-guide/tasks/","text":"Tasks \u00b6 Tasks are the basic building blocks of Dagger . They wrap a Python function that will be executed as a step of a DAG . They also define: Where the inputs of the function are coming from (e.g. from a DAG parameter, from the output of another node...). What outputs does the task produce. How to retrieve the task's outputs from the return value of the function. \ud83d\udca1 Example \u00b6 Imperative DSL 1 2 3 4 5 6 from dagger import dsl @dsl . task () def hello ( name ): return f \"Hello { name } !\" Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from dagger import FromParam , FromReturnValue , Task def hello ( name ): return f \"Hello { name } !\" task = Task ( hello , inputs = { \"name\" : FromParam (), }, outputs = { \"hello_message\" : FromReturnValue (), }, ) \u27a1\ufe0f Inputs \u00b6 A task can have multiple inputs. The names of the inputs must correspond to those of the function's arguments. Inputs can come: FromParam ( name : str ) . This indicates the input comes from a parameter named name , passed to the task's parent (a DAG). FromNodeOutput ( node : str , output : str ) . This indicates the input comes from an output named output , which comes from another node named node . The current task and the node must be siblings in the same DAG. \u2b05\ufe0f Outputs \u00b6 A task can have multiple outputs. The specific type of output indicates how it should be retrieved from the return value of the function: FromReturnValue () will expose the returned value as it is. That is, given a function lambda : { \"a\" : 1 , \"b\" : 2 } \" , it will return { \"a\" : 1 , \"b\" : 2 } . FromKey ( name : str ) . If the function returns a mapping, this output exposes the value of one of its keys. That is, given the function lambda : { \"a\" : 1 , \"b\" : 2 } \" , FromKey ( \"a\" ) will return 1 . FromProperty ( name : str ) . If the function returns an object, this output exposes the value of one of its properties. That is, given the function lambda : complex ( 2 , 3 ) , FromProperty ( \"imag\" ) will return 3 . \u26d4 Limitations \u00b6 Tasks are validated against the following rules: Input and output names must be 1-64 characters long, begin by a letter or number, and only contain letters, numbers, underscores and hyphens. The names of the inputs must match the names of the function's arguments. The function should have as many inputs as the function has arguments. Optional arguments in the function still need to be supplied as inputs to the task. Tasks can only be partitioned by inputs that come from partitioned outputs. Therefore, they may not be partitioned by an input FromParam() . \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about the specific options supported by a Task. \ud83e\udde0 Learn more about... \u00b6 How to connect multiple tasks together in a Directed Acyclic Graph (DAG) . How to use different serializers for your inputs and outputs.","title":"Tasks"},{"location":"user-guide/tasks/#tasks","text":"Tasks are the basic building blocks of Dagger . They wrap a Python function that will be executed as a step of a DAG . They also define: Where the inputs of the function are coming from (e.g. from a DAG parameter, from the output of another node...). What outputs does the task produce. How to retrieve the task's outputs from the return value of the function.","title":"Tasks"},{"location":"user-guide/tasks/#example","text":"Imperative DSL 1 2 3 4 5 6 from dagger import dsl @dsl . task () def hello ( name ): return f \"Hello { name } !\" Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from dagger import FromParam , FromReturnValue , Task def hello ( name ): return f \"Hello { name } !\" task = Task ( hello , inputs = { \"name\" : FromParam (), }, outputs = { \"hello_message\" : FromReturnValue (), }, )","title":"\ud83d\udca1 Example"},{"location":"user-guide/tasks/#inputs","text":"A task can have multiple inputs. The names of the inputs must correspond to those of the function's arguments. Inputs can come: FromParam ( name : str ) . This indicates the input comes from a parameter named name , passed to the task's parent (a DAG). FromNodeOutput ( node : str , output : str ) . This indicates the input comes from an output named output , which comes from another node named node . The current task and the node must be siblings in the same DAG.","title":"\u27a1\ufe0f Inputs"},{"location":"user-guide/tasks/#outputs","text":"A task can have multiple outputs. The specific type of output indicates how it should be retrieved from the return value of the function: FromReturnValue () will expose the returned value as it is. That is, given a function lambda : { \"a\" : 1 , \"b\" : 2 } \" , it will return { \"a\" : 1 , \"b\" : 2 } . FromKey ( name : str ) . If the function returns a mapping, this output exposes the value of one of its keys. That is, given the function lambda : { \"a\" : 1 , \"b\" : 2 } \" , FromKey ( \"a\" ) will return 1 . FromProperty ( name : str ) . If the function returns an object, this output exposes the value of one of its properties. That is, given the function lambda : complex ( 2 , 3 ) , FromProperty ( \"imag\" ) will return 3 .","title":"\u2b05\ufe0f Outputs"},{"location":"user-guide/tasks/#limitations","text":"Tasks are validated against the following rules: Input and output names must be 1-64 characters long, begin by a letter or number, and only contain letters, numbers, underscores and hyphens. The names of the inputs must match the names of the function's arguments. The function should have as many inputs as the function has arguments. Optional arguments in the function still need to be supplied as inputs to the task. Tasks can only be partitioned by inputs that come from partitioned outputs. Therefore, they may not be partitioned by an input FromParam() .","title":"\u26d4 Limitations"},{"location":"user-guide/tasks/#api-reference","text":"Check the API Reference for more details about the specific options supported by a Task.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/tasks/#learn-more-about","text":"How to connect multiple tasks together in a Directed Acyclic Graph (DAG) . How to use different serializers for your inputs and outputs.","title":"\ud83e\udde0 Learn more about..."},{"location":"user-guide/runtimes/alternatives/","text":"Runtimes \u00b6 Presumably, the reason why you're using Dagger is to run your DAGs on a distributed workflow orchestration engine, such as Argo Workflows . In Dagger , the components that allow you to do this are called Runtimes . \ud83d\udce6 Built-in Runtimes \u00b6 Dagger comes with a handful of runtimes out of the box. Namely: dagger.runtimes.local - A local runtime that allows you to execute tasks and DAGs locally. You will use it mainly for testing purposes and local development. Other runtimes may use it under the hood to run certain tasks in a DAG. dagger.runtimes.cli - A runtime that accepts inputs and outputs to a DAG from the local filesystem through command-line arguments. You will probably not use it directly, but other runtimes may use it to run certain tasks in a DAG. dagger.runtimes.argo - A runtime that prepares your DAG to be executed by Argo Workflows on Kubernetes . This runtime does not execute your DAG directly, but it creates the resource manifests that tell Argo how your DAG should be executed. \ud83c\udfaf Leveraging Specific Runtime Features \u00b6 Every workflow orchestration technology has its strengths and weaknesses. When we were designing Dagger , one of our main tenets was that we didn't want to create an abstraction that relied on the lowest common denominator of all these technologies. We want to allow users to take advantage of the strengths of each runtime . If you are using Argo , you should be able to use retry strategies or set complex scheduling rules for your Kubernetes Pods. If you are using Airflow , you should be able to use the community plugins. And you should be able to do all of this without needing to wait for a new version of the py-dagger library to be published! To make this possible, you can inject arbitrary \"runtime options\" to your tasks and DAGs . Runtimes are free to inspect these extra options and use them to modify the way they execute or transform your DAGs. Check the documentation for each runtime to understand which options are available. Example \u00b6 In this example, we have a DAG with two tasks: A task that queries data from a database (on Argo, we want to retry this operation in case there's a temporary problem with the network). A task that processes that data (and needs a specific amount of memory and CPU power to do it efficiently). We are also injecting some options to the DAG itself, but in this case, it is only for illustrative purposes. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from dagger import dsl @dsl . task ( runtime_options = { \"argo_template_overrides\" : { \"retryStrategy\" : { \"limit\" : 3 }, \"activeDeadlineSeconds\" : 300 , }, } ) def query_database ( query ): return \"query results...\" @dsl . task ( runtime_options = { \"argo_container_overrides\" : { \"resources\" : { \"requests\" : { \"cpu\" : \"16\" , \"memory\" : \"64Gi\" , }, }, }, } ) def process_data ( data ): return \"processed results\" @dsl . DAG ( runtime_options = { \"argo_dag_template_overrides\" : { \"failFast\" : False , }, } ) def dag ( query ): data = query_database ( query ) return process_data ( data ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 from dagger import DAG , FromNodeOutput , FromParam , FromReturnValue , Task def query_database ( query ): return \"query results...\" def process_data ( data ): return \"processed results\" dag = DAG ( inputs = { \"query\" : FromParam (), }, outputs = { \"results\" : FromNodeOutput ( \"process-data\" , \"data\" ), }, nodes = { \"query-database\" : Task ( query_database , inputs = { \"query\" : FromParam ( \"query\" ), }, outputs = { \"data\" : FromReturnValue (), }, runtime_options = { \"argo_template_overrides\" : { \"retryStrategy\" : { \"limit\" : 3 }, \"activeDeadlineSeconds\" : 300 , }, }, ), \"process-data\" : Task ( process_data , inputs = { \"data\" : FromNodeOutput ( \"query-database\" , \"data\" ), }, outputs = { \"data\" : FromReturnValue (), }, runtime_options = { \"argo_container_overrides\" : { \"resources\" : { \"requests\" : { \"cpu\" : \"16\" , \"memory\" : \"64Gi\" , }, }, }, }, ), }, runtime_options = { \"argo_dag_template_overrides\" : { \"failFast\" : False , }, }, ) \u26e9\ufe0f Runtime Architecture \u00b6 Runtimes are built using a layered approach: The Local runtime specializes on executing specific nodes in a DAG. The CLI runtime specializes on exposing a DAG through a machine-oriented interface, so that the different nodes in your DAG can be triggered from a container or virtual machine image. Once all the inputs have been obtained from the local filesystem, the CLI runtime delegates the execution to the local runtime . The Argo runtime specializes on generating the resource manifests that describe the kind: Workflow resources Argo and Kubernetes will use to execute your DAG. Each task is executed inside of a container. The Argo runtime expects the container to expose your DAG as a Command-Line Interface using the CLI runtime . The following diagram shows how the built-in runtimes interact with each other: Future Runtimes \u00b6 Following the architecture introduced in the previous section, Dagger can be extended to have any number of runtimes (some will be packaged with the library; some may be implemented in external or custom libraries). For instance, you may want to build: A Kubeflow Pipelines runtime. Because Kubeflow uses Argo Workflows under the hood, this runtime may rely on the Argo runtime to build the resource manifest, and then extend it with some Kubeflow-specific features and metadata . An Airflow runtime which translates the dagger.DAG data structure into Airflow 's DAGs and operators, and uses the CLI interface to run certain tasks. The following diagram represents how these new runtimes may fit into the current runtime architecture: \ud83d\udee0\ufe0f Writing a Runtime \u00b6 You can read this guide to understand how you can write a custom runtime.","title":"Runtimes"},{"location":"user-guide/runtimes/alternatives/#runtimes","text":"Presumably, the reason why you're using Dagger is to run your DAGs on a distributed workflow orchestration engine, such as Argo Workflows . In Dagger , the components that allow you to do this are called Runtimes .","title":"Runtimes"},{"location":"user-guide/runtimes/alternatives/#built-in-runtimes","text":"Dagger comes with a handful of runtimes out of the box. Namely: dagger.runtimes.local - A local runtime that allows you to execute tasks and DAGs locally. You will use it mainly for testing purposes and local development. Other runtimes may use it under the hood to run certain tasks in a DAG. dagger.runtimes.cli - A runtime that accepts inputs and outputs to a DAG from the local filesystem through command-line arguments. You will probably not use it directly, but other runtimes may use it to run certain tasks in a DAG. dagger.runtimes.argo - A runtime that prepares your DAG to be executed by Argo Workflows on Kubernetes . This runtime does not execute your DAG directly, but it creates the resource manifests that tell Argo how your DAG should be executed.","title":"\ud83d\udce6 Built-in Runtimes"},{"location":"user-guide/runtimes/alternatives/#leveraging-specific-runtime-features","text":"Every workflow orchestration technology has its strengths and weaknesses. When we were designing Dagger , one of our main tenets was that we didn't want to create an abstraction that relied on the lowest common denominator of all these technologies. We want to allow users to take advantage of the strengths of each runtime . If you are using Argo , you should be able to use retry strategies or set complex scheduling rules for your Kubernetes Pods. If you are using Airflow , you should be able to use the community plugins. And you should be able to do all of this without needing to wait for a new version of the py-dagger library to be published! To make this possible, you can inject arbitrary \"runtime options\" to your tasks and DAGs . Runtimes are free to inspect these extra options and use them to modify the way they execute or transform your DAGs. Check the documentation for each runtime to understand which options are available.","title":"\ud83c\udfaf Leveraging Specific Runtime Features"},{"location":"user-guide/runtimes/alternatives/#example","text":"In this example, we have a DAG with two tasks: A task that queries data from a database (on Argo, we want to retry this operation in case there's a temporary problem with the network). A task that processes that data (and needs a specific amount of memory and CPU power to do it efficiently). We are also injecting some options to the DAG itself, but in this case, it is only for illustrative purposes. Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from dagger import dsl @dsl . task ( runtime_options = { \"argo_template_overrides\" : { \"retryStrategy\" : { \"limit\" : 3 }, \"activeDeadlineSeconds\" : 300 , }, } ) def query_database ( query ): return \"query results...\" @dsl . task ( runtime_options = { \"argo_container_overrides\" : { \"resources\" : { \"requests\" : { \"cpu\" : \"16\" , \"memory\" : \"64Gi\" , }, }, }, } ) def process_data ( data ): return \"processed results\" @dsl . DAG ( runtime_options = { \"argo_dag_template_overrides\" : { \"failFast\" : False , }, } ) def dag ( query ): data = query_database ( query ) return process_data ( data ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 from dagger import DAG , FromNodeOutput , FromParam , FromReturnValue , Task def query_database ( query ): return \"query results...\" def process_data ( data ): return \"processed results\" dag = DAG ( inputs = { \"query\" : FromParam (), }, outputs = { \"results\" : FromNodeOutput ( \"process-data\" , \"data\" ), }, nodes = { \"query-database\" : Task ( query_database , inputs = { \"query\" : FromParam ( \"query\" ), }, outputs = { \"data\" : FromReturnValue (), }, runtime_options = { \"argo_template_overrides\" : { \"retryStrategy\" : { \"limit\" : 3 }, \"activeDeadlineSeconds\" : 300 , }, }, ), \"process-data\" : Task ( process_data , inputs = { \"data\" : FromNodeOutput ( \"query-database\" , \"data\" ), }, outputs = { \"data\" : FromReturnValue (), }, runtime_options = { \"argo_container_overrides\" : { \"resources\" : { \"requests\" : { \"cpu\" : \"16\" , \"memory\" : \"64Gi\" , }, }, }, }, ), }, runtime_options = { \"argo_dag_template_overrides\" : { \"failFast\" : False , }, }, )","title":"Example"},{"location":"user-guide/runtimes/alternatives/#runtime-architecture","text":"Runtimes are built using a layered approach: The Local runtime specializes on executing specific nodes in a DAG. The CLI runtime specializes on exposing a DAG through a machine-oriented interface, so that the different nodes in your DAG can be triggered from a container or virtual machine image. Once all the inputs have been obtained from the local filesystem, the CLI runtime delegates the execution to the local runtime . The Argo runtime specializes on generating the resource manifests that describe the kind: Workflow resources Argo and Kubernetes will use to execute your DAG. Each task is executed inside of a container. The Argo runtime expects the container to expose your DAG as a Command-Line Interface using the CLI runtime . The following diagram shows how the built-in runtimes interact with each other:","title":"\u26e9\ufe0f Runtime Architecture"},{"location":"user-guide/runtimes/alternatives/#future-runtimes","text":"Following the architecture introduced in the previous section, Dagger can be extended to have any number of runtimes (some will be packaged with the library; some may be implemented in external or custom libraries). For instance, you may want to build: A Kubeflow Pipelines runtime. Because Kubeflow uses Argo Workflows under the hood, this runtime may rely on the Argo runtime to build the resource manifest, and then extend it with some Kubeflow-specific features and metadata . An Airflow runtime which translates the dagger.DAG data structure into Airflow 's DAGs and operators, and uses the CLI interface to run certain tasks. The following diagram represents how these new runtimes may fit into the current runtime architecture:","title":"Future Runtimes"},{"location":"user-guide/runtimes/alternatives/#writing-a-runtime","text":"You can read this guide to understand how you can write a custom runtime.","title":"\ud83d\udee0\ufe0f Writing a Runtime"},{"location":"user-guide/runtimes/argo/","text":"Argo Runtime \u00b6 The Argo runtime allows you to run your DAGs on Argo Workflows . In their words: Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD. What this means is that Argo will create a handful of Custom Kubernetes Resources : Workflow , CronWorkflow and WorkflowTemplate , among others. Then, it will expect workflows to be expressed declaratively (usually using YAML), following the specification of those resources. Here are some examples that show how Workflow manifests look like. The responsibility of the Argo runtime is to generate those manifests . \ud83d\udcdc Generating Manifests \u00b6 The Argo runtime exposes a series of methods to generate manifests for the custom resources mentioned above. This is how you would generate different resource manifests using the Argo runtime: Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Metadata , Workflow , workflow_manifest manifest = workflow_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), ) Cron Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Cron , Metadata , Workflow , cron_workflow_manifest manifest = cron_workflow_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), cron = Cron ( schedule = \"0 0 * * *\" , # Every day at 00:00 ), ) Workflow Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Metadata , Workflow , workflow_template_manifest manifest = workflow_template_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), ) Cluster Workflow Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Metadata , Workflow , cluster_workflow_template_manifest manifest = cluster_workflow_template_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), ) Why do we need container images and entrypoints \u00b6 Argo Workflows will execute each of the tasks in your DAG using a container. This requires you to build a container image that can run the DAG's tasks. Building container images is out of the scope of this guide, but the official Python Docker Image provides some useful examples. Once you've built a container, you need to specify which entrypoint of that container is able to execute the DAG through the command-line interface provided by the CLI runtime . Here is an example project that defines a DAG, exposes it through the CLI runtime, specifies how to build a container image using a Dockerfile, and finally has a script that generates the Argo manifests and dumps them into a YAML file. my_pipeline.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from dagger import dsl @dsl . task () def say_hello ( name ): print ( f \"Hello { name } !\" ) @dsl . DAG () def my_pipeline ( name ): say_hello ( name ) if __name__ == \"__main__\" : from dagger.runtime.cli import invoke # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) invoke ( dag ) Dockerfile 1 2 3 4 5 6 7 8 9 10 FROM python:3 WORKDIR /usr/src/app COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY . . ENTRYPOINT [ \"python\" ] generate_manifests.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import yaml from dagger import DAG from dagger.runtime.argo import Metadata , Workflow , workflow_manifest def dump_argo_manifests ( name : str , filename : str , dag : DAG , output_path : str , ): manifest = workflow_manifest ( dag , metadata = Metadata ( name = name , ), workflow = Workflow ( container_image = \"my_image\" , container_entrypoint_to_dag_cli = [ \"python\" , filename , ], ), ) with open ( output_path , \"w\" ) as f : yaml . dump ( manifest , f , Dumper = yaml . SafeDumper ) \ud83d\udd27 Runtime options \u00b6 Many of Argo's features are not first-class citizens in Dagger . For instance: Dagger doesn't understand that tasks may have timeouts or retry strategies. Dagger doesn't understand that tasks may have resource requests or limits. Dagger doesn't understand that you may want to fine-tune how your tasks are scheduled in your Kubernetes cluster using node selectors, tolerations or affinities. Nevertheless, Dagger allows you to set all of these settings together with the behavior of your task , so you don't lose your head doing complex post-processing of the manifests, or defining all these options in a separate configuration file. Container options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dagger import dsl @dsl . task ( runtime_options = { \"argo_container_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#container \"resources\" : { \"requests\" : { \"cpu\" : \"16\" , \"memory\" : \"64Gi\" , }, }, }, } ) def say_hello (): print ( \"Hello!\" ) Task options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from dagger import dsl @dsl . task ( runtime_options = { \"argo_task_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#dagtask \"continueOn\" : { \"failed\" : True , }, }, } ) def say_hello (): print ( \"Hello!\" ) Task Template options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from dagger import dsl @dsl . task ( runtime_options = { \"argo_template_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#template \"retryStrategy\" : { \"limit\" : 3 }, \"activeDeadlineSeconds\" : 300 , }, } ) def say_hello (): print ( \"Hello!\" ) DAG Template options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dagger import dsl @dsl . task () def say_hello (): print ( \"Hello!\" ) @dsl . DAG ( runtime_options = { \"argo_dag_template_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#dagtemplate \"failFast\" : False , }, } ) def dag (): say_hello () Workflow options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from dagger import dsl from dagger.runtime.argo import Metadata , Workflow , workflow_manifest @dsl . task () def say_hello (): print ( \"Hello!\" ) @dsl . DAG () def dag (): say_hello () manifest = workflow_manifest ( dsl . build ( dag ), metadata = Metadata ( name = \"my-pipeline\" ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , extra_spec_options = { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#workflowspec \"priority\" : 100 , \"nodeSelector\" : { \"my-label\" : \"my-value\" , }, }, ), ) Cron Workflow options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from dagger import dsl from dagger.runtime.argo import ( Cron , CronConcurrencyPolicy , Metadata , Workflow , cron_workflow_manifest , ) @dsl . task () def say_hello (): print ( \"Hello!\" ) @dsl . DAG () def dag (): say_hello () manifest = cron_workflow_manifest ( dsl . build ( dag ), metadata = Metadata ( name = \"my-pipeline\" ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" ), cron = Cron ( schedule = \"0 0 * * *\" , starting_deadline_seconds = 60 , concurrency_policy = CronConcurrencyPolicy . FORBID , timezone = \"Europe/Barcelona\" , successful_jobs_history_limit = 10 , failed_jobs_history_limit = 50 , extra_spec_options = { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#cronworkflowspec \"workflowMetadata\" : { \"annotations\" : { \"my-annotation\" : \"my-value\" , }, }, }, ), ) \ud83d\udc6e Enforcing your Company's Conventions and Standards \u00b6 The Argo runtime allows you to specify arbitrary options for many of the elements in the Workflow specification. However, when implementing Dagger inside of a large corporation, platform teams may want to enforce certain conventions and standards to comply with the company's governance, cost, observability or compliance policies . The following example shows how easy it is to extend the existing decorators to provide a more opinionated API and ease the day-to-day of both platform teams and Dagger users in the organization. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 from typing import Any , List , Mapping from dagger import dsl DEFAULT_CONFIG_MAPS = [ \"default\" ] DEFAULT_SECRETS = [ \"default\" ] def argo_task ( cpu_units : float , memory_in_gigabytes : float , timeout_in_minutes : int , max_retries : int = 0 , env_from_config_maps : List [ str ] = DEFAULT_CONFIG_MAPS , env_from_secrets : List [ str ] = DEFAULT_SECRETS , serializer : dsl . Serialize = dsl . Serialize (), extra_container_options : Mapping [ str , Any ] = None , extra_template_options : Mapping [ str , Any ] = None , ): \"\"\" Decorate a specific function as a task. This decorator extends `dagger.dsl.task` and provides an interface that: - Enforces the specification of a timeout, cpu and memory resources. - Allows injecting custom config maps and secrets as environment variables. - Adds custom metrics to record the duration of the task. Parameters ---------- cpu_units Units of CPU the task will require during execution. memory_in_gigabytes GiB of memory the task will require during execution. timeout_in_minutes Maximum minutes the task is expected to run. After the task times out, it will be killed by Argo. max_retries The number of times the task will be retried before considering it \"failed\" env_from_config_maps The names of resources of type 'ConfigMap' to inject to the task. All the keys in the config maps will be exposed as environment variables. env_from_secrets The names of resources of type 'Secret' to inject to the task. All the keys in the config maps will be exposed as environment variables. serializer The number of times the task will be retried before considering it \"failed\" extra_container_options Arbitrary options to pass to the Argo template container (see https://argoproj.github.io/argo-workflows/fields/#container) extra_template_options Arbitrary options to pass to the Argo template (see https://argoproj.github.io/argo-workflows/fields/#template) Returns ------- A decorated function that can be invoked with the same parameters as the original function \"\"\" assert cpu_units > 0 assert memory_in_gigabytes > 0 assert timeout_in_minutes > 0 assert max_retries >= 0 extra_container_options = extra_container_options or {} extra_template_options = extra_template_options or {} argo_container_overrides = { \"resources\" : { \"requests\" : { \"cpu\" : str ( cpu_units ), \"memory\" : f \" { memory_in_gigabytes } Gi\" , }, }, \"envFrom\" : [ * [ { \"configMapRef\" : { \"name\" : config_map_name }} for config_map_name in env_from_config_maps ], * [{ \"secretRef\" : { \"name\" : secret_name }} for secret_name in env_from_secrets ], ], ** extra_container_options , } argo_template_overrides = { \"activeDeadlineSeconds\" : timeout_in_minutes * 60 , \"retryStrategy\" : { \"limit\" : max_retries , }, ** extra_template_options , } return dsl . task ( serializer = serializer , runtime_options = { \"argo_container_overrides\" : argo_container_overrides , \"argo_template_overrides\" : argo_template_overrides , }, ) This example was taken from Glovo , which has been the first company to adopt Dagger . \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about this runtime and the options each of the methods accept.","title":"Argo Runtime"},{"location":"user-guide/runtimes/argo/#argo-runtime","text":"The Argo runtime allows you to run your DAGs on Argo Workflows . In their words: Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD. What this means is that Argo will create a handful of Custom Kubernetes Resources : Workflow , CronWorkflow and WorkflowTemplate , among others. Then, it will expect workflows to be expressed declaratively (usually using YAML), following the specification of those resources. Here are some examples that show how Workflow manifests look like. The responsibility of the Argo runtime is to generate those manifests .","title":"Argo Runtime"},{"location":"user-guide/runtimes/argo/#generating-manifests","text":"The Argo runtime exposes a series of methods to generate manifests for the custom resources mentioned above. This is how you would generate different resource manifests using the Argo runtime: Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Metadata , Workflow , workflow_manifest manifest = workflow_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), ) Cron Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Cron , Metadata , Workflow , cron_workflow_manifest manifest = cron_workflow_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), cron = Cron ( schedule = \"0 0 * * *\" , # Every day at 00:00 ), ) Workflow Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Metadata , Workflow , workflow_template_manifest manifest = workflow_template_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), ) Cluster Workflow Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) from dagger.runtime.argo import Metadata , Workflow , cluster_workflow_template_manifest manifest = cluster_workflow_template_manifest ( dag , metadata = Metadata ( name = \"my-pipeline\" , ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , container_entrypoint_to_dag_cli = [ \"python\" , \"dag_exposed_via_cli_runtime.py\" ], params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ), )","title":"\ud83d\udcdc Generating Manifests"},{"location":"user-guide/runtimes/argo/#why-do-we-need-container-images-and-entrypoints","text":"Argo Workflows will execute each of the tasks in your DAG using a container. This requires you to build a container image that can run the DAG's tasks. Building container images is out of the scope of this guide, but the official Python Docker Image provides some useful examples. Once you've built a container, you need to specify which entrypoint of that container is able to execute the DAG through the command-line interface provided by the CLI runtime . Here is an example project that defines a DAG, exposes it through the CLI runtime, specifies how to build a container image using a Dockerfile, and finally has a script that generates the Argo manifests and dumps them into a YAML file. my_pipeline.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from dagger import dsl @dsl . task () def say_hello ( name ): print ( f \"Hello { name } !\" ) @dsl . DAG () def my_pipeline ( name ): say_hello ( name ) if __name__ == \"__main__\" : from dagger.runtime.cli import invoke # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) invoke ( dag ) Dockerfile 1 2 3 4 5 6 7 8 9 10 FROM python:3 WORKDIR /usr/src/app COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY . . ENTRYPOINT [ \"python\" ] generate_manifests.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import yaml from dagger import DAG from dagger.runtime.argo import Metadata , Workflow , workflow_manifest def dump_argo_manifests ( name : str , filename : str , dag : DAG , output_path : str , ): manifest = workflow_manifest ( dag , metadata = Metadata ( name = name , ), workflow = Workflow ( container_image = \"my_image\" , container_entrypoint_to_dag_cli = [ \"python\" , filename , ], ), ) with open ( output_path , \"w\" ) as f : yaml . dump ( manifest , f , Dumper = yaml . SafeDumper )","title":"Why do we need container images and entrypoints"},{"location":"user-guide/runtimes/argo/#runtime-options","text":"Many of Argo's features are not first-class citizens in Dagger . For instance: Dagger doesn't understand that tasks may have timeouts or retry strategies. Dagger doesn't understand that tasks may have resource requests or limits. Dagger doesn't understand that you may want to fine-tune how your tasks are scheduled in your Kubernetes cluster using node selectors, tolerations or affinities. Nevertheless, Dagger allows you to set all of these settings together with the behavior of your task , so you don't lose your head doing complex post-processing of the manifests, or defining all these options in a separate configuration file. Container options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dagger import dsl @dsl . task ( runtime_options = { \"argo_container_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#container \"resources\" : { \"requests\" : { \"cpu\" : \"16\" , \"memory\" : \"64Gi\" , }, }, }, } ) def say_hello (): print ( \"Hello!\" ) Task options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from dagger import dsl @dsl . task ( runtime_options = { \"argo_task_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#dagtask \"continueOn\" : { \"failed\" : True , }, }, } ) def say_hello (): print ( \"Hello!\" ) Task Template options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from dagger import dsl @dsl . task ( runtime_options = { \"argo_template_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#template \"retryStrategy\" : { \"limit\" : 3 }, \"activeDeadlineSeconds\" : 300 , }, } ) def say_hello (): print ( \"Hello!\" ) DAG Template options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dagger import dsl @dsl . task () def say_hello (): print ( \"Hello!\" ) @dsl . DAG ( runtime_options = { \"argo_dag_template_overrides\" : { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#dagtemplate \"failFast\" : False , }, } ) def dag (): say_hello () Workflow options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from dagger import dsl from dagger.runtime.argo import Metadata , Workflow , workflow_manifest @dsl . task () def say_hello (): print ( \"Hello!\" ) @dsl . DAG () def dag (): say_hello () manifest = workflow_manifest ( dsl . build ( dag ), metadata = Metadata ( name = \"my-pipeline\" ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" , extra_spec_options = { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#workflowspec \"priority\" : 100 , \"nodeSelector\" : { \"my-label\" : \"my-value\" , }, }, ), ) Cron Workflow options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from dagger import dsl from dagger.runtime.argo import ( Cron , CronConcurrencyPolicy , Metadata , Workflow , cron_workflow_manifest , ) @dsl . task () def say_hello (): print ( \"Hello!\" ) @dsl . DAG () def dag (): say_hello () manifest = cron_workflow_manifest ( dsl . build ( dag ), metadata = Metadata ( name = \"my-pipeline\" ), workflow = Workflow ( container_image = \"my-docker-registry/my-image:my-tag\" ), cron = Cron ( schedule = \"0 0 * * *\" , starting_deadline_seconds = 60 , concurrency_policy = CronConcurrencyPolicy . FORBID , timezone = \"Europe/Barcelona\" , successful_jobs_history_limit = 10 , failed_jobs_history_limit = 50 , extra_spec_options = { # Available options to override: https://argoproj.github.io/argo-workflows/fields/#cronworkflowspec \"workflowMetadata\" : { \"annotations\" : { \"my-annotation\" : \"my-value\" , }, }, }, ), )","title":"\ud83d\udd27 Runtime options"},{"location":"user-guide/runtimes/argo/#enforcing-your-companys-conventions-and-standards","text":"The Argo runtime allows you to specify arbitrary options for many of the elements in the Workflow specification. However, when implementing Dagger inside of a large corporation, platform teams may want to enforce certain conventions and standards to comply with the company's governance, cost, observability or compliance policies . The following example shows how easy it is to extend the existing decorators to provide a more opinionated API and ease the day-to-day of both platform teams and Dagger users in the organization. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 from typing import Any , List , Mapping from dagger import dsl DEFAULT_CONFIG_MAPS = [ \"default\" ] DEFAULT_SECRETS = [ \"default\" ] def argo_task ( cpu_units : float , memory_in_gigabytes : float , timeout_in_minutes : int , max_retries : int = 0 , env_from_config_maps : List [ str ] = DEFAULT_CONFIG_MAPS , env_from_secrets : List [ str ] = DEFAULT_SECRETS , serializer : dsl . Serialize = dsl . Serialize (), extra_container_options : Mapping [ str , Any ] = None , extra_template_options : Mapping [ str , Any ] = None , ): \"\"\" Decorate a specific function as a task. This decorator extends `dagger.dsl.task` and provides an interface that: - Enforces the specification of a timeout, cpu and memory resources. - Allows injecting custom config maps and secrets as environment variables. - Adds custom metrics to record the duration of the task. Parameters ---------- cpu_units Units of CPU the task will require during execution. memory_in_gigabytes GiB of memory the task will require during execution. timeout_in_minutes Maximum minutes the task is expected to run. After the task times out, it will be killed by Argo. max_retries The number of times the task will be retried before considering it \"failed\" env_from_config_maps The names of resources of type 'ConfigMap' to inject to the task. All the keys in the config maps will be exposed as environment variables. env_from_secrets The names of resources of type 'Secret' to inject to the task. All the keys in the config maps will be exposed as environment variables. serializer The number of times the task will be retried before considering it \"failed\" extra_container_options Arbitrary options to pass to the Argo template container (see https://argoproj.github.io/argo-workflows/fields/#container) extra_template_options Arbitrary options to pass to the Argo template (see https://argoproj.github.io/argo-workflows/fields/#template) Returns ------- A decorated function that can be invoked with the same parameters as the original function \"\"\" assert cpu_units > 0 assert memory_in_gigabytes > 0 assert timeout_in_minutes > 0 assert max_retries >= 0 extra_container_options = extra_container_options or {} extra_template_options = extra_template_options or {} argo_container_overrides = { \"resources\" : { \"requests\" : { \"cpu\" : str ( cpu_units ), \"memory\" : f \" { memory_in_gigabytes } Gi\" , }, }, \"envFrom\" : [ * [ { \"configMapRef\" : { \"name\" : config_map_name }} for config_map_name in env_from_config_maps ], * [{ \"secretRef\" : { \"name\" : secret_name }} for secret_name in env_from_secrets ], ], ** extra_container_options , } argo_template_overrides = { \"activeDeadlineSeconds\" : timeout_in_minutes * 60 , \"retryStrategy\" : { \"limit\" : max_retries , }, ** extra_template_options , } return dsl . task ( serializer = serializer , runtime_options = { \"argo_container_overrides\" : argo_container_overrides , \"argo_template_overrides\" : argo_template_overrides , }, ) This example was taken from Glovo , which has been the first company to adopt Dagger .","title":"\ud83d\udc6e Enforcing your Company's Conventions and Standards"},{"location":"user-guide/runtimes/argo/#api-reference","text":"Check the API Reference for more details about this runtime and the options each of the methods accept.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/runtimes/cli/","text":"CLI Runtime \u00b6 The CLI runtime is responsible for exposing a DAG through a Command-Line Interface. Warning This runtime is not meant to be used by humans. It is meant to be used by other runtimes, as a building block to run DAGs from container images or virtual machines. The reason why it may be a bit tricky for humans to use is that it requires every input to be passed as a file whose contents are the serialized representation of that input's value . \ud83d\udcbb The Interface \u00b6 Say you have a file say_hello.py that describes your DAG. You can use the CLI runtime to expose that DAG on the command line, like this: Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from dagger import dsl @dsl . task () def say_hello ( name ): print ( f \"Hello { name } !\" ) @dsl . DAG () def my_pipeline ( name ): say_hello ( name ) if __name__ == \"__main__\" : from dagger.runtime.cli import invoke # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) invoke ( dag ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dagger import DAG , FromParam , Task def say_hello ( name ): print ( f \"Hello { name } !\" ) dag = DAG ( inputs = { \"name\" : FromParam (), }, nodes = { \"say-hello\" : Task ( say_hello , inputs = { \"name\" : FromParam ( \"name\" ), }, ), }, ) if __name__ == \"__main__\" : from dagger.runtime.cli import invoke invoke ( dag ) If you invoke python say_hello.py --help , you will notice a message similar to this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 usage: say_hello.py [-h] [--node-name NODE_NAME] [--output name location] [--input name location] Run a DAG, either completely, or partially using the filters specified in the arguments optional arguments: -h, --help show this help message and exit --node-name NODE_NAME Select a specific node to run. It must be properly namespaced with the name of all the parent DAGs. --output name location Store a given output into the location specified. Currently, we only support storing outputs in the local filesystem --input name location Retrieve a given input from the location specified. Currently, we only support retrieving inputs from the local filesystem As you can see, you can do 3 things with the CLI: You can select a specific node for execution (try doing python say_hello --node-name=say-hello ). You can pass any number of inputs. The location of each input needs to be a local file that contains the serialized value of the input. You can pass any number of outputs. The location of each output needs to be a local file where the serialized value of the output will be stored. \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about this runtime.","title":"CLI Runtime"},{"location":"user-guide/runtimes/cli/#cli-runtime","text":"The CLI runtime is responsible for exposing a DAG through a Command-Line Interface. Warning This runtime is not meant to be used by humans. It is meant to be used by other runtimes, as a building block to run DAGs from container images or virtual machines. The reason why it may be a bit tricky for humans to use is that it requires every input to be passed as a file whose contents are the serialized representation of that input's value .","title":"CLI Runtime"},{"location":"user-guide/runtimes/cli/#the-interface","text":"Say you have a file say_hello.py that describes your DAG. You can use the CLI runtime to expose that DAG on the command line, like this: Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from dagger import dsl @dsl . task () def say_hello ( name ): print ( f \"Hello { name } !\" ) @dsl . DAG () def my_pipeline ( name ): say_hello ( name ) if __name__ == \"__main__\" : from dagger.runtime.cli import invoke # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) invoke ( dag ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dagger import DAG , FromParam , Task def say_hello ( name ): print ( f \"Hello { name } !\" ) dag = DAG ( inputs = { \"name\" : FromParam (), }, nodes = { \"say-hello\" : Task ( say_hello , inputs = { \"name\" : FromParam ( \"name\" ), }, ), }, ) if __name__ == \"__main__\" : from dagger.runtime.cli import invoke invoke ( dag ) If you invoke python say_hello.py --help , you will notice a message similar to this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 usage: say_hello.py [-h] [--node-name NODE_NAME] [--output name location] [--input name location] Run a DAG, either completely, or partially using the filters specified in the arguments optional arguments: -h, --help show this help message and exit --node-name NODE_NAME Select a specific node to run. It must be properly namespaced with the name of all the parent DAGs. --output name location Store a given output into the location specified. Currently, we only support storing outputs in the local filesystem --input name location Retrieve a given input from the location specified. Currently, we only support retrieving inputs from the local filesystem As you can see, you can do 3 things with the CLI: You can select a specific node for execution (try doing python say_hello --node-name=say-hello ). You can pass any number of inputs. The location of each input needs to be a local file that contains the serialized value of the input. You can pass any number of outputs. The location of each output needs to be a local file where the serialized value of the output will be stored.","title":"\ud83d\udcbb The Interface"},{"location":"user-guide/runtimes/cli/#api-reference","text":"Check the API Reference for more details about this runtime.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/runtimes/local/","text":"Local Runtime \u00b6 The local runtime enables you to run any task or DAG locally. Just do: Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) from dagger.runtime.local import invoke # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) invoke ( dag , params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from dagger import DAG , FromParam , Task def echo ( message ): print ( message ) dag = DAG ( inputs = { \"message1\" : FromParam (), \"message2\" : FromParam (), }, nodes = { \"echo-message-1\" : Task ( echo , inputs = { \"message\" : FromParam ( \"message1\" ), }, ), \"echo-message-2\" : Task ( echo , inputs = { \"message\" : FromParam ( \"message2\" ), }, ), }, ) from dagger.runtime.local import invoke invoke ( dag , params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ) \ud83c\udfe0 Local Development and Testing \u00b6 When you are developing your DAGs, you will want to test their behavior as frequently as possible. This practice helps you iterate more quickly. The local runtime is great for local development and testing. On top of the validations that the data structures already perform, the local runtime serializes every output and connects all the nodes together, so it is a great way to verify that all your nodes can communicate effectively . For data pipelines that deal with large amounts of data or take a long time to execute, we recommend you inject a parameter or environment variable named is_running_locally : bool to your DAGs. Then, you can short-circuit some of the tasks based on the value of this parameter. For instance, a task that ingests several terabytes of data from a database may react to this parameter by ingesting less data, or even returning a fixture. This pattern will allow you to perform integration tests on your DAGs and still validate that they behave as expected. \u26d4 Limitations \u00b6 The nodes in a DAG will run in the right order according to their dependencies (i.e. using their topological sorting). However, they will NOT run in parallel. \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about this runtime.","title":"Local Runtime"},{"location":"user-guide/runtimes/local/#local-runtime","text":"The local runtime enables you to run any task or DAG locally. Just do: Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from dagger import dsl @dsl . task () def echo ( message ): print ( message ) @dsl . DAG () def my_pipeline ( message1 , message2 ): echo ( message1 ) echo ( message2 ) from dagger.runtime.local import invoke # When using the DSL, remember to ALWAYS call dsl.build() dag = dsl . build ( my_pipeline ) invoke ( dag , params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, ) Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from dagger import DAG , FromParam , Task def echo ( message ): print ( message ) dag = DAG ( inputs = { \"message1\" : FromParam (), \"message2\" : FromParam (), }, nodes = { \"echo-message-1\" : Task ( echo , inputs = { \"message\" : FromParam ( \"message1\" ), }, ), \"echo-message-2\" : Task ( echo , inputs = { \"message\" : FromParam ( \"message2\" ), }, ), }, ) from dagger.runtime.local import invoke invoke ( dag , params = { \"message1\" : \"Hello\" , \"message2\" : \"World\" , }, )","title":"Local Runtime"},{"location":"user-guide/runtimes/local/#local-development-and-testing","text":"When you are developing your DAGs, you will want to test their behavior as frequently as possible. This practice helps you iterate more quickly. The local runtime is great for local development and testing. On top of the validations that the data structures already perform, the local runtime serializes every output and connects all the nodes together, so it is a great way to verify that all your nodes can communicate effectively . For data pipelines that deal with large amounts of data or take a long time to execute, we recommend you inject a parameter or environment variable named is_running_locally : bool to your DAGs. Then, you can short-circuit some of the tasks based on the value of this parameter. For instance, a task that ingests several terabytes of data from a database may react to this parameter by ingesting less data, or even returning a fixture. This pattern will allow you to perform integration tests on your DAGs and still validate that they behave as expected.","title":"\ud83c\udfe0 Local Development and Testing"},{"location":"user-guide/runtimes/local/#limitations","text":"The nodes in a DAG will run in the right order according to their dependencies (i.e. using their topological sorting). However, they will NOT run in parallel.","title":"\u26d4 Limitations"},{"location":"user-guide/runtimes/local/#api-reference","text":"Check the API Reference for more details about this runtime.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/runtimes/write-your-own/","text":"Write your own Runtime \u00b6 The number of technologies that allow you to orchestrate data pipelines at scale is growing every day. If you want to take advantage of Dagger 's existing features, but run DAGs using a different orchestration engine, a valid option is to implement your own runtime. Implementing a runtime may be easy (for instance, we believe it would be fairly easy to implement a runtime for Kubeflow Pipelines based on the existing runtime for Argo Workflows ) or it may be a complex endeavor, depending on the orchestration engine you choose. Therefore, we don't have a specific guide on how to write runtimes, but rather a series of tips based on our experience writing them: Explore the code of the existing runtimes to understand how they're built. Explore the APIs of the core data structures ( tasks , dags , inputs , outputs and serializers ) to see all the information you can get from them. Begin with a small proof of concept . Try to get a hello world example going. Then move on to more complex scenarios, bit by bit. Leverage the existing functionality available in the other runtimes . Do not copy; reuse. Open an issue in our GitHub repo or a thread in GitHub Discussions . We will be happy to talk about your idea and provide some guidance along the way.","title":"Write your own Runtime"},{"location":"user-guide/runtimes/write-your-own/#write-your-own-runtime","text":"The number of technologies that allow you to orchestrate data pipelines at scale is growing every day. If you want to take advantage of Dagger 's existing features, but run DAGs using a different orchestration engine, a valid option is to implement your own runtime. Implementing a runtime may be easy (for instance, we believe it would be fairly easy to implement a runtime for Kubeflow Pipelines based on the existing runtime for Argo Workflows ) or it may be a complex endeavor, depending on the orchestration engine you choose. Therefore, we don't have a specific guide on how to write runtimes, but rather a series of tips based on our experience writing them: Explore the code of the existing runtimes to understand how they're built. Explore the APIs of the core data structures ( tasks , dags , inputs , outputs and serializers ) to see all the information you can get from them. Begin with a small proof of concept . Try to get a hello world example going. Then move on to more complex scenarios, bit by bit. Leverage the existing functionality available in the other runtimes . Do not copy; reuse. Open an issue in our GitHub repo or a thread in GitHub Discussions . We will be happy to talk about your idea and provide some guidance along the way.","title":"Write your own Runtime"},{"location":"user-guide/serializers/alternatives/","text":"Serializers \u00b6 When you run your DAGs with a runtime, the outputs produced by every task are serialized from a native Python type (such as a str , a dict , a pd.DataFrame or even a custom class ) into a string of bytes . Serializing outputs is necessary because two tasks may run on completely different physical machines, and Dagger needs a consistent format to store them and transmit them over the network. \ud83d\udce6 Built-in Serializers \u00b6 Dagger comes with a few serializers built in. Namely: dagger.AsJSON , which uses Python's json library . dagger.AsPickle , which uses Python's pickle library \ud83c\udccf Default Serializer: AsJSON \u00b6 By default, outputs are serialized using JSON. JSON works well for most basic types ( int , float , str , bool , dict , list and None values). However, soon you will need to work with more complex types of values. For instance: You may be working with Pandas DataFrames , which you'd like to serialize as CSV or Parquet files. You may be working with contracts which you'd like to serialize using Protocol Buffers or Apache Avro . For those cases, Dagger allows you to change the serializers used for each value returned by a task, and implement your custom serializers . \ud83d\udca1 Setting a specific serializer to use with an output \u00b6 Say you have a task that returns an object which we want to serialize using the Pickle protocol . Here's how you can instruct Dagger to do so: Imperative DSL 1 2 3 4 5 6 7 8 9 10 from dagger import AsPickle , dsl class CustomType : pass @dsl . task ( serializer = dsl . Serialize ( AsPickle ())) def generate_object (): return CustomType () Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from dagger import AsPickle , FromReturnValue , Task class CustomType : pass def generate_object (): return CustomType () task = Task ( generate_object , outputs = { \"object\" : FromReturnValue ( serializer = AsPickle ()), }, ) Different Serializers for Multiple Outputs \u00b6 Now imagine the task actually returned multiple outputs: A boolean which can be serialized as JSON. A custom object which we want to serialize using Pickle. Here's how you can instruct Dagger to use a different serializer for each of the outputs: Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dagger import AsJSON , AsPickle , dsl class CustomType : pass @dsl . task ( serializer = dsl . Serialize ( a_boolean = AsJSON (), a_custom_object = AsPickle (), ) ) def generate_multiple_outputs (): return { \"a_boolean\" : True , \"a_custom_object\" : CustomType (), } Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from dagger import AsPickle , FromKey , Task class CustomType : pass def generate_multiple_outputs (): return { \"a_boolean\" : True , \"a_custom_object\" : CustomType (), } task = Task ( generate_multiple_outputs , outputs = { \"boolean\" : FromKey ( \"a_boolean\" ), \"custom_obj\" : FromKey ( \"a_custom_object\" , serializer = AsPickle ()), }, ) \u2699\ufe0f Performance and memory \u00b6 One of the main tenets of Dagger is that it should support arbitrarily large data pipelines without a big impact on its memory footprint. To fulfill that promise, serializers work with I/O streams, allowing you to serialize and deserialize values backed by a local or remote file system. This means you should be able to use native Python types such as Dask's DataFrames to process large datasets and pass them between nodes without requiring the machine's memory to scale linearly with the size of the datasets. \ud83d\udee0\ufe0f Implementing your own serializer \u00b6 To understand how to write your own serialization mechanism you can read this guide .","title":"Serializers"},{"location":"user-guide/serializers/alternatives/#serializers","text":"When you run your DAGs with a runtime, the outputs produced by every task are serialized from a native Python type (such as a str , a dict , a pd.DataFrame or even a custom class ) into a string of bytes . Serializing outputs is necessary because two tasks may run on completely different physical machines, and Dagger needs a consistent format to store them and transmit them over the network.","title":"Serializers"},{"location":"user-guide/serializers/alternatives/#built-in-serializers","text":"Dagger comes with a few serializers built in. Namely: dagger.AsJSON , which uses Python's json library . dagger.AsPickle , which uses Python's pickle library","title":"\ud83d\udce6 Built-in Serializers"},{"location":"user-guide/serializers/alternatives/#default-serializer-asjson","text":"By default, outputs are serialized using JSON. JSON works well for most basic types ( int , float , str , bool , dict , list and None values). However, soon you will need to work with more complex types of values. For instance: You may be working with Pandas DataFrames , which you'd like to serialize as CSV or Parquet files. You may be working with contracts which you'd like to serialize using Protocol Buffers or Apache Avro . For those cases, Dagger allows you to change the serializers used for each value returned by a task, and implement your custom serializers .","title":"\ud83c\udccf Default Serializer: AsJSON"},{"location":"user-guide/serializers/alternatives/#setting-a-specific-serializer-to-use-with-an-output","text":"Say you have a task that returns an object which we want to serialize using the Pickle protocol . Here's how you can instruct Dagger to do so: Imperative DSL 1 2 3 4 5 6 7 8 9 10 from dagger import AsPickle , dsl class CustomType : pass @dsl . task ( serializer = dsl . Serialize ( AsPickle ())) def generate_object (): return CustomType () Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from dagger import AsPickle , FromReturnValue , Task class CustomType : pass def generate_object (): return CustomType () task = Task ( generate_object , outputs = { \"object\" : FromReturnValue ( serializer = AsPickle ()), }, )","title":"\ud83d\udca1 Setting a specific serializer to use with an output"},{"location":"user-guide/serializers/alternatives/#different-serializers-for-multiple-outputs","text":"Now imagine the task actually returned multiple outputs: A boolean which can be serialized as JSON. A custom object which we want to serialize using Pickle. Here's how you can instruct Dagger to use a different serializer for each of the outputs: Imperative DSL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dagger import AsJSON , AsPickle , dsl class CustomType : pass @dsl . task ( serializer = dsl . Serialize ( a_boolean = AsJSON (), a_custom_object = AsPickle (), ) ) def generate_multiple_outputs (): return { \"a_boolean\" : True , \"a_custom_object\" : CustomType (), } Declarative Data Structures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from dagger import AsPickle , FromKey , Task class CustomType : pass def generate_multiple_outputs (): return { \"a_boolean\" : True , \"a_custom_object\" : CustomType (), } task = Task ( generate_multiple_outputs , outputs = { \"boolean\" : FromKey ( \"a_boolean\" ), \"custom_obj\" : FromKey ( \"a_custom_object\" , serializer = AsPickle ()), }, )","title":"Different Serializers for Multiple Outputs"},{"location":"user-guide/serializers/alternatives/#performance-and-memory","text":"One of the main tenets of Dagger is that it should support arbitrarily large data pipelines without a big impact on its memory footprint. To fulfill that promise, serializers work with I/O streams, allowing you to serialize and deserialize values backed by a local or remote file system. This means you should be able to use native Python types such as Dask's DataFrames to process large datasets and pass them between nodes without requiring the machine's memory to scale linearly with the size of the datasets.","title":"\u2699\ufe0f Performance and memory"},{"location":"user-guide/serializers/alternatives/#implementing-your-own-serializer","text":"To understand how to write your own serialization mechanism you can read this guide .","title":"\ud83d\udee0\ufe0f Implementing your own serializer"},{"location":"user-guide/serializers/json/","text":"AsJSON \u00b6 AsJSON serializes values to and from JSON using Python's standard json library . This is the default serialization mechanism for Dagger . If you don't override it, Dagger will try to use JSON to serialize the output of each task. \u26d4 Limitations: Serialization needs to be a symmetrical process \u00b6 There are some types of objects in Python (such as named tuples) where the serialization and deserialization processes are not symmetrical. For instance: 1 2 3 4 5 6 7 import json original_t = MyNamedTuple ( x = 1 , y = 2 ) serialized_t = json . dumps ( original_t ) deserialized_t = json . loads ( serialized_t ) assert original_t != deserialized_t Note Python relies on you to write custom JSONEncoder and JSONDecoder implementations to achieve symmetry. However, the standard AsJSON serializer does not support this. If you want to serialize custom objects as JSON, we recommend you implement your own serializer . \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about this serializer.","title":"AsJSON"},{"location":"user-guide/serializers/json/#asjson","text":"AsJSON serializes values to and from JSON using Python's standard json library . This is the default serialization mechanism for Dagger . If you don't override it, Dagger will try to use JSON to serialize the output of each task.","title":"AsJSON"},{"location":"user-guide/serializers/json/#limitations-serialization-needs-to-be-a-symmetrical-process","text":"There are some types of objects in Python (such as named tuples) where the serialization and deserialization processes are not symmetrical. For instance: 1 2 3 4 5 6 7 import json original_t = MyNamedTuple ( x = 1 , y = 2 ) serialized_t = json . dumps ( original_t ) deserialized_t = json . loads ( serialized_t ) assert original_t != deserialized_t Note Python relies on you to write custom JSONEncoder and JSONDecoder implementations to achieve symmetry. However, the standard AsJSON serializer does not support this. If you want to serialize custom objects as JSON, we recommend you implement your own serializer .","title":"\u26d4 Limitations: Serialization needs to be a symmetrical process"},{"location":"user-guide/serializers/json/#api-reference","text":"Check the API Reference for more details about this serializer.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/serializers/pickle/","text":"AsPickle \u00b6 AsPickle serializes values to and from Python's Pickle protocol . \u26d4 Limitations \u00b6 The Pickle protocol has no limitations as far as serializing native Python objects goes. Nevertheless, we strongly recommend you read the warnings in the official documentation, and the comparison with other kinds of serializers to understand the behavior and the dangers of pickle serialization. \ud83d\udcd7 API Reference \u00b6 Check the API Reference for more details about this serializer.","title":"AsPickle"},{"location":"user-guide/serializers/pickle/#aspickle","text":"AsPickle serializes values to and from Python's Pickle protocol .","title":"AsPickle"},{"location":"user-guide/serializers/pickle/#limitations","text":"The Pickle protocol has no limitations as far as serializing native Python objects goes. Nevertheless, we strongly recommend you read the warnings in the official documentation, and the comparison with other kinds of serializers to understand the behavior and the dangers of pickle serialization.","title":"\u26d4 Limitations"},{"location":"user-guide/serializers/pickle/#api-reference","text":"Check the API Reference for more details about this serializer.","title":"\ud83d\udcd7 API Reference"},{"location":"user-guide/serializers/write-your-own/","text":"Write your own Serializer \u00b6 Sometimes you may be dealing with objects that need to be serialized in a specific format that doesn't come out of the box with Dagger . For example, you may want to serialize: A Pandas DataFrame as a CSV or a Parquet file. A Protocol Buffers contract. In those cases, you can bring your custom serialization implementation to Dagger and use it in your tasks . \u2611\ufe0f Serializer Protocol \u00b6 To write a custom serializer, you need to create a class that implements the following protocol: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \"\"\"Protocol all serializers should conform to.\"\"\" from typing import Any , BinaryIO , Protocol , runtime_checkable @runtime_checkable class Serializer ( Protocol ): # pragma: no cover \"\"\"Protocol all serializers should conform to.\"\"\" @property def extension ( self ) -> str : \"\"\"Extension to use for the files generated by this serializer.\"\"\" ... def serialize ( self , value : Any , writer : BinaryIO ): \"\"\"Serialize a value and write it to the provided writer stream.\"\"\" ... def deserialize ( self , reader : BinaryIO ) -> Any : \"\"\"Deserialize a stream of bytes into a value.\"\"\" ... \ud83d\udca1 Example: A YAML serializer \u00b6 Say you want to serialize some of your outputs as YAML. Here's how a (slightly naive) implementation of a YAML serializer would look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from typing import Any , BinaryIO import yaml from dagger import DeserializationError , SerializationError class AsYAML : \"\"\"Serializer implementation that uses YAML to marshal/unmarshal Python data structures.\"\"\" extension = \"yaml\" def serialize ( self , value : Any , writer : BinaryIO ): \"\"\"Serialize a value into a YAML object, encoded into binary format using utf-8.\"\"\" try : yaml . dump ( value , writer , Dumper = yaml . SafeDumper , encoding = \"utf-8\" ) except yaml . YAMLError as e : raise SerializationError ( e ) def deserialize ( self , reader : BinaryIO ) -> Any : \"\"\"Deserialize a utf-8-encoded yaml object into the value it represents.\"\"\" try : return yaml . load ( reader , Loader = yaml . SafeLoader ) except yaml . YAMLError as e : raise DeserializationError ( e )","title":"Write your own Serializer"},{"location":"user-guide/serializers/write-your-own/#write-your-own-serializer","text":"Sometimes you may be dealing with objects that need to be serialized in a specific format that doesn't come out of the box with Dagger . For example, you may want to serialize: A Pandas DataFrame as a CSV or a Parquet file. A Protocol Buffers contract. In those cases, you can bring your custom serialization implementation to Dagger and use it in your tasks .","title":"Write your own Serializer"},{"location":"user-guide/serializers/write-your-own/#serializer-protocol","text":"To write a custom serializer, you need to create a class that implements the following protocol: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \"\"\"Protocol all serializers should conform to.\"\"\" from typing import Any , BinaryIO , Protocol , runtime_checkable @runtime_checkable class Serializer ( Protocol ): # pragma: no cover \"\"\"Protocol all serializers should conform to.\"\"\" @property def extension ( self ) -> str : \"\"\"Extension to use for the files generated by this serializer.\"\"\" ... def serialize ( self , value : Any , writer : BinaryIO ): \"\"\"Serialize a value and write it to the provided writer stream.\"\"\" ... def deserialize ( self , reader : BinaryIO ) -> Any : \"\"\"Deserialize a stream of bytes into a value.\"\"\" ...","title":"\u2611\ufe0f Serializer Protocol"},{"location":"user-guide/serializers/write-your-own/#example-a-yaml-serializer","text":"Say you want to serialize some of your outputs as YAML. Here's how a (slightly naive) implementation of a YAML serializer would look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from typing import Any , BinaryIO import yaml from dagger import DeserializationError , SerializationError class AsYAML : \"\"\"Serializer implementation that uses YAML to marshal/unmarshal Python data structures.\"\"\" extension = \"yaml\" def serialize ( self , value : Any , writer : BinaryIO ): \"\"\"Serialize a value into a YAML object, encoded into binary format using utf-8.\"\"\" try : yaml . dump ( value , writer , Dumper = yaml . SafeDumper , encoding = \"utf-8\" ) except yaml . YAMLError as e : raise SerializationError ( e ) def deserialize ( self , reader : BinaryIO ) -> Any : \"\"\"Deserialize a utf-8-encoded yaml object into the value it represents.\"\"\" try : return yaml . load ( reader , Loader = yaml . SafeLoader ) except yaml . YAMLError as e : raise DeserializationError ( e )","title":"\ud83d\udca1 Example: A YAML serializer"}]}