{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dagger \u00b6 Define sophisticated data pipelines and run them on different distributed systems (such as Argo Workflows). Dagger is a Python library that allows you to: Define sophisticated DAGs (direct acyclic graphs) using very straightforward Pythonic code. Run those DAGs seamlessly in different runtimes or workflow orchestrators (such as Argo Workflows, Kubeflow Pipelines, and more). Features \u00b6 Define tasks and DAGs, and compose them together seamlessly. Parameterize DAGs and pass parameters between nodes in plain Python (the runtime takes care of serializing and transmitting data on your behalf). Create dynamic for loops, map-reduce operations easily. Run your DAGs locally or using a distributed workflow orchestrator (such as Argo Workflows). Extend your tasks to take advantage of all the features offered by your runtime (e.g. Retry strategies, Kubernetes scheduling directives, etc.) ... All with a simple Pythonic DSL that feels just like coding regular Python functions. Other nice features of Dagger are: Zero dependencies, 100% test coverage, great documentation and plenty of examples to get you started. Guiding Principles \u00b6 Dagger was created to facilitate the creation and ongoing maintenance of data and ML pipelines. This goal is reflected in Dagger 's architecture and main design decisions: To make common use cases and patterns (such as dynamic loops or map-reduce operations) as easy as possible . To minimize boilerplate, plumbing or low-level code (such as serializing inputs/outputs and storing them in a local/remote file system). To onboard users in just a couple of hours through great documentation, comprehensive examples and tutorials. To never sacrifice reliability and performance . Architecture \u00b6 Dagger is built around 3 components: A set of core data structures that represent the intended behavior of a DAG. A domain-specific language (DSL) that uses metaprogramming to capture how a DAG should behave, and represents it using the core data structures. Multiple runtimes that inspect the core data structures to run the corresponding DAG, or prepare the DAG to run in a specific pipeline executor. The Core Data Structures \u00b6 Dagger defines DAGs using a series of immutable data structures. These structures are responsible for: Exposing all the relevant information so that runtimes can run the DAGs, or translate them into formats supported by other pipeline executors. Validating all the pieces of a DAG to catch errors as early as possible in the development lifecycle. For instance: Node inputs must not reference outputs that do not exist. The DAG must not contain any cycles. Nodes and outputs are partitioned in ways supported by the library. etc. They are divided into different categories: Nodes may be tasks (functions) or DAGs (a series of nodes connected together). DAGs can be nested into other dags and composed elegantly. Inputs may come from a DAG parameter or from the output of another node. Outputs may be retrieved directly from the return value of a task's function, or from a sub-element of that value (a key or a property). Every input/output has a serializer associated with it. The serializer is responsible for turning the value of that input/output into a string of bytes, and a string of bytes back into its original value.","title":"Overview"},{"location":"#dagger","text":"Define sophisticated data pipelines and run them on different distributed systems (such as Argo Workflows). Dagger is a Python library that allows you to: Define sophisticated DAGs (direct acyclic graphs) using very straightforward Pythonic code. Run those DAGs seamlessly in different runtimes or workflow orchestrators (such as Argo Workflows, Kubeflow Pipelines, and more).","title":"Dagger"},{"location":"#features","text":"Define tasks and DAGs, and compose them together seamlessly. Parameterize DAGs and pass parameters between nodes in plain Python (the runtime takes care of serializing and transmitting data on your behalf). Create dynamic for loops, map-reduce operations easily. Run your DAGs locally or using a distributed workflow orchestrator (such as Argo Workflows). Extend your tasks to take advantage of all the features offered by your runtime (e.g. Retry strategies, Kubernetes scheduling directives, etc.) ... All with a simple Pythonic DSL that feels just like coding regular Python functions. Other nice features of Dagger are: Zero dependencies, 100% test coverage, great documentation and plenty of examples to get you started.","title":"Features"},{"location":"#guiding-principles","text":"Dagger was created to facilitate the creation and ongoing maintenance of data and ML pipelines. This goal is reflected in Dagger 's architecture and main design decisions: To make common use cases and patterns (such as dynamic loops or map-reduce operations) as easy as possible . To minimize boilerplate, plumbing or low-level code (such as serializing inputs/outputs and storing them in a local/remote file system). To onboard users in just a couple of hours through great documentation, comprehensive examples and tutorials. To never sacrifice reliability and performance .","title":"Guiding Principles"},{"location":"#architecture","text":"Dagger is built around 3 components: A set of core data structures that represent the intended behavior of a DAG. A domain-specific language (DSL) that uses metaprogramming to capture how a DAG should behave, and represents it using the core data structures. Multiple runtimes that inspect the core data structures to run the corresponding DAG, or prepare the DAG to run in a specific pipeline executor.","title":"Architecture"},{"location":"#the-core-data-structures","text":"Dagger defines DAGs using a series of immutable data structures. These structures are responsible for: Exposing all the relevant information so that runtimes can run the DAGs, or translate them into formats supported by other pipeline executors. Validating all the pieces of a DAG to catch errors as early as possible in the development lifecycle. For instance: Node inputs must not reference outputs that do not exist. The DAG must not contain any cycles. Nodes and outputs are partitioned in ways supported by the library. etc. They are divided into different categories: Nodes may be tasks (functions) or DAGs (a series of nodes connected together). DAGs can be nested into other dags and composed elegantly. Inputs may come from a DAG parameter or from the output of another node. Outputs may be retrieved directly from the return value of a task's function, or from a sub-element of that value (a key or a property). Every input/output has a serializer associated with it. The serializer is responsible for turning the value of that input/output into a string of bytes, and a string of bytes back into its original value.","title":"The Core Data Structures"},{"location":"quick-start/","text":"Quick Start \u00b6 To get a first idea of what Dagger can do, let's install it, create our first DAG and run it locally. Installation \u00b6 Dagger is published to the Python Package Index (PyPI) under the name py-dagger . To install it, you can simply run: pip install py-dagger Creating a DAG \u00b6 Take the following piece of code: import random from dagger import dsl @dsl.task() def generate_numbers(): length = random.randint(3, 20) numbers = list(range(length)) print(f\"Generating the following list of numbers: {numbers}\") return numbers @dsl.task() def raise_number(n, exponent): print(f\"Raising {n} to a power of {exponent}\") return n ** exponent @dsl.task() def sum_numbers(numbers): print(f\"Calculating the sum of {numbers}\") return sum(numbers) @dsl.DAG() def map_reduce_pipeline(exponent): numbers = generate_numbers() raised_numbers = [] for n in numbers: raised_numbers.append( raise_number(n, exponent) ) return sum_numbers(raised_numbers) dag = dsl.build(map_reduce_pipeline) from dagger.runtime.local import invoke result = invoke(dag, params={\"exponent\": 2}) print(f\"The final result was {result}\") Let's go step by step. First, we use the dagger.dsl.task decorator to define different tasks. Tasks in Dagger are just Python functions. In this case, we have 3 tasks: generate_numbers() returns a list of numbers. The list has a variable length, to show how we can do dynamic loops. raise_number(n, exponent) receives a number and an exponent, and returns n^exponent . sum_numbers(numbers) receives a list of numbers and returns the sum of all of them. Next, we use the dagger.dsl.DAG decorator on another function that invokes all the previously defined tasks and connects their inputs/outputs. The example uses a for loop and appends elements to a list to gather all the different results. But you can try replacing it with something more Pythonic : @dsl.DAG() def map_reduce_pipeline(exponent): return sum_numbers([raise_number(n, exponent) for n in generate_numbers()]) Finally, we use dagger.dsl.build to transform that decorated function into a dagger.dag.DAG data structure, and we test it locally with dagger.runtime.local.invoke . Other Runtimes \u00b6 The previous example showed how we can model a fairly complex use case (a dynamic map-reduce) and run it locally in just a few lines of code. The great thing about Dagger is that running your pipeline in a distributed pipeline engine (such as Argo Workflows or Kubeflow Pipelines) is just as easy! At the moment, we support the following runtimes: dagger.runtime.local for local experimentation and testing. dagger.runtime.cli , used by other runtimes. dagger.runtime.argo , to run your pipelines on Argo Workflows . You can check the Runtimes Documentation to get started with any of them. Tutorials, Examples and User Guides \u00b6 Does it sound interesting? We're just scratching the surface of what's possible with Dagger . If you're interested, you can begin exploring: The Tutorial for a step-by-step introduction to Dagger . The User Guide for an in-depth explanation of all the different components available, extensibility points and design decisions. The API Reference . The Examples .","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"To get a first idea of what Dagger can do, let's install it, create our first DAG and run it locally.","title":"Quick Start"},{"location":"quick-start/#installation","text":"Dagger is published to the Python Package Index (PyPI) under the name py-dagger . To install it, you can simply run: pip install py-dagger","title":"Installation"},{"location":"quick-start/#creating-a-dag","text":"Take the following piece of code: import random from dagger import dsl @dsl.task() def generate_numbers(): length = random.randint(3, 20) numbers = list(range(length)) print(f\"Generating the following list of numbers: {numbers}\") return numbers @dsl.task() def raise_number(n, exponent): print(f\"Raising {n} to a power of {exponent}\") return n ** exponent @dsl.task() def sum_numbers(numbers): print(f\"Calculating the sum of {numbers}\") return sum(numbers) @dsl.DAG() def map_reduce_pipeline(exponent): numbers = generate_numbers() raised_numbers = [] for n in numbers: raised_numbers.append( raise_number(n, exponent) ) return sum_numbers(raised_numbers) dag = dsl.build(map_reduce_pipeline) from dagger.runtime.local import invoke result = invoke(dag, params={\"exponent\": 2}) print(f\"The final result was {result}\") Let's go step by step. First, we use the dagger.dsl.task decorator to define different tasks. Tasks in Dagger are just Python functions. In this case, we have 3 tasks: generate_numbers() returns a list of numbers. The list has a variable length, to show how we can do dynamic loops. raise_number(n, exponent) receives a number and an exponent, and returns n^exponent . sum_numbers(numbers) receives a list of numbers and returns the sum of all of them. Next, we use the dagger.dsl.DAG decorator on another function that invokes all the previously defined tasks and connects their inputs/outputs. The example uses a for loop and appends elements to a list to gather all the different results. But you can try replacing it with something more Pythonic : @dsl.DAG() def map_reduce_pipeline(exponent): return sum_numbers([raise_number(n, exponent) for n in generate_numbers()]) Finally, we use dagger.dsl.build to transform that decorated function into a dagger.dag.DAG data structure, and we test it locally with dagger.runtime.local.invoke .","title":"Creating a DAG"},{"location":"quick-start/#other-runtimes","text":"The previous example showed how we can model a fairly complex use case (a dynamic map-reduce) and run it locally in just a few lines of code. The great thing about Dagger is that running your pipeline in a distributed pipeline engine (such as Argo Workflows or Kubeflow Pipelines) is just as easy! At the moment, we support the following runtimes: dagger.runtime.local for local experimentation and testing. dagger.runtime.cli , used by other runtimes. dagger.runtime.argo , to run your pipelines on Argo Workflows . You can check the Runtimes Documentation to get started with any of them.","title":"Other Runtimes"},{"location":"quick-start/#tutorials-examples-and-user-guides","text":"Does it sound interesting? We're just scratching the surface of what's possible with Dagger . If you're interested, you can begin exploring: The Tutorial for a step-by-step introduction to Dagger . The User Guide for an in-depth explanation of all the different components available, extensibility points and design decisions. The API Reference . The Examples .","title":"Tutorials, Examples and User Guides"},{"location":"api/dag/","text":"package dagger. dag Define workflows/pipelines as Directed Acyclic Graphs (DAGs) of Tasks. class dagger.dag.dag. DAG ( nodes , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Data Structure that represents a DAG for later execution. DAGs are made up of: - A set of nodes, connected to each other through their inputs/outputs - A set of inputs to the DAG - A set of outputs from the DAG Attributes inputs \u2014 Get the inputs the DAG expects. node_execution_order \u2014 Get a list of nodes to execute in order, respecting dependencies between nodes. nodes \u2014 Get the nodes that compose the DAG. outputs \u2014 Get the outputs the DAG produces. partition_by_input (str, optional) \u2014 Return the input this task should be partitioned by, if any. runtime_options \u2014 Get the specified runtime options. Methods __eq__ ( obj ) (bool) \u2014 Return true if the two DAGs are equivalent to each other. __repr__ ( ) (str) \u2014 Return a human-readable representation of the DAG. method __repr__ ( ) \u2192 str Return a human-readable representation of the DAG. method __eq__ ( obj ) \u2192 bool Return true if the two DAGs are equivalent to each other.","title":"dagger.dag"},{"location":"api/dag/#daggerdag","text":"Define workflows/pipelines as Directed Acyclic Graphs (DAGs) of Tasks. class dagger.dag.dag. DAG ( nodes , inputs=None , outputs=None , runtime_options=None , partition_by_input=None ) Data Structure that represents a DAG for later execution. DAGs are made up of: - A set of nodes, connected to each other through their inputs/outputs - A set of inputs to the DAG - A set of outputs from the DAG Attributes inputs \u2014 Get the inputs the DAG expects. node_execution_order \u2014 Get a list of nodes to execute in order, respecting dependencies between nodes. nodes \u2014 Get the nodes that compose the DAG. outputs \u2014 Get the outputs the DAG produces. partition_by_input (str, optional) \u2014 Return the input this task should be partitioned by, if any. runtime_options \u2014 Get the specified runtime options. Methods __eq__ ( obj ) (bool) \u2014 Return true if the two DAGs are equivalent to each other. __repr__ ( ) (str) \u2014 Return a human-readable representation of the DAG. method __repr__ ( ) \u2192 str Return a human-readable representation of the DAG. method __eq__ ( obj ) \u2192 bool Return true if the two DAGs are equivalent to each other.","title":"dagger.dag"},{"location":"api/dsl/","text":"","title":"Dsl"},{"location":"api/init/","text":"package dagger Define sophisticated workflows/pipelines as Directed Acyclic Graphs (DAGs) and execute them with different runtimes, either locally or remotely. Exports: from dagger import dsl, DAG, Task","title":"dagger"},{"location":"api/init/#dagger","text":"Define sophisticated workflows/pipelines as Directed Acyclic Graphs (DAGs) and execute them with different runtimes, either locally or remotely. Exports: from dagger import dsl, DAG, Task","title":"dagger"},{"location":"api/input/","text":"","title":"Input"},{"location":"api/output/","text":"","title":"Output"},{"location":"api/runtime-argo/","text":"","title":"Runtime argo"},{"location":"api/runtime-cli/","text":"","title":"Runtime cli"},{"location":"api/runtime-local/","text":"","title":"Runtime local"},{"location":"api/serializer/","text":"","title":"Serializer"},{"location":"api/task/","text":"","title":"Task"},{"location":"tutorial/argo-workflows/","text":"","title":"Argo workflows"},{"location":"tutorial/hello-world/","text":"","title":"Hello world"},{"location":"tutorial/introduction/","text":"","title":"Introduction"},{"location":"tutorial/local-tests/","text":"","title":"Local tests"},{"location":"tutorial/machine-learning/","text":"","title":"Machine learning"},{"location":"tutorial/map-reduce/","text":"","title":"Map reduce"},{"location":"tutorial/passing-parameters/","text":"","title":"Passing parameters"},{"location":"user-guide/introduction/","text":"","title":"Introduction"},{"location":"user-guide/dags/dag-composition/","text":"","title":"Dag composition"},{"location":"user-guide/dags/dags/","text":"","title":"Dags"},{"location":"user-guide/dags/inputs/","text":"","title":"Inputs"},{"location":"user-guide/dags/map-reduce/","text":"","title":"Map reduce"},{"location":"user-guide/dags/outputs/","text":"","title":"Outputs"},{"location":"user-guide/dags/partitioning/","text":"","title":"Partitioning"},{"location":"user-guide/dags/tasks/","text":"","title":"Tasks"},{"location":"user-guide/runtimes/alternatives/","text":"","title":"Alternatives"},{"location":"user-guide/runtimes/argo/","text":"","title":"Argo"},{"location":"user-guide/runtimes/cli/","text":"","title":"Cli"},{"location":"user-guide/runtimes/local/","text":"","title":"Local"},{"location":"user-guide/runtimes/write-your-own/","text":"","title":"Write your own"},{"location":"user-guide/serializers/json/","text":"","title":"Json"},{"location":"user-guide/serializers/pickle/","text":"","title":"Pickle"},{"location":"user-guide/serializers/write-your-own/","text":"","title":"Write your own"},{"location":"user-guide/syntax/declarative/","text":"","title":"Declarative"},{"location":"user-guide/syntax/imperative/","text":"","title":"Imperative"}]}