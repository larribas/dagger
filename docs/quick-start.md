# Quick Start

To get a first idea of what _Dagger_ can do, let's install it, create our first DAG and run it locally.


## ðŸ’¾ Installation

_Dagger_ is published to the Python Package Index (PyPI) under the name `py-dagger`. To install it, you can simply run:

```sh
pip install py-dagger
```


## ðŸ¦¸ _Dagger_ in Action

### Defining a DAG using the imperative DSL

The following piece of code demonstrates how to build a DAG that performs a map-reduce operation on a series of numbers:

```python
--8<-- "docs/code_snippets/quick_start/quick_start.py"
```

Let's take it step by step. First, we use the `dagger.dsl.task` decorator to define different tasks. Tasks in _Dagger_ are just Python functions. In this case, we have 3 tasks:

- `generate_numbers()` returns a list of numbers. The length of the list varies dynamically.
- `raise_number(n, exponent)` receives a number and an exponent, which has a default value, 2, and returns `n^exponent`.
- `sum_numbers(numbers)` receives a list of numbers and returns the sum of all of them.

Next, we use the `dagger.dsl.DAG` decorator on another function that __invokes all the tasks we've defined and connects their inputs/outputs__.

In the example, we are iterating over all the numbers generated by the first task and running the second one. Then, we collect all the results of running the second task into a list and pass that list to a function that sums all the results together.

We've used a traditional `for` loop and appended elements to the list, but you can try replacing it with something more _Pythonic_:

```python linenums="27"
@dsl.DAG()
def map_reduce_pipeline(exponent):
    return sum_numbers([raise_number(n, exponent) for n in generate_numbers()])
```


### Transforming the DAG into a set of data structures

After defining how our DAG should behave using a function decorated by `dagger.dsl.DAG`, we will need to use `dagger.dsl.build` to __transform it into a `dagger.DAG` data structure__, like this:

```python
dag = dsl.build(map_reduce_pipeline)
```

The variable `dag` now contains our pipeline expressed as a collection of data structures. These data structures validate that our DAG has been built correctly, and allow us to run it using one of the available runtimes.


### Running our DAG locally

The final step will be to test our DAG locally using the `dagger.runtime.local`.

Just do:

```python
from dagger.runtime.local import invoke

result = invoke(dag, params={"seed": 1, "exponent": 2})
print(f"The final result was {result}")
```

After this, you should see the results of the DAG printed to your screen. 

Equivalently, because the parameter `exponent` has a default value of 2, you can omit passing the value 2 when invoking the DAG:

```python
from dagger.runtime.local import invoke

result = invoke(map_reduce_pipeline, params={"seed": 1})
print(f"The final result was {result}")
```


### Other Runtimes

The previous example showed how we can model a fairly complex use case (a dynamic map-reduce) and run it locally in just a few lines of code.

The great thing about _Dagger_ is that running your pipeline in a distributed pipeline engine (such as _Argo Workflows_ or _Kubeflow Pipelines_) is just as easy!

You can check the [Runtimes Documentation](user-guide/runtimes/alternatives.md) to learn about the available runtimes.


## ðŸ§  Tutorials, Examples and API Reference

Does it look useful so far? We're just scratching the surface of what's possible with _Dagger_. To learn more, you can:

- Start by defining some [tasks](user-guide/tasks.md).
- Check all the [examples](https://github.com/larribas/dagger/tree/main/examples).
- Browse the [API Reference](api/init.md).
